<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"./public/search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="求知若饥,虚心若愚。">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="求知若饥,虚心若愚。">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jeff Tian">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>求知若饥,虚心若愚。</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">求知若饥,虚心若愚。</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="search-pop-overlay">
  <div class="popup search-popup">
      <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

  </div>
</div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/%E7%90%86%E8%A7%A3%E4%B8%8A%E9%87%87%E6%A0%B7%E3%80%81%E4%B8%8B%E9%87%87%E6%A0%B7%E3%80%81%E6%B1%A0%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/%E7%90%86%E8%A7%A3%E4%B8%8A%E9%87%87%E6%A0%B7%E3%80%81%E4%B8%8B%E9%87%87%E6%A0%B7%E3%80%81%E6%B1%A0%E5%8C%96/" class="post-title-link" itemprop="url">理解上采样、下采样、池化</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:55:40 / Modified: 12:55:47" itemprop="dateCreated datePublished" datetime="2020-03-05T12:55:40+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="上采样、下采样"><a href="#上采样、下采样" class="headerlink" title="上采样、下采样"></a>上采样、下采样</h1><h2 id="缩小图像（或称为下采样（subsampled）或降采样（downsampled））"><a href="#缩小图像（或称为下采样（subsampled）或降采样（downsampled））" class="headerlink" title="缩小图像（或称为下采样（subsampled）或降采样（downsampled））"></a>缩小图像（或称为下采样（subsampled）或降采样（downsampled））</h2><p>主要目的有两个：<br>1、使得图像符合显示区域的大小；<br>2、生成对应图像的缩略图。</p>
<h2 id="放大图像（或称为上采样（upsampling）或图像插值（interpolating））"><a href="#放大图像（或称为上采样（upsampling）或图像插值（interpolating））" class="headerlink" title="放大图像（或称为上采样（upsampling）或图像插值（interpolating））"></a>放大图像（或称为上采样（upsampling）或图像插值（interpolating））</h2><p>主要目的是：<br>放大原图像,从而可以显示在更高分辨率的显示设备上。对图像的缩放操作并不能带来更多关于该图像的信息, 因此图像的质量将不可避免地受到影响。然而，确实有一些缩放方法能够增加图像的信息，从而使得缩放后的图像质量超过原图质量的。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>下采样原理：对于一幅图像I尺寸为M<em>N，对其进行s倍下采样，即得到(M/s)</em>(N/s)尺寸的得分辨率图像，当然s应该是M和N的公约数才行，如果考虑的是矩阵形式的图像，就是把原始图像s*s窗口内的图像变成一个像素，这个像素点的值就是窗口内所有像素的均值：</p>
<p>上采样原理：图像放大几乎都是采用内插值方法，即在原有图像像素的基础上在像素点之间采用合适的插值算法插入新的元素。如最近邻插值，双线性插值，均值插值，中值插值等方法。各种插值方法都有各自的优缺点。</p>
<p>无论缩小图像（下采样）还是放大图像（上采样），采样方式有很多种。</p>
<h1 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h1><p>池化（Pooling）是卷积神经网络中一个重要的概念，它实际上是一种形式的降采样。它会压缩输入的特征图，一方面减少了特征，导致了参数减少，进而简化了卷积网络计算时的复杂度；另一方面保持了特征的某种不变性（旋转、平移、伸缩等）。</p>
<p>池化操作主要有两种，一种是平均池化(Average Pooling)，即对邻域内的特征点求平均；另一种是最大池化(Max Pooling)，即对邻域内的特征点取最大。最大池化（Max pooling）是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。直觉上，这种机制能够有效地原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了过拟合。通常来说，CNN的卷积层之间都会周期性地插入池化层。</p>
<p>池化层通常会分别作用于每个输入的特征并减小其大小。当前最常用形式的池化层是每隔2个元素从图像划分出2*2的区块，然后对每个区块中的4个数取最大值。这将会减少75%的数据量。<br><img src="https://img-blog.csdnimg.cn/20200216214827576.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>池化方法特征提取误差主要来自两个部分：一是，邻域大小受限造成了估计值方差增大；二是，卷积层参数误差造成了估计均值的偏移。一般来说，在图像研究领域，对图像进行平均池化操作能减少第一种误差，同时更多地保留图像的背景信息；而另一方面，最大池化能减小第二种误差，更多地保留纹理信息。因此在进行卷积神经网络结构设计时，这两种池化方式往往交替使用。</p>
<p>简而言之，池化就是去除杂余信息，保留关键信息</p>
<h2 id="池化的作用"><a href="#池化的作用" class="headerlink" title="池化的作用"></a>池化的作用</h2><p>池化操作后的结果相比其输入缩小了。池化层的引入是仿照人的视觉系统对视觉输入对象进行降维和抽象。在卷积神经网络过去的工作中，研究者普遍认为池化层有如下三个功效：</p>
<p>1.特征不变形：池化操作是模型更加关注是否存在某些特征而不是特征具体的位置。<br>2.特征降维：池化相当于在空间范围内做了维度约减，从而使模型可以抽取更加广范围的特征。同时减小了下一层的输入大小，进而减少计算量和参数个数。<br>3.在一定程度上防止过拟合，更方便优化。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/%E5%88%9D%E8%AF%86%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/%E5%88%9D%E8%AF%86%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/" class="post-title-link" itemprop="url">初识语义分割</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:55:30 / Modified: 12:55:37" itemprop="dateCreated datePublished" datetime="2020-03-05T12:55:30+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="语义分割基础知识"><a href="#语义分割基础知识" class="headerlink" title="语义分割基础知识"></a>语义分割基础知识</h1><p>在计算机视觉的语义感知部分主要分为图像分类、目标检测、语义分割、实例分割等。</p>
<p>语义分割是一种视觉场景理解任务，它从像素水平上理解、识别图片内容，然后根据语义信息进行图像分割；它是一种稠密标签，目的是预测输入图片中每一个像素的类别标签。</p>
<p>现有语义分割技术会出现分割图像边缘粗糙的现象，这在应用领域造成了一定的影响，尤其在对分割精确度要求较高的医学领域；而且在语义分割模型训练的过程中主要依赖人工标注的数据集样本，耗费较多的人力物力。因此，如何对分割图像边缘粗糙问题进行处理，提高分割精确度；如何降低语义分割模型训练对人工标注的数据集样本的依赖程度，提升网络的泛化能力，对语义分割技术的广泛应用具有非常重要的现实意义。</p>
<h1 id="图像语义分割综述"><a href="#图像语义分割综述" class="headerlink" title="图像语义分割综述"></a>图像语义分割综述</h1><p>转载自<a href="https://zhuanlan.zhihu.com/p/37801090" target="_blank" rel="noopener">这儿</a></p>
<h2 id="什么是语义分割"><a href="#什么是语义分割" class="headerlink" title="什么是语义分割"></a>什么是语义分割</h2><p>语义分割是在像素级别上的分类，属于同一类的像素都要被归为一类，因此语义分割是从像素级别来理解图像的。比如说如下的照片，属于人的像素都要分成一类，属于摩托车的像素也要分成一类，除此之外还有背景像素也被分为一类。注意语义分割不同于实例分割，举例来说，如果一张照片中有多个人，对于语义分割来说，只要将所由人的像素都归为一类，但是实例分割还要将不同人的像素归为不同的类。也就是说实例分割比语义分割更进一步。</p>
<h2 id="语义分割的思路"><a href="#语义分割的思路" class="headerlink" title="语义分割的思路"></a>语义分割的思路</h2><h3 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h3><p>在深度学习方法流行之前，TextonForest和基于随机森林分类器等语义分割方法是用得比较多的方法。不过在深度卷积网络流行之后，深度学习方法比传统方法提升了很多，所以这里就不详细讲传统方法了。</p>
<h3 id="深度学习方法"><a href="#深度学习方法" class="headerlink" title="深度学习方法"></a>深度学习方法</h3><p>深度学习方法在语义分割上得到了巨大成功，深度学习方法解决语义分割问题可以概括为几种思路。下面进行详细介绍。</p>
<p>1.Patch classification<br>最初的深度学习方法应用于图像分割就是Patch classification。Patch classification方法，顾名思义，图像是切成块喂给深度模型的，然后对像素进行分类。使用图像块的主要原因是全连接层需要固定大小的图像。</p>
<p>2.全卷积方法<br>2014年，全卷积网络（FCN）横空出世，FCN将网络全连接层用卷积取代，因此使任意图像大小的输入都变成可能，而且速度比Patch classification方法快很多。</p>
<p>尽管移除了全连接层，但是CNN模型用于语义分割还存在一个问题，就是下采样操作（比如，pooling）。pooling操作可以扩大感受野因而能够很好地整合上下文信息（context中文称为语境或者上下文，通俗的理解就是综合了更多的信息来进行决策），对high-level的任务（比如分类），这是很有效的。但同时，由于pooling下采样操作，使得分辨率降低，因此削弱了位置信息，而语义分割中需要score map和原图对齐，因此需要丰富的位置信息。</p>
<p>3.encoder-decoder架构<br>encoder-decoder是基于FCN的架构。encoder由于pooling逐渐减少空间维度，而decoder逐渐恢复空间维度和细节信息。通常从encoder到decoder还有shortcut connetction（捷径连接，也就是跨层连接）。其中U-net就是这种架构很流行的一种，如下图：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216121702929.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216121702929.png#pic_center</a> =500x300)<br>4.空洞卷积<br>dilated/atrous （空洞卷积）架构，这种结构代替了pooling，一方面它可以保持空间分辨率，另外一方面它由于可以扩大感受野因而可以很好地整合上下文信息。如下图：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216121135445.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216121135445.png#pic_center</a> =500x300)<br>5.条件随机场<br>除了以上思路，还有一种对分割结果进行后处理的方法，那就是条件随机场(Conditional Random Fields (CRFs))后处理用来改善分割效果。DeepLab系列文章基本都采用这种后处理方法，可以较好地改善分割结果，如下图：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216121319384.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216121319384.png#pic_center</a> =500x300)</p>
<h2 id="深度学习语义分割方法"><a href="#深度学习语义分割方法" class="headerlink" title="深度学习语义分割方法"></a>深度学习语义分割方法</h2><p>现在的深度学习语义分割模型基本上都是基于FCN发展而来的，它是开山鼻祖，一张图概括FCN的延伸方法：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216121544933.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216121544933.png#pic_center</a> =500x300)<br>各方法的详细信息<br><img src="https://img-blog.csdnimg.cn/20200216121838301.png" alt="在这里插入图片描述"></p>
<h3 id="各方法的简要介绍"><a href="#各方法的简要介绍" class="headerlink" title="各方法的简要介绍"></a>各方法的简要介绍</h3><p>下面简单总结一些从FCN进行改进的几种架构，关于每种架构的详细解读请看专栏中其他文章。</p>
<h4 id="1-FCN"><a href="#1-FCN" class="headerlink" title="1.FCN"></a>1.FCN</h4><blockquote>
<p>Fully Convolutional Networks for Semantic Segmentation<br>Submitted on 14 Nov 2014</p>
</blockquote>
<p>主要贡献</p>
<p>1.使端对端的卷积语义分割网络变得流行起来。<br>2.通过deconvolutional layers进行上采样。<br>3.通过skip connection改善了上采样的粗糙度。</p>
<p>概要</p>
<p>1.<strong>全卷积化(Fully Convolutional)</strong>：用于解决逐像素(pixel-wise)的预测问题。通过将基础网络(例如VGG)最后面几个全连接层换成卷积层，可实现任意大小的图像输入，并且输出图像大小与输入相对应；<br>2.<strong>反卷积(deconvolution)</strong> ：上采样操作，用于恢复图片尺寸，方便后续进行逐像素预测;<br>3.<strong>跳跃结构(skip architecture)</strong>：用于融合高低层特征信息。通过跨层连接的结构，结合了网络浅层的细(fine-grain)粒度信息信息以及深层的粗糙(coarse)信息，以实现精准的分割任务。</p>
<p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216123101449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216123101449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =500x300)<br>FCN是基于深度学习的语义分割的开山之作，尽管现在很多方法都超越了FCN，但它的思想仍然有很重要的意义。</p>
<h4 id="2-Segnet"><a href="#2-Segnet" class="headerlink" title="2. Segnet"></a>2. Segnet</h4><blockquote>
<p>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation<br>Submitted on 2 Nov 2015</p>
</blockquote>
<p>主要贡献</p>
<p>使用Maxpooling indices来增强位置信息。</p>
<p>简要概述</p>
<p>FCN的upconvolution层+shortcut connections产生的分割图比较粗糙，因此SegNet增加了更多的shortcut connections。不过，SegNet并不是直接将encoder的特征进行直接复制，而是对maxpooling中的indices进行复制，这使得SegNet的效率更高。</p>
<p>maxpooling 的indices复制原理如下：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216220438471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216220438471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =300x)<br>FCN和SegNet都是encoder-decoder架构。<br>SegNet的benchmark表现太差了，不建议用这个网络。</p>
<h4 id="3-Dilated-convolution"><a href="#3-Dilated-convolution" class="headerlink" title="3. Dilated convolution"></a>3. Dilated convolution</h4><p>论文信息</p>
<blockquote>
<p>Multi-Scale Context Aggregation by Dilated Convolutions<br>Submitted on 23 Nov 2015</p>
</blockquote>
<p>创新点</p>
<p>1.使用空洞卷积用来进行稠密预测（dense prediction）。<br>2.提出上下文模块（context module），使用空洞卷积（Dilated Convolutions）来进行多尺度信息的的整合。</p>
<p>简要解释</p>
<p>pooling操作可以增大感受野，对于图像分类任务来说这有很大好处，但由于pooling操作降低了分辨率，这对语义分割来说很不利。因此作者提出一种叫做dilated convolution的操作来解决这个问题。dilated卷积(在deeplab中称为atrous卷积)。可以很好地提升感受野的同时可以保持空间分辨率。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216220838429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216220838429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =500x)<br>网络架构有两种，一种是前端网络，另外一种是前端网络+上下文模块，分别介绍如下：</p>
<p>将VGG网络的最后两个pooling层给拿掉了，之后的卷积层被dilated 卷积取代。并且在pool3和pool4之间空洞卷积的空洞率=2，pool4之后的空洞卷积的空洞率=4。作者将这种架构称为前端（front-end）。</p>
<p>除了前端网络之外，作者还设计了一种叫做上下文模块（context module）的架构，加在前端网络之后。上下文模块中级联了多种不同空洞率的空洞卷积，使得多尺度的上下文信息可以得到整合，从而改善前端网络预测的效果。需要注意的是前端网络和上下文模块是分开训练的，因为作者在实验中发现，如果是联合在一起进行端对端的训练并不能改善性能。</p>
<p>需要特别注意的是，网络输出的分割图并不是和原始图像大小一样的，而是其1/8，需要对输出的分割图进行线性插值才能得到最终的分割结果。这种做法也是很多其他的方法都使用的。</p>
<h4 id="4-DeepLab-v1-v2"><a href="#4-DeepLab-v1-v2" class="headerlink" title="4. DeepLab(v1,v2)"></a>4. DeepLab(v1,v2)</h4><p><strong>论文信息</strong></p>
<blockquote>
<p>v1: Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs<br>Submitted on 22 Dec 2014<br>v2 : DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs<br>Submitted on 2 Jun 2016</p>
</blockquote>
<p><strong>主要贡献</strong></p>
<p>1.使用atrous卷积，也就是后来的空洞卷积，扩大感受野，保持分辨率。<br>2.提出了atrous spatial pyramid pooling (ASPP)，整合多尺度信息。<br>3.使用全连接条件随机场（fully connected CRF)进行后处理，改善分割结果。</p>
<p><strong>简要概述</strong></p>
<p>1.空洞卷积可以在不增加参数的情况下增加感受野。<br>2.通过两种方式来进行多尺度的处理：A.将原始图像的多种尺度喂给网络进行训练。B.通过平行的不同空洞率的空洞卷积层来获得。<br>3.通过全连接条件随机场来进行后处理，以改善分割结果。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216221235418.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216221235418.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =700x)</p>
<h4 id="5-RefineNet"><a href="#5-RefineNet" class="headerlink" title="5.RefineNet"></a>5.RefineNet</h4><p><strong>论文信息</strong></p>
<blockquote>
<p>RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation<br>Submitted on 20 Nov 2016</p>
</blockquote>
<p><strong>主要贡献</strong></p>
<p>精心设计了encoder-decoder架构中的decoder部分，使得性能提升。<br>整个网络的设计都遵循residual connections，网络表达能力更强，梯度更容易反向传播。</p>
<p><strong>简要概述</strong></p>
<p>作者提出空洞卷积方法应用于语义分割也是有缺点的，包括：</p>
<p>因为使用了大分辨率的feature map，因此计算代价大，并且需要大量的内存。对于这个问题，DeepLab的做法是只预测原始输入的1／8。<br>本文提出使用encoder-decoder架构。encoder部分是RESNET-101。decoder具有RefineNet blocks，它将此前的RefineNet blocks的低分辨率特征和encoder部分高分辨率特征进行concatenate/fuse。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216221557410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216221557410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =700x)<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216221623130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216221623130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =700x)</p>
<h4 id="6-PSPNet"><a href="#6-PSPNet" class="headerlink" title="6. PSPNet"></a>6. PSPNet</h4><p>论文信息</p>
<blockquote>
<p>Pyramid Scene Parsing Network<br>Submitted on 4 Dec 2016</p>
</blockquote>
<p><strong>主要贡献</strong></p>
<p>使用pyramid pooling整合context。<br>使用auxiliary loss。</p>
<p><strong>概要</strong></p>
<p>骨架网络使用Resnet，并在此基础上加上pyramid pooling module。该模块用到了很多kernel大小不一的pooling 。将pooling的结果再上采样，经过concatenate进行融合。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216221725398.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216221725398.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =700x)<br>在RESNET的第四阶段（即输入到金字塔池模块）之后，应用auxiliary loss。这种方法在别的地方也被称为intermediate supervision。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/%E3%80%8A%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%E3%80%8B%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/%E3%80%8A%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%E3%80%8B%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">《视觉SLAM十四讲》笔记</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:55:16 / Modified: 12:55:22" itemprop="dateCreated datePublished" datetime="2020-03-05T12:55:16+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-视觉SLAM-系统概述"><a href="#1-视觉SLAM-系统概述" class="headerlink" title="1. 视觉SLAM 系统概述"></a>1. 视觉SLAM 系统概述</h1><p>SLAM 是Simultaneous Localization and Mapping 的缩写，中文译作“同时定位与地图构建” 。它是指搭载特定传感器的主体，在没有环境先验信息的情况下，于运动过程中建立环境的模型，同时估计自己的运动。如果这里的传感器主要为相机，那就称为“视觉SLAM”。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216222952173.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216222952173.png#pic_center</a> =300x)<br>视觉SLAM流程分为以下几步</p>
<ul>
<li>传感器信息读取。在视觉SLAM 中主要为相机图像信息的读取和预处理。<ul>
<li>视觉里程计。视觉里程计任务是估算相邻图像间相机的运动，以及局部地图的样子。</li>
<li>后端优化。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行优化，得到全局一致的轨迹和地图。</li>
<li>回环检测。回环检测判断机器人是否曾经到达过先前的位置。如果检测到回环，它会把信息提供给后端进行处理。</li>
<li>建图。它根据估计的轨迹，建立与任务要求对应的地图。<h1 id="2-前端视觉里程计"><a href="#2-前端视觉里程计" class="headerlink" title="2. 前端视觉里程计"></a>2. 前端视觉里程计</h1>视觉里程计根据相邻图像的信息，估计出粗略的相机运动，给后端提供较好的初始值。视觉里程计的算法主要分为两个大类：特征点法和直接法。基于特征点法的前端，长久以来（直到现在）被认为是视觉里程计的主流方法。它运行稳定，对光照、动态物体不敏感，是目前比较成熟的解决方案。<br>核心问题：如何根据图像估计相机运动。</li>
</ul>
</li>
</ul>
<h2 id="2-1-特征点法"><a href="#2-1-特征点法" class="headerlink" title="2.1. 特征点法"></a>2.1. 特征点法</h2><p>特征点：由关键点和描述子两部分组成。关键点是指该特征点在图像里的位置，有些特征点还具有朝向、大小等信息。描述子通常是一个向量，按照某种人为设计的方式，描述了该关键点周围像素的信息。描述子是按照“外观相似的特征应该有相似的描述子”的原则设计的。</p>
<p>特征匹配：视觉SLAM 中极为关键的一步，特征匹配解决了SLAM 中的数据关联问题，即确定当前看到的路标与之前看到的路标之间的对应关系。通过对图像与图像，或者图像与地图之间的描述子进行准确的匹配，我们可以为后续的姿态估计，优化等操作减轻大量负担。匹配方法：暴力匹配等。</p>
<p>当相机为单目时，我们只知道2D 的像素坐标，因而问题是根据两组2D 点估计运动。该问题用对极几何来解决。</p>
<p>当相机为双目、RGB-D 时，或者我们通过某种方法得到了距离信息，那问题就是根据两组3D 点估计运动。该问题通常用ICP 来解决。</p>
<p>如果我们有3D 点和它们在相机的投影位置，也能估计相机的运动。该问题通过PnP求解。</p>
<h3 id="2-1-1-2D-2D-对极几何"><a href="#2-1-1-2D-2D-对极几何" class="headerlink" title="2.1.1. 2D-2D: 对极几何"></a>2.1.1. 2D-2D: 对极几何</h3><p>假设我们从两张图像中，得到了一对配对好的特征点，如果我们有若干对这样的匹配点，就可以通过这些二维图像点的对应关系，恢复出在两帧之间摄像机的运动。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216223504198.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216223504198.png#pic_center</a> =300x)<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216223545251.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216223545251.png#pic_center</a> =300x)<br>八点法+奇异值分解即可求得旋转矩阵R和位移向量t。</p>
<h3 id="2-1-2-三角测量"><a href="#2-1-2-三角测量" class="headerlink" title="2.1.2. 三角测量"></a>2.1.2. 三角测量</h3><p>在得到运动之后，下一步我们需要用相机的运动估计特征点的空间位置。在单目SLAM 中，仅通过单张图像无法获得像素的深度信息，我们需要通过三角测量（Triangulation）（或三角化）的方法来估计地图点的深度。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216223738272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216223738272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =300x)![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216223807598.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216223807598.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =300x)</p>
<h3 id="2-1-3-3D-2D-PnP"><a href="#2-1-3-3D-2D-PnP" class="headerlink" title="2.1.3.  3D-2D: PnP"></a>2.1.3.  3D-2D: PnP</h3><p>PnP（Perspective-n-Point）是求解3D 到2D 点对运动的方法。它描述了当我们知道n 个3D 空间点以及它们的投影位置时，如何估计相机所在的位姿</p>
<p>如果两张图像中，其中一张特征点的3D 位置已知，那么最少只需三个点对（需要至少一个额外点验证结果）就可以估计相机运动</p>
<p>在双目或RGB-D 的视觉里程计中，我们可以直接使用PnP 估计相机运动。而在单目视觉里程计中，必须先进行初始化，然后才能使用PnP</p>
<p>PnP 问题有很多种求解方法，例如用三对点估计位姿的P3P，直接线性变换（DLT），非线性优化构建最小二乘问题并迭代求解</p>
<h3 id="2-1-4-3D-3D-ICP"><a href="#2-1-4-3D-3D-ICP" class="headerlink" title="2.1.4. 3D-3D: ICP"></a>2.1.4. 3D-3D: ICP</h3><p>假设一组配对好的3D 点（比如对两个RGB-D 图像进行了匹配）：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216224001316.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216224001316.png#pic_center</a> =300x)<br>现在，找一个欧氏变换R; t，使得：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216224025260.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216224025260.png#pic_center</a> =300x)<br>这个问题可以用迭代最近点（Iterative Closest Point, ICP）求解<br>ICP 的求解也分为两种方式：利用线性代数的求解（主要是SVD），以及利用非线性优化方式的求解（类似于Bundle Adjustment）。</p>
<h3 id="2-1-5-特征点法的缺陷"><a href="#2-1-5-特征点法的缺陷" class="headerlink" title="2.1.5. 特征点法的缺陷"></a>2.1.5. 特征点法的缺陷</h3><p><strong>特征点法存在的问题：</strong></p>
<ul>
<li>关键点的提取与描述子的计算非常耗时</li>
<li>使用特征点时，忽略了除特征点以外的所有信息。一张图像有几十万个像素，而特征点只有几百个。只使用特征点丢弃了大部分可能有用的图像信息。</li>
<li>相机有时会运动到特征缺失的地方，往往这些地方没有明显的纹理信息。</li>
</ul>
<p><strong>克服对策：</strong></p>
<ul>
<li>只计算关键点，不计算描述子。同时，使用光流法（Optical Flow）来跟踪特征点的运动。这样可以回避计算和匹配描述子带来的时间，而光流本身的计算时间要小于特征点的计算与匹配。</li>
<li>只计算关键点，不计算描述子。同时，使用直接法（Direct Method）来计算特征点在下一时刻图像的位置。这同样可以跳过描述子的计算过程，而且直接法的计算更加简单。</li>
<li><h2 id="2-2-光流法"><a href="#2-2-光流法" class="headerlink" title="2.2. 光流法"></a>2.2. 光流法</h2>光流法仍然使用特征点，只是把匹配描述子替换成了光流跟踪，估计相机运动时仍使用对极几何、PnP 或ICP 算法。</li>
</ul>
<p>光流是一种描述像素随着时间，在图像之间运动的方法，计算部分像素运动的称为稀疏光流，计算所有像素的称为稠密光流。</p>
<p>LK光流是光流法的一种，它对观测量做了“灰度不变”假设和“某个窗口内的像素具有相同的运动”假设。因而能够从前后两幅图片中追踪到同一个点的位置移动。</p>
<p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216224237540.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216224237540.png#pic_center</a> =300x)<br>在实际应用中，LK光流的作用就是跟踪特征点。与对每一帧提取特征点相比，使用LK光流只需要提取一次特征点，后续视频帧只需要跟踪就可以了，节约了许多特征提取时间。</p>
<h2 id="2-3-直接法"><a href="#2-3-直接法" class="headerlink" title="2.3. 直接法"></a>2.3. 直接法</h2><p>在直接法中，根据图像的像素灰度信息同时估计相机的运动和点的投影，不要求提取到的点必须为角点。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216224336943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216224336943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =400x)<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216224410575.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216224410575.png#pic_center</a> =400x)<br>直接法的思路是根据当前相机的位姿估计值，来寻找p2 的位置。但若相机位姿不够好，p2 的外观和p1 会有明显差别。于是，为了减小这个差别，我们优化相机的位姿，来寻找与p1 更相似的p2。光度误差（Photometric Error），也就是P 的两个像的亮度误差：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/2020021622444270.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/2020021622444270.png#pic_center</a> =300x)<br>优化目标为该误差的二范数</p>
<p>能够做这种优化的理由，仍是基于灰度不变假设。在直接法中，假设一个空间点在各个视角下，成像的灰度是不变的。有许多个（比如N 个）空间点Pi，那么，整个相机位姿估计问题变为：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216224522325.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216224522325.png#pic_center</a> =400x)<br>然后使用G-N 或L-M 计算增量，迭代求解。</p>
<h1 id="3-后端优化"><a href="#3-后端优化" class="headerlink" title="3. 后端优化"></a>3. 后端优化</h1><p>前端视觉里程计能给出一个短时间内的轨迹和地图，但由于不可避免的误差累积，这个地图在长时间内是不准确的。所以，在视觉里程计的基础上，我们还希望构建一个尺度、规模更大的优化问题，以考虑长时间内的最优轨迹和地图。</p>
<h2 id="3-1-线性系统和卡尔曼滤波KF"><a href="#3-1-线性系统和卡尔曼滤波KF" class="headerlink" title="3.1. 线性系统和卡尔曼滤波KF"></a>3.1. 线性系统和卡尔曼滤波KF</h2><p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/2020021718160174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/2020021718160174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =400x)</p>
<h2 id="3-2-非线性系统和扩展的卡尔曼滤波EKF"><a href="#3-2-非线性系统和扩展的卡尔曼滤波EKF" class="headerlink" title="3.2. 非线性系统和扩展的卡尔曼滤波EKF"></a>3.2. 非线性系统和扩展的卡尔曼滤波EKF</h2><p>把卡尔曼滤波器的结果拓展到非线性系统中来，称为扩展卡尔曼滤波器（ExtendedKalman Filter，EKF）。通常的做法是，在某个点附近考虑运动方程以及观测方程的一阶泰勒展开，只保留一阶项，即线性的部分，然后按照线性系统进行推导。</p>
<p>先定义一个卡尔曼增益Kk：</p>
<p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200217181656173.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200217181656173.png#pic_center</a> =300x)<br>在卡尔曼增益的基础上，后验概率的形式为：<br><img src="https://img-blog.csdnimg.cn/20200217181722547.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="3-3-光束法平差BA"><a href="#3-3-光束法平差BA" class="headerlink" title="3.3. 光束法平差BA"></a>3.3. 光束法平差BA</h2><p>所谓的Bundle Adjustment，是指从视觉重建中提炼出最优的3D 模型和相机参数（内参数和外参数）。从每一个特征点反射出来的几束光线（bundles of light rays），在我们把相机姿态和特征点空间位置做出最优的调整(adjustment) 之后，最后收束到相机光心的这个过程，简称为BA。<br><img src="https://img-blog.csdnimg.cn/20200217181837888.png#pic_center" alt="在这里插入图片描述"><br>左侧的p 是全局坐标系下的三维坐标点，右侧的us， vs 是该点在图像平面上的最终像素坐标。<br>系统的观测方程为：</p>
<p>z=h(T,p)</p>
<p>其中，T为相机的位姿变换矩阵,其对应的李代数为ξ。<br>则以最小二乘的角度考虑，可得此次观测的误差：</p>
<p>e=z-h(T,p)</p>
<p>然后，把其他时刻的观测量也考虑进来，我们可以给误差添加一个下标。设zij 为在位姿ξi 处观察路标pj 产生的数据，那么整体的代价函数（Cost Function）为：<br><img src="https://img-blog.csdnimg.cn/20200217181939854.png#pic_center" alt="在这里插入图片描述"><br>对这个最小二乘进行求解，相当于对位姿和路标同时作了调整，也就是所谓的BA。</p>
<h2 id="3-4-位姿图优化"><a href="#3-4-位姿图优化" class="headerlink" title="3.4. 位姿图优化"></a>3.4. 位姿图优化</h2><p>构建一个只有轨迹的图优化，而位姿节点之间的边，可以由两个关键帧之间通过特征匹配之后得到的运动估计来给定初始值。一旦初始估计完成，我们就不再优化那些路标点的位置，而只关心所有的相机位姿之间的联系了。通过这种方式，我们省去了大量的特征点优化的计算，只保留了关键帧的轨迹，从而构建了所谓的位姿图<br><img src="https://img-blog.csdnimg.cn/20200217182110268.png#pic_center" alt="在这里插入图片描述"><br>位姿图优化中的节点表示相机位姿，边表示两个节点之间相对运动的估计。<br>边可表示为<br><img src="https://img-blog.csdnimg.cn/20200217182156248.png#pic_center" alt="在这里插入图片描述"><br>或按李群的写法：<br><img src="https://img-blog.csdnimg.cn/20200217182226320.png#pic_center" alt="在这里插入图片描述"><br>然后构建误差eij：<br><img src="https://img-blog.csdnimg.cn/20200217182249842.png#pic_center" alt="在这里插入图片描述"><br>所有的位姿顶点和位姿——位姿边构成了一个图优化，本质上是一个最小二乘问题，优化变量为各个顶点的位姿，边来自于位姿观测约束。记ε 为所有边的集合，那么总体目标函数为：<br><img src="https://img-blog.csdnimg.cn/20200217182312196.png#pic_center" alt="在这里插入图片描述"><br>我们依然可以用Gauss-Newton、Levenberg-Marquardt 等方法求解此问题，除了用李代数表示优化位姿以外，别的都是相似的。</p>
<h1 id="4-回环检测"><a href="#4-回环检测" class="headerlink" title="4. 回环检测"></a>4. 回环检测</h1><h2 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1. 概述"></a>4.1. 概述</h2><h3 id="4-1-1-回环检测的意义"><a href="#4-1-1-回环检测的意义" class="headerlink" title="4.1.1. 回环检测的意义"></a>4.1.1. 回环检测的意义</h3><p>前端提供特征点的提取和轨迹、地图的初值，而后端负责对这所有的数据进行优化。然而，如果像VO 那样仅考虑相邻时间上的关联，那么，之前产生的误差将不可避免地累计到下一个时刻，使得整个SLAM 会出现累积误差。长期估计的结果将不可靠，或者说，我们无法构建全局一致的轨迹和地图。</p>
<p>回环检测模块，能够给出除了相邻帧之外的，一些时隔更加久远的约束。回环检测的关键，就是如何有效地检测出相机经过同一个地方这件事。如果能够成功地检测这件事，就可以为后端的Pose Graph 提供更多的有效数据，使之得到更好的估计，特别是得到一个全局一致（Global Consistent）的估计。</p>
<h3 id="4-1-2-核心问题："><a href="#4-1-2-核心问题：" class="headerlink" title="4.1.2. 核心问题："></a>4.1.2. 核心问题：</h3><p>如何计算图像间的相似性</p>
<h3 id="4-1-3-准确率和召回率"><a href="#4-1-3-准确率和召回率" class="headerlink" title="4.1.3. 准确率和召回率"></a>4.1.3. 准确率和召回率</h3><p><img src="https://img-blog.csdnimg.cn/20200217182505929.png#pic_center" alt="在这里插入图片描述"><br>准确率：算法提取的所有回环中，确实是真实回环的概率。<br>召回率：在所有真实回环中，被正确检测出来的概率。</p>
<p>为了评价算法的好坏，我们会测试它在各种配置下的P 和R 值，然后做出一条Precision-Recall 曲线。当用召回率为横轴，用准确率为纵轴时，我们会关心整条曲线偏向右上方的程度、100% 准确率下的召回率，或者50% 召回率时候的准确率，作为评价算法的指标。</p>
<p>值得一提的是，在SLAM 中，我们对准确率要求更高，而对召回率则相对宽容一些。由于假阳性的（检测结果是而实际不是的）回环将在后端的Pose Graph 中添加根本错误的边，有些时候会导致优化算法给出完全错误的结果。而相比之下，召回率低一些，则顶多有部分的回环没有被检测到，地图可能受一些累积误差的影响——然而仅需一两次回环就可以完全消除它们了。所以说在选择回环检测算法时，我们更倾向于把参数设置地更严格一些，或者在检测之后再加上回环验证的步骤。</p>
<h2 id="4-2-词袋模型"><a href="#4-2-词袋模型" class="headerlink" title="4.2. 词袋模型"></a>4.2. 词袋模型</h2><p>词袋，也就是Bag-of-Words（BoW），目的是用“图像上有哪几种特征”来描述一个图像。<br>字典中的单词，假设为w1、w2、w3。然后，对于任意图像A，根据它们含有的单词，可记为：<br><img src="https://img-blog.csdnimg.cn/20200217182622842.png#pic_center" alt="在这里插入图片描述"><br>字典是固定的，所以只要用[1  1  0]T 这个向量就可以表达A 的意义。通过字典和单词，只需一个向量就可以描述整张图像了。</p>
<p>同理，用[2  0  1]T 可以描述图像B。如果只考虑“是否出现”而不考虑数量的话，也可以是[1  0  1]T ，这时候这个向量就是二值的。于是，根据这两个向量，设计一定的计算方式，就能确定图像间的相似性了。当然如果对两个向量求差仍然有一些不同的做法，比如说对于a，b∈ RW，可以计算：<br><img src="https://img-blog.csdnimg.cn/20200217182649201.png#pic_center" alt="在这里插入图片描述"><br>其中范数取L1 范数，即各元素绝对值之和。请注意在两个向量完全一样时，我们将得到1；完全相反时（a 为0 的地方b 为1）得到0。这样就定义了两个描述向量的相似性，也就定义了图像之间的相似程度。</p>
<h2 id="4-3-字典创建"><a href="#4-3-字典创建" class="headerlink" title="4.3. 字典创建"></a>4.3. 字典创建</h2><h3 id="4-3-1-K均值算法"><a href="#4-3-1-K均值算法" class="headerlink" title="4.3.1. K均值算法"></a>4.3.1. K均值算法</h3><p>字典由很多单词组成，而每一个单词代表了一个概念。一个单词与一个单独的特征点不同，它不是从单个图像上提取出来的，而是某一类特征的组合。所以，字典生成问题类似于一个聚类问题。当我们有N 个特征点，想要归成k 个类，那么用K-means 来做，主要有以下几个步骤：</p>
<p><img src="https://img-blog.csdnimg.cn/20200217182738524.png" alt="在这里插入图片描述"></p>
<h3 id="4-3-2-k叉树字典"><a href="#4-3-2-k叉树字典" class="headerlink" title="4.3.2. k叉树字典"></a>4.3.2. k叉树字典</h3><p>使用一种k叉树来表达字典。它的思路很简单，类似于层次聚类，是K-means的直接扩展。假定我们有N 个特征点，希望构建一个深度为d，每次分叉为k 的树，那么做法如下：<br><img src="https://img-blog.csdnimg.cn/2020021718280963.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200217182842827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>实际上，最终我们仍在叶子层构建了单词，而树结构中的中间节点仅供快速查找时使用。这样一个k 分支，深度为d 的树，可以容纳kd个单词。另一方面，在查找某个给定特征对应的单词时，只需将它与每个中间结点的聚类中心比较（一共d 次），即可找到最后的单词，保证了对数级别的查找效率。</p>
<h2 id="4-4-相似度计算"><a href="#4-4-相似度计算" class="headerlink" title="4.4. 相似度计算"></a>4.4. 相似度计算</h2><p>考虑权重以后，对于某个图像A，它的特征点可对应到许多个单词，组成它的Bag-of-Words：</p>
<p><img src="https://img-blog.csdnimg.cn/20200217182921656.png#pic_center" alt="在这里插入图片描述"><br>对于给定的VA和VB，通过某些方式即可比较其相似度。如L1范数：<br><img src="https://img-blog.csdnimg.cn/20200217182945286.png#pic_center" alt="在这里插入图片描述"></p>
<h1 id="5-建图"><a href="#5-建图" class="headerlink" title="5. 建图"></a>5. 建图</h1><p>所谓地图，即所有路标点的集合。一旦我们确定了路标点的位置，那就可以说我们完成了建图。SLAM 作为一种底层技术，往往是用来为上层应用提供信息的。应用层面对于“定位”的需求是相似的，他们希望SLAM 提供相机或搭载相机的主体的空间位姿信息。而对于地图，则存在着许多不同的需求。<br><img src="https://img-blog.csdnimg.cn/20200217183024765.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>稀疏地图只建模感兴趣的部分，也就是前面说了很久的特征点（路标点）。<br>稠密地图是指，建模所有看到过的部分。<br>在稠密重建，我们需要知道每一个像素点（或大部分像素点）的距离，那么大致上有以下几种解决方案：</p>
<ol>
<li>使用单目相机，利用移动相机之后进行三角化，测量像素的距离。</li>
<li>使用双目相机，利用左右目的视差计算像素的距离（多目原理相同）。</li>
<li>使用RGB-D 相机直接获得像素距离。<br>前两种方式称为立体视觉（Stereo Vision），其中移动单目的又称为移动视角的立体视觉（Moving View Stereo）。相比于RGB-D 直接测量的深度，单目和双目对深度的获取往往是“费力不讨好”的——我们需要花费大量的计算，最后得到一些不怎么可靠的¬深度估计。当然，RGB-D 也有一些量程、应用范围和光照的限制，不过相比于单目和双目的结果，使用RGB-D 进行稠密重建往往是更常见的选择。而单目双目的好处，是在目前RGB-D还无法很好应用的室外、大场景场合中，仍能通过立体视觉估计深度信息。<h2 id="5-1-单目稠密建图"><a href="#5-1-单目稠密建图" class="headerlink" title="5.1. 单目稠密建图"></a>5.1. 单目稠密建图</h2>在稠密深度图估计中，我们无法把每个像素都当作特征点，计算描述子。因此，稠密深度估计问题中，匹配就成为很重要的一环：如何确定第一张图的某像素，出现在其他图里的位置呢？这需要用到极线搜索和块匹配技术。然后，当我们知道了某个像素在各个图中的位置，就能像特征点那样，利用三角测量确定它的深度。<br><img src="https://img-blog.csdnimg.cn/20200217183125466.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>左边的相机观测到了某个像素p1。由于这是一个单目相机，我们无从知道它的深度，所以假设这个深度可能在某个区域之内，不妨说是某最小值到无穷远之间：(dmin，+∞)。因此，该像素对应的空间点就分布在某条线段（本例中是射线）上。在另一个视角（右侧相机）看来，这条线段的投影也形成图像平面上的一条线，我们知道这称为极线。<br>在p1 周围取一个大小为w * w 的小块，然后在极线上也取很多同样大小的小块进行比较，就可以一定程度上提高区分性。这就是所谓的块匹配。<br>然后计算小块与小块间的差异，存在很多计算方法，如<br><img src="https://img-blog.csdnimg.cn/20200217183155258.png#pic_center" alt="在这里插入图片描述"><br>它计算的是两个小块的相关性，接近0表示两个图像不相似，而接近1表示相似。<br><img src="https://img-blog.csdnimg.cn/2020021718322212.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><h2 id="5-2-RGB-D稠密建图"><a href="#5-2-RGB-D稠密建图" class="headerlink" title="5.2. RGB-D稠密建图"></a>5.2. RGB-D稠密建图</h2>除了使用单目和双目进行稠密重建之外，在适用范围内，RGB-D 相机是一种更好的选择。在RGB-D 相机中可以完全通过传感器中硬件测量得到深度，无需消耗大量的计算资源来估计它们。并且，RGB-D 的结构光或飞时原理，保证了深度数据对纹理的无关性。即使面对纯色的物体，只要它能够反射光，我们就能测量到它的深度。这亦是RGB-D 传感器的一大优势。</li>
</ol>
<p>利用RGB-D 进行稠密建图是相对容易的。不过，根据地图形式不同，也存在着若干种不同的主流建图方式。最直观最简单的方法，就是根据估算的相机位姿，将RGB-D 数据转化为点云（Point Cloud），然后进行拼接，最后得到一个由离散的点组成的点云地图（Point Cloud Map）。在此基础上，如果我们对外观有进一步的要求，希望估计物体的表面，可以使用三角网格（Mesh），面片（Surfel）进行建图。另一方面，如果希望知道地图的障碍物信息并在地图上导航，亦可通过体素（Voxel）建立占据网格地图（Occupancy Map）。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/%E3%80%8ADeepLearning%20with%20Python%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/%E3%80%8ADeepLearning%20with%20Python%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/" class="post-title-link" itemprop="url">《DeepLearning with Python》读书笔记（一）</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:55:01 / Modified: 12:55:08" itemprop="dateCreated datePublished" datetime="2020-03-05T12:55:01+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-什么是深度学习"><a href="#1-什么是深度学习" class="headerlink" title="1.什么是深度学习"></a>1.什么是深度学习</h1><p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200217185528695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200217185528695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =800x)</p>
<h2 id="三张图理解深度学习工作原理"><a href="#三张图理解深度学习工作原理" class="headerlink" title="三张图理解深度学习工作原理"></a>三张图理解深度学习工作原理</h2><p>神经网络中每层对输入数据所做的具体操作保存在该层的权重（weight）中，其本质是一串数字。用术语来说，每层实现的变换由其权重来参数化（parameterize，见图 1-7）。权重有时也被称为该层的参数（parameter）。在这种语境下，学习的意思是为神经网络的所有层找到一组权重值，使得该网络能够将每个示例输入与其目标正确地一一对应。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200217190131517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200217190131517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =500x)<br>想要控制一件事物，首先需要能够观察它。想要控制神经网络的输出，就需要能够衡量该输出与预期值之间的距离。这是神经网络损失函数（loss function）的任务，该函数也叫目标函数（objective function）。损失函数的输入是网络预测值与真实目标值（即你希望网络输出的<br>结果），然后计算一个距离值，衡量该网络在这个示例上的效果好坏<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200217190500415.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200217190500415.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =400x)<br>深度学习的基本技巧是利用这个距离值作为反馈信号来对权重值进行微调，以降低当前示 例对应的损失值（见图 1-9）。这种调节由优化器（optimizer）来完成，它实现了所谓的反向 传播（backpropagation）算法，这是深度学习的核心算法。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200217194424145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200217194424145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =400x)<br>一开始对神经网络的权重随机赋值，因此网络只是实现了一系列随机变换。其输出结果自然也和理想值相去甚远，相应地，损失值也很高。但随着网络处理的示例越来越多，权重值也 在向正确的方向逐步微调，损失值也逐渐降低。这就是训练循环（training loop），将这种循环重复足够多的次数（通常对数千个示例进行数十次迭代），得到的权重值可以使损失函数最小。具有最小损失的网络，其输出值与目标值尽可能地接近，这就是训练好的网络。再次强调，这是一个简单的机制，一旦具有足够大的规模，将会产生魔法般的效果。</p>
<h1 id="2-神经网络的数学基础"><a href="#2-神经网络的数学基础" class="headerlink" title="2.神经网络的数学基础"></a>2.神经网络的数学基础</h1><h2 id="2-2-神经网络的数据表示"><a href="#2-2-神经网络的数据表示" class="headerlink" title="2.2 神经网络的数据表示"></a>2.2 神经网络的数据表示</h2><p>数据存储在多维 Numpy 数组中，也叫张量（tensor）。一般来说，当前所 有机器学习系统都使用张量作为基本数据结构。张量对这个领域非常重要，重要到 Google 的 TensorFlow 都以它来命名。那么什么是张量？ 张量这一概念的核心在于，它是一个数据容器。它包含的数据几乎总是数值数据，因此它 是数字的容器。你可能对矩阵很熟悉，它是二维张量。张量是矩阵向任意维度的推广［注意， 张量的维度（dimension）通常叫作轴（axis）］。</p>
<h3 id="2-2-1标量（0D张量）"><a href="#2-2-1标量（0D张量）" class="headerlink" title="2.2.1标量（0D张量）"></a>2.2.1标量（0D张量）</h3><p>仅包含一个数字的张量叫作标量（scalar，也叫标量张量、零维张量、0D 张量）。在 Numpy 中，一个 float32 或 float64 的数字就是一个标量张量（或标量数组）。标量张量有 0 个轴。张量轴的个数也叫作 阶（rank）。</p>
<h3 id="2-2-2向量（1D张量）"><a href="#2-2-2向量（1D张量）" class="headerlink" title="2.2.2向量（1D张量）"></a>2.2.2向量（1D张量）</h3><p>数字组成的数组叫作向量（vector）或一维张量（1D 张量）。一维张量只有一个轴。下面是一个 Numpy 向量。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200217202516264.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200217202516264.png#pic_center</a> =300x)<br>这个向量有 5 个元素，所以被称为 5D 向量。不要把 5D 向量和 5D 张量弄混！ 5D 向量只 有一个轴，沿着轴有 5 个维度，而 5D 张量有 5 个轴（沿着每个轴可能有任意个维度）。维度 （dimensionality）可以表示沿着某个轴上的元素个数（比如 5D 向量），也可以表示张量中轴的个 数（比如 5D 张量），这有时会令人感到混乱。对于后一种情况，技术上更准确的说法是 5 阶张量 （张量的阶数即轴的个数），但 5D 张量这种模糊的写法更常见。</p>
<h3 id="2-2-3-矩阵（2D张量）"><a href="#2-2-3-矩阵（2D张量）" class="headerlink" title="2.2.3 矩阵（2D张量）"></a>2.2.3 矩阵（2D张量）</h3><p>向量组成的数组叫作矩阵（matrix）或二维张量（2D 张量）。矩阵有 2 个轴（通常叫作行和列）。</p>
<h3 id="2-2-4-3D-张量与更高维张量"><a href="#2-2-4-3D-张量与更高维张量" class="headerlink" title="2.2.4     3D 张量与更高维张量"></a>2.2.4     3D 张量与更高维张量</h3><p>将多个矩阵组合成一个新的数组，可以得到一个 3D 张量，你可以将其直观地理解为数字组成的立方体。将多个 3D 张量组合成一个数组，可以创建一个 4D 张量，以此类推。深度学习处理的一般是 0D 到 4D 的张量，但处理视频数据时可能会遇到 5D 张量。</p>
<h3 id="2-2-5-关键属性"><a href="#2-2-5-关键属性" class="headerlink" title="2.2.5 关键属性"></a>2.2.5 关键属性</h3><p>张量是由以下三个关键属性来定义的。</p>
<ul>
<li><strong>轴的个数（阶）</strong>。例如，3D 张量有 3 个轴，矩阵有 2 个轴。这在 Numpy 等 Python 库中也叫张量的 ndim。</li>
<li><strong>形状</strong>。这是一个整数元组，表示张量沿每个轴的维度大小（元素个数）。例如，前面矩阵示例的形状为 (3, 5)，3D 张量示例的形状为 (3, 3, 5)。向量的形状只包含一个元素，比如 (5,)，而标量的形状为空，即 ()。</li>
<li><strong>数据类型（在 Python 库中通常叫作 dtype）</strong>。这是张量中所包含数据的类型，例如，张量的类型可以是 float32、uint8、float64 等。在极少数情况下，你可能会遇到字符（char）张量。注意，Numpy（以及大多数其他库）中不存在字符串张量，因为张量存储在预先分配的连续内存段中，而字符串的长度是可变的，无法用这种方式存储。<h3 id="2-2-6-在-Numpy-中操作张量"><a href="#2-2-6-在-Numpy-中操作张量" class="headerlink" title="2.2.6　在 Numpy 中操作张量"></a>2.2.6　在 Numpy 中操作张量</h3>我们使用语法 train_images[i] 来选择沿着第一个轴的特定数字。选择张量的特定元素叫作张量切片（tensor slicing）<h3 id="2-2-7-数据批量的概念"><a href="#2-2-7-数据批量的概念" class="headerlink" title="2.2.7　数据批量的概念"></a>2.2.7　数据批量的概念</h3>通常来说，深度学习中所有数据张量的第一个轴（0 轴，因为索引从 0 开始）都是样本轴（samples axis，有时也叫样本维度）。在 MNIST 的例子中，样本就是数字图像。</li>
</ul>
<p>此外，深度学习模型不会同时处理整个数据集，而是将数据拆分成小批量。具体来看，下面是 MNIST 数据集的一个批量，批量大小为 128。<br><img src="https://img-blog.csdnimg.cn/20200217203734895.png" alt="在这里插入图片描述"><br>对于这种批量张量，第一个轴（0 轴）叫作批量轴（batch axis）或批量维度（batch dimension）。在使用 Keras 和其他深度学习库时，你会经常遇到这个术语。</p>
<h3 id="2-2-8-现实世界中的数据张量"><a href="#2-2-8-现实世界中的数据张量" class="headerlink" title="2.2.8　现实世界中的数据张量"></a>2.2.8　现实世界中的数据张量</h3><p>我们用几个你未来会遇到的示例来具体介绍数据张量。你需要处理的数据几乎总是以下类别之一。<br><img src="https://img-blog.csdnimg.cn/20200217203908145.png" alt="在这里插入图片描述"></p>
<h3 id="2-2-9-向量数据"><a href="#2-2-9-向量数据" class="headerlink" title="2.2.9　向量数据"></a>2.2.9　向量数据</h3><p>这是最常见的数据。对于这种数据集，每个数据点都被编码为一个向量，因此一个数据批量就被编码为 2D 张量（即向量组成的数组），其中第一个轴是样本轴，第二个轴是特征轴。</p>
<p>例如，人口统计数据集，其中包括每个人的年龄、邮编和收入。每个人可以表示为包含 3 个值的向量，而整个数据集包含 100 000 个人，因此可以存储在形状为 (100000, 3) 的 2D张量中。</p>
<h3 id="2-2-10-时间序列数据或序列数据"><a href="#2-2-10-时间序列数据或序列数据" class="headerlink" title="2.2.10　时间序列数据或序列数据"></a>2.2.10　时间序列数据或序列数据</h3><p>当时间（或序列顺序）对于数据很重要时，应该将数据存储在带有时间轴的 3D 张量中。每个样本可以被编码为一个向量序列（即 2D 张量），因此一个数据批量就被编码为一个 3D 张量（见图 2-3）。<br><img src="https://img-blog.csdnimg.cn/20200217204620754.png#pic_center" alt="在这里插入图片描述"><br>根据惯例，时间轴始终是第 2 个轴（索引为 1 的轴）。例如，股票价格数据集。每一分钟，我们将股票的当前价格、前一分钟的最高价格和前一分钟的最低价格保存下来。因此每分钟被编码为一个 3D 向量，整个交易日被编码为一个形状为 (390, 3) 的 2D 张量（一个交易日有 390 分钟），而 250 天的数据则可以保存在一个形状为 (250, 390, 3) 的 3D 张量中。这里每个样本是一天的股票数据。</p>
<h3 id="2-2-11-图像数据"><a href="#2-2-11-图像数据" class="headerlink" title="2.2.11　图像数据"></a>2.2.11　图像数据</h3><p>图像通常具有三个维度：高度、宽度和颜色深度。虽然灰度图像（比如 MNIST 数字图像） 只有一个颜色通道，因此可以保存在 2D 张量中，但按照惯例，图像张量始终都是 3D 张量，灰 度图像的彩色通道只有一维。因此，如果图像大小为 256×256，那么 128 张灰度图像组成的批 量可以保存在一个形状为 (128, 256, 256, 1) 的张量中，而 128 张彩色图像组成的批量则 可以保存在一个形状为 (128, 256, 256, 3) 的张量中（见图 2-4）。</p>
<p><img src="https://img-blog.csdnimg.cn/20200217205041235.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>图像张量的形状有两种约定：通道在后（channels-last）的约定（在 TensorFlow 中使用）和 通道在前（channels-first）的约定（在 Theano 中使用）。Google 的 TensorFlow 机器学习框架将 颜色深度轴放在最后：(samples, height, width, color_depth)。与此相反，Theano 将图像深度轴放在批量轴之后：(samples, color_depth, height, width)。如果采 用 Theano 约定，前面的两个例子将变成 (128, 1, 256, 256) 和 (128, 3, 256, 256)。 Keras 框架同时支持这两种格式。</p>
<h3 id="2-2-12-视频数据"><a href="#2-2-12-视频数据" class="headerlink" title="2.2.12　视频数据"></a>2.2.12　视频数据</h3><p>视频数据是现实生活中需要用到 5D 张量的少数数据类型之一。视频可以看作一系列帧， 每一帧都是一张彩色图像。由于每一帧都可以保存在一个形状为 (height, width, color_ depth) 的 3D 张量中，因此一系列帧可以保存在一个形状为 (frames, height, width,  color_depth) 的 4D 张量中，而不同视频组成的批量则可以保存在一个 5D 张量中，其形状为 (samples, frames, height, width, color_depth)。 举个例子，一个以每秒 4 帧采样的 60 秒 YouTube 视频片段，视频尺寸为 144×256，这个 视频共有 240 帧。4 个这样的视频片段组成的批量将保存在形状为 (4, 240, 144, 256, 3) 的张量中。总共有 106 168 320 个值！如果张量的数据类型（dtype）是 float32，每个值都是 32 位，那么这个张量共有 405MB。好大！你在现实生活中遇到的视频要小得多，因为它们不以 float32 格式存储，而且通常被大大压缩，比如 MPEG 格式。</p>
<h2 id="2-3-神经网络的“齿轮”：张量运算"><a href="#2-3-神经网络的“齿轮”：张量运算" class="headerlink" title="2.3 神经网络的“齿轮”：张量运算"></a>2.3 神经网络的“齿轮”：张量运算</h2><h3 id="2-3-6-深度学习的几何解释"><a href="#2-3-6-深度学习的几何解释" class="headerlink" title="2.3.6　深度学习的几何解释"></a>2.3.6　深度学习的几何解释</h3><p>前面讲过，神经网络完全由一系列张量运算组成，而这些张量运算都只是输入数据的几何变换。因此，你可以将神经网络解释为高维空间中非常复杂的几何变换，这种变换可以通过许多简单的步骤来实现。</p>
<p>对于三维的情况，下面这个思维图像是很有用的。想象有两张彩纸：一张红色，一张蓝色。将其中一张纸放在另一张上。现在将两张纸一起揉成小球。这个皱巴巴的纸球就是你的输入数据，每张纸对应于分类问题中的一个类别。神经网络（或者任何机器学习模型）要做的就是找<br>到可以让纸球恢复平整的变换，从而能够再次让两个类别明确可分。通过深度学习，这一过程可以用三维空间中一系列简单的变换来实现，比如你用手指对纸球做的变换，每次做一个动作，如图 2-9 所示。<br><img src="https://img-blog.csdnimg.cn/20200217211411141.png#pic_center" alt="在这里插入图片描述"><br>让纸球恢复平整就是机器学习的内容：为复杂的、高度折叠的数据流形找到简洁的表示。现在你应该能够很好地理解，为什么深度学习特别擅长这一点：它将复杂的几何变换逐步分解为一长串基本的几何变换，这与人类展开纸球所采取的策略大致相同。深度网络的每一层都通<br>过变换使数据解开一点点——许多层堆叠在一起，可以实现非常复杂的解开过程。</p>
<h2 id="2-4-神经网络的“引擎”：基于梯度的优化"><a href="#2-4-神经网络的“引擎”：基于梯度的优化" class="headerlink" title="2.4　神经网络的“引擎”：基于梯度的优化"></a>2.4　神经网络的“引擎”：基于梯度的优化</h2><h1 id="4-机器学习基础"><a href="#4-机器学习基础" class="headerlink" title="4.机器学习基础"></a>4.机器学习基础</h1><h2 id="4-1-机器学习的四个分支"><a href="#4-1-机器学习的四个分支" class="headerlink" title="4.1 机器学习的四个分支"></a>4.1 机器学习的四个分支</h2><p><img src="https://img-blog.csdnimg.cn/20200220170900394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>监督学习</strong></p>
<p>监督学习是目前最常见的机器学习类型。给定一组样本（通常由人工标注），它可以学会将输入数据映射到已知目标［也叫标注（annotation）］。一般来说，近年来广受关注的深度学习应用几乎都属于监督学习，比如光学字符识别、语音识别、图像分类和语言翻译。虽然监督学习主要包括分类和回归，但还有更多的奇特变体，主要包括如下几种。</p>
<ul>
<li>序列生成（sequence generation）。给定一张图像，预测描述图像的文字。序列生成有时可以被重新表示为一系列分类问题，比如反复预测序列中的单词或标记。</li>
<li>语法树预测（syntax tree prediction）。给定一个句子，预测其分解生成的语法树。</li>
<li>目标检测（object detection）。给定一张图像，在图中特定目标的周围画一个边界框。这个问题也可以表示为分类问题（给定多个候选边界框，对每个框内的目标进行分类）或分类与回归联合问题（用向量回归来预测边界框的坐标）。</li>
<li>图像分割（image segmentation）。给定一张图像，在特定物体上画一个像素级的掩模（mask）。</li>
</ul>
<p><strong>无监督学习</strong></p>
<p>无监督学习是指在没有目标的情况下寻找输入数据的有趣变换，其目的在于数据可视化、数据压缩、数据去噪或更好地理解数据中的相关性。无监督学习是数据分析的必备技能，在解决监督学习问题之前，为了更好地了解数据集，它通常是一个必要步骤。降维（dimensionality<br>reduction）和聚类（clustering）都是众所周知的无监督学习方法。</p>
<p>　<strong>自监督学习</strong><br>自监督学习是监督学习的一个特例，它与众不同，值得单独归为一类。自监督学习是没有 人工标注的标签的监督学习，你可以将它看作没有人类参与的监督学习。标签仍然存在（因为 总要有什么东西来监督学习过程），但它们是从输入数据中生成的，通常是使用启发式算法生 成的。 </p>
<p>举个例子，自编码器（autoencoder）是有名的自监督学习的例子，其生成的目标就是未经 修改的输入。同样，给定视频中过去的帧来预测下一帧，或者给定文本中前面的词来预测下一个词， 都是自监督学习的例子［这两个例子也属于时序监督学习（temporally supervised learning），即用 未来的输入数据作为监督］。注意，监督学习、自监督学习和无监督学习之间的区别有时很模糊， 这三个类别更像是没有明确界限的连续体。自监督学习可以被重新解释为监督学习或无监督学 习，这取决于你关注的是学习机制还是应用场景。</p>
<p>　<strong>强化学习</strong><br>　 强化学习一直以来被人们所忽视，但最近随着 Google 的 DeepMind 公司将其成功应用于学 习玩 Atari 游戏（以及后来学习下围棋并达到最高水平），机器学习的这一分支开始受到大量关注。 在强化学习中，智能体（agent）接收有关其环境的信息，并学会选择使某种奖励最大化的行动。 例如，神经网络会“观察”视频游戏的屏幕并输出游戏操作，目的是尽可能得高分，这种神经 网络可以通过强化学习来训练。</p>
<p><strong>分类和回归术语表</strong><br>分类和回归都包含很多专业术语。<br>这些术语在机器学习领域都有确切的定义，你应该了解这些定义。</p>
<ul>
<li>样本（sample）或输入（input）：进入模型的数据点。</li>
<li>预测（prediction）或输出（output）：从模型出来的结果。</li>
<li>目标（target）：真实值。对于外部数据源，理想情况下，模型应该能够预测出目标。</li>
<li>预测误差（prediction error）或损失值（loss value）：模型预测与目标之间的距离。</li>
<li>类别（class）：分类问题中供选择的一组标签。例如，对猫狗图像进行分类时，“狗”和“猫”就是两个类别。</li>
<li>标签（label）：分类问题中类别标注的具体例子。比如，如果 1234 号图像被标注为包含类别“狗”，那么“狗”就是 1234 号图像的标签。</li>
<li>真值（ground-truth）或标注（annotation）：数据集的所有目标，通常由人工收集。</li>
<li>二分类（binary classification）：一种分类任务，每个输入样本都应被划分到两个互斥的类别中。</li>
<li>多分类（multiclass classification）：一种分类任务，每个输入样本都应被划分到两个以上的类别中，比如手写数字分类。</li>
<li>多标签分类（multilabel classification）：一种分类任务，每个输入样本都可以分配多个标签。举个例子，如果一幅图像里可能既有猫又有狗，那么应该同时标注“猫”标签和“狗”标签。每幅图像的标签个数通常是可变的。</li>
<li>标量回归（scalar regression）：目标是连续标量值的任务。预测房价就是一个很好的例子，不同的目标价格形成一个连续的空间。</li>
<li>向量回归（vector regression）：目标是一组连续值（比如一个连续向量）的任务。如果对多个值（比如图像边界框的坐标）进行回归，那就是向量回归。</li>
<li>小批量（mini-batch）或批量（batch）：模型同时处理的一小部分样本（样本数通常为 8~128）。样本数通常取 2 的幂，这样便于 GPU 上的内存分配。训练时，小批量用来为模型权重计算一次梯度下降更新。</li>
</ul>
<h1 id="5-深度学习用于计算机视觉"><a href="#5-深度学习用于计算机视觉" class="headerlink" title="5.深度学习用于计算机视觉"></a>5.深度学习用于计算机视觉</h1><h2 id="5-1-卷积神经网络简介"><a href="#5-1-卷积神经网络简介" class="headerlink" title="5.1　卷积神经网络简介"></a>5.1　卷积神经网络简介</h2><h3 id="5-1-1-卷积运算"><a href="#5-1-1-卷积运算" class="headerlink" title="5.1.1　卷积运算"></a>5.1.1　卷积运算</h3><p>密集连接层和卷积层的根本区别在于，Dense 层从输入特征空间中学到的是全局模式,比如对于 MNIST 数字，全局模式就是涉及所有像素的模式），而卷积层学到的是局部模式（见 图 5-1），对于图像来说，学到的就是在输入图像的二维小窗口中发现的模式。在上面的例子中， 这些窗口的大小都是 3×3。<br><img src="https://img-blog.csdnimg.cn/20200220171521683.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这个重要特性使卷积神经网络具有以下两个有趣的性质。</p>
<ul>
<li>卷积神经网络学到的模式具有<strong>平移不变性（translation invariant）</strong>。卷积神经网络在图像 右下角学到某个模式之后，它可以在任何地方识别这个模式，比如左上角。对于密集连 接网络来说，如果模式出现在新的位置，它只能重新学习这个模式。这使得卷积神经网 络在处理图像时可以高效利用数据（因为视觉世界从根本上具有平移不变性），它只需 要更少的训练样本就可以学到具有泛化能力的数据表示。 <ul>
<li>卷积神经网络可以学到模式的<strong>空间层次结构（spatial hierarchies of patterns）</strong>，见图 5-2。 第一个卷积层将学习较小的局部模式（比如边缘），第二个卷积层将学习由第一层特征 组成的更大的模式，以此类推。这使得卷积神经网络可以有效地学习越来越复杂、越来 越抽象的视觉概念（因为视觉世界从根本上具有空间层次结构）。<br><img src="https://img-blog.csdnimg.cn/20200220171822621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>在 MNIST 示例中，第一个卷积层接收一个大小为 (28, 28, 1) 的特征图，并输出一个大 小为 (26, 26, 32) 的特征图，即它在输入上计算 32 个过滤器。对于这 32 个输出通道，每个 通道都包含一个 26×26 的数值网格，它是过滤器对输入的响应图（response map），表示这个过 滤器模式在输入中不同位置的响应（见图 5-3）。这也是特征图这一术语的含义：深度轴的每个 维度都是一个特征（或过滤器），而 2D 张量 output[:, :, n] 是这个过滤器在输入上的响应 的二维空间图（map）。<br><img src="https://img-blog.csdnimg.cn/20200220172040749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>卷积由以下两个关键参数所定义。</li>
</ul>
</li>
<li>从输入中提取的图块尺寸：这些图块的大小通常是 3×3 或 5×5。本例中为 3×3，这是很常见的选择。</li>
<li>输出特征图的深度：卷积所计算的过滤器的数量。本例第一层的深度为 32，最后一层的深度是 64。</li>
</ul>
<p>卷积的工作原理：在 3D 输入特征图上滑动（slide）这些 3×3 或 5×5 的窗口，在每个可能 的位置停止并提取周围特征的 3D 图块［形状为 (window_height, window_width, input_ depth)］。然后每个 3D 图块与学到的同一个权重矩阵［叫作卷积核（convolution kernel）］做 张量积，转换成形状为 (output_depth,) 的 1D 向量。然后对所有这些向量进行空间重组， 使其转换为形状为 (height, width, output_depth) 的 3D 输出特征图。输出特征图中的 每个空间位置都对应于输入特征图中的相同位置（比如输出的右下角包含了输入右下角的信 息）。举个例子，利用 3×3 的窗口，向量 output[i, j, :] 来自 3D 图块 input[i-1:i+1,  j-1:j+1, :]。整个过程详见下图 。<br><img src="https://img-blog.csdnimg.cn/20200220173236295.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>注意，输出的宽度和高度可能与输入的宽度和高度不同。不同的原因可能有两点。</p>
<ul>
<li>边界效应，可以通过对输入特征图进行填充来抵消。</li>
<li>使用了步幅（stride），稍后会给出其定义。</li>
</ul>
<p><strong>填充</strong><br>如果你希望输出特征图的空间维度与输入相同，那么可以使用填充（padding）。填充是在输入特征图的每一边添加适当数目的行和列，使得每个输入方块都能作为卷积窗口的中心。对于 3×3 的窗口，在左右各添加一列，在上下各添加一行。对于 5×5 的窗口，各添加两行和两<br>列（见图 5-6）。<br><img src="https://img-blog.csdnimg.cn/20200220173424317.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>对于 Conv2D 层，可以通过 padding 参数来设置填充，这个参数有两个取值：”valid” 表示不使用填充（只使用有效的窗口位置）；”same” 表示“填充后输出的宽度和高度与输入相同”。padding 参数的默认值为 “valid”。</p>
<p><strong>卷积步幅</strong><br>两个连续窗口的距离是卷积的一个参数，叫作步幅，默认值为 1。也可以使用步进卷积（strided convolution），即步幅大于 1 的卷积。在图 5-7 中，你可以看到用步幅为 2 的 3×3 卷积从 5×5 输入中提取的图块（无填充）。<br><img src="https://img-blog.csdnimg.cn/20200220173618418.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h3 id="5-1-2-最大池化运算"><a href="#5-1-2-最大池化运算" class="headerlink" title="5.1.2　最大池化运算"></a>5.1.2　最大池化运算</h3><p>最大池化的作用：对特征图进行下采样，与步进卷积类似。<br>最大池化是从输入特征图中提取窗口，并输出每个通道的最大值。它的概念与卷积类似，但是最大池化使用硬编码的 max 张量运算对局部图块进行变换，而不是使用学到的线性变换（卷积核）。最大池化与卷积的最大不同之处在于，最大池化通常使用 2×2 的窗口和步幅 2，其目的是将特征图下采样 2 倍。与此相对的是，卷积通常使用 3×3 窗口和步幅 1。</p>
<p>简而言之，使用下采样的原因，一是减少需要处理的特征图的元素个数，二是通过让连续 卷积层的观察窗口越来越大（即窗口覆盖原始输入的比例越来越大），从而引入空间过滤器的层 级结构。 </p>
<p>注意，最大池化不是实现这种下采样的唯一方法。你已经知道，还可以在前一个卷积层中 使用步幅来实现。此外，你还可以使用平均池化来代替最大池化，其方法是将每个局部输入图 块变换为取该图块各通道的平均值，而不是最大值。但最大池化的效果往往比这些替代方法更好。 简而言之，原因在于特征中往往编码了某种模式或概念在特征图的不同位置是否存在（因此得 名特征图），而观察不同特征的最大值而不是平均值能够给出更多的信息。因此，最合理的子采 样策略是首先生成密集的特征图（通过无步进的卷积），然后观察特征每个小图块上的最大激活， 而不是查看输入的稀疏窗口（通过步进卷积）或对输入图块取平均，因为后两种方法可能导致 错过或淡化特征是否存在的信息。</p>
<h3 id="5-3-3-小结"><a href="#5-3-3-小结" class="headerlink" title="5.3.3　小结"></a>5.3.3　小结</h3><ul>
<li>卷积神经网络是用于计算机视觉任务的最佳机器学习模型。即使在非常小的数据集上也可以从头开始训练一个卷积神经网络，而且得到的结果还不错。</li>
<li>在小型数据集上的主要问题是过拟合。在处理图像数据时，数据增强是一种降低过拟合的强大方法。</li>
<li>利用特征提取，可以很容易将现有的卷积神经网络复用于新的数据集。对于小型图像数据集，这是一种很有价值的方法。</li>
<li>作为特征提取的补充，你还可以使用微调，将现有模型之前学到的一些数据表示应用于新问题。这种方法可以进一步提高模型性能。</li>
<li>现在你已经拥有一套可靠的工具来处理图像分类问题，特别是对于小型数据集。</li>
</ul>
<h2 id="9-2-深度学习的局限性"><a href="#9-2-深度学习的局限性" class="headerlink" title="9.2　深度学习的局限性"></a>9.2　深度学习的局限性</h2><p>深度学习模型只是将一个向量空间映射到另一个向量空间的简单而又连续的几何 变换链。它能做的只是将一个数据流形 X 映射到另一个流形 Y，前提是从 X 到 Y 存在可学习的 连续变换。深度学习模型可以被看作一种程序，但反过来说，大多数程序都不能被表示为深度 学习模型。对于大多数任务而言，要么不存在相应的深度神经网络能够解决任务，要么即使存 在这样的网络，它也可能是不可学习的（learnable）。后一种情况的原因可能是相应的几何变换 过于复杂，也可能是没有合适的数据用于学习。 </p>
<p>通过堆叠更多的层并使用更多训练数据来扩展当前的深度学习技术，只能在表面上缓解一 些问题，无法解决更根本的问题，比如深度学习模型可以表示的内容非常有限，比如大多数你 想要学习的程序都不能被表示为数据流形的连续几何变形。</p>
<p>简而言之，深度学习模型并不理解它们的输入，至少不是人类所说的理解。我们自己对图 像、声音和语言的理解是基于我们作为人类的感觉运动体验。机器学习模型无法获得这些体验， 因此也就无法用与人类相似的方式来理解它们的输入。通过对输入模型的大量训练样本进行标 记，我们可以让模型学会一个简单几何变换，这个变换在一组特定样本上将数据映射到人类概念， 但这种映射只是我们头脑中原始模型的简化。我们头脑中的原始模型是从我们作为具身主体的 体验发展而来的。机器学习模型就像是镜子中的模糊图像（见图 9-3）。<br><img src="https://img-blog.csdnimg.cn/20200220175249156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/Linux%E5%A4%87%E5%BF%98%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/Linux%E5%A4%87%E5%BF%98%E5%BD%95/" class="post-title-link" itemprop="url">Linux备忘录</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:54:34 / Modified: 13:03:17" itemprop="dateCreated datePublished" datetime="2020-03-05T12:54:34+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Ubuntu备忘"><a href="#Ubuntu备忘" class="headerlink" title="Ubuntu备忘"></a>Ubuntu备忘</h1><table>
<thead>
<tr>
<th align="center">指令</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Ctrl+h     或者 ls   -a</td>
<td>显示隐藏文件</td>
</tr>
<tr>
<td align="center">ls -l name.file</td>
<td>查看文件详细信息</td>
</tr>
<tr>
<td align="center">cp source.file target.file -r</td>
<td>复制文件和目录</td>
</tr>
<tr>
<td align="center">mv file1 file2</td>
<td>改名字</td>
</tr>
<tr>
<td align="center">mv flie1 ~/a/b/file2</td>
<td>移动位置</td>
</tr>
<tr>
<td align="center">file  target.file</td>
<td>查看文件类型</td>
</tr>
<tr>
<td align="center">less name.file</td>
<td>以bash手册的形式查看name.file的内容，支持空格、PageDown翻下页，PageUp翻上页，Enter翻下行，q退出。</td>
</tr>
<tr>
<td align="center">touch name.file</td>
<td>若name.file不存在，则创建新文件；若name.file存在，则更新修改时间。</td>
</tr>
<tr>
<td align="center">ps -ef</td>
<td>显示所有进程的扩展信息。</td>
</tr>
<tr>
<td align="center">top</td>
<td>实时监控进程，按q退出,按d设置刷新间隔。</td>
</tr>
<tr>
<td align="center">kill PID</td>
<td>关闭第PID个进程，程度弱，可能被忽略。</td>
</tr>
<tr>
<td align="center">killall name*</td>
<td>强制关闭name开头的所有进程，谨慎使用，使用通配符*时可能关闭系统进程，损坏系统。</td>
</tr>
<tr>
<td align="center">××××××××××</td>
<td>××××××××××××</td>
</tr>
<tr>
<td align="center">sudo dpkg -i package.deb</td>
<td>安装软件包</td>
</tr>
<tr>
<td align="center">sudo apt-get install -f</td>
<td>修复有问题的软件包的依赖项</td>
</tr>
<tr>
<td align="center">grep netease-cloud-music</td>
<td>查看网易云音乐软件包是否已经安装</td>
</tr>
<tr>
<td align="center">sudo apt-get remove netease-cloud-music</td>
<td>卸载网易云音乐</td>
</tr>
<tr>
<td align="center">dpkg -l</td>
<td>显示所有安装了的软件包</td>
</tr>
<tr>
<td align="center">×××××××××××××××</td>
<td>×××××××××××××××××××</td>
</tr>
<tr>
<td align="center"></td>
<td></td>
</tr>
</tbody></table>
<ul>
<li>点击图标实现最小化应用</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gsettings <span class="built_in">set</span> org.compiz.unityshell:/org/compiz/profiles/unity/plugins/unityshell/ launcher-minimize-window <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h1 id="vim备忘"><a href="#vim备忘" class="headerlink" title="vim备忘"></a>vim备忘</h1><table>
<thead>
<tr>
<th>指令</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>Ctrl + f</td>
<td>向下翻整页</td>
</tr>
<tr>
<td>Ctrl + b</td>
<td>向上翻整页</td>
</tr>
<tr>
<td>-  <code>/etc/vim/vimrc</code>      vimrc是vim的配置文件</td>
<td></td>
</tr>
</tbody></table>
<h1 id="shell备忘"><a href="#shell备忘" class="headerlink" title="shell备忘"></a>shell备忘</h1><ul>
<li>查看电脑有哪些shell</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/shells</span><br></pre></td></tr></table></figure>

<h1 id="ros备忘"><a href="#ros备忘" class="headerlink" title="ros备忘"></a>ros备忘</h1><h2 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"source /opt/ros/kinetic/setup.bash"</span> &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>source这个单词，代表驱寻找的意思，后面一长串/opt/ros/kinetic/setup.bash就是ROS本身工作空间环境变量配置脚本文件的路径，&gt;&gt; ~/.bashrc表示将这个环境变量配置脚本写到终端配置文件.bashrc中。</p>
<p>这里出现了一个新的文件，也就是终端配置文件.bashrc，这是我们打开的终端的一个配置文件，配置环境变量就相当于将工作空间的环境变量脚本，包括其路径，记录到这个终端配置文件.bashrc上。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"source /home/twb/catkin_ws/devel/setup.bash"</span> &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>这个代码就是配置我们所创建的工作空间的环境变量，/home/twb/catkin_ws/devel/setup.bash这个是路径。<br>.bashrc中增加内容如下<br><img src="https://img-blog.csdnimg.cn/20200303191501134.png#pic_center" alt="在这里插入图片描述"><br>查看一下现在ROS内部的环境变量有哪些</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$ROS_PACKAGE_PATH</span></span><br></pre></td></tr></table></figure>
<p>结果如下：<br><img src="https://img-blog.csdnimg.cn/20200303191825194.png#pic_center" alt="在这里插入图片描述"><br>冒号前面是我自己创建的工作空间的路径，那个/src文件夹是专门存放源代码和功能包的。冒号后面是ROS本身工作空间下源代码和软件包所存放的文件夹的路径。</p>
<h1 id="Python备忘"><a href="#Python备忘" class="headerlink" title="Python备忘"></a>Python备忘</h1><ul>
<li>安装 anaconda 后Linux的终端界面前部出现（base）字样</li>
</ul>
<p>1.打开一个终端 ，输入命令：vim ～/.bashrc<br>2.在 .bashrc文件最后面添加命令：conda deactivate</p>
<p>再重新打开终端即可消除base字样</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/Python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/Python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">Python基础知识学习</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:52:30 / Modified: 12:52:37" itemprop="dateCreated datePublished" datetime="2020-03-05T12:52:30+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-Python历史"><a href="#1-Python历史" class="headerlink" title="1.Python历史"></a>1.Python历史</h1><p><img src="https://img-blog.csdnimg.cn/20200229200935152.png#pic_center" alt="在这里插入图片描述"></p>
<h1 id="2-基础环境准备"><a href="#2-基础环境准备" class="headerlink" title="2.基础环境准备"></a>2.基础环境准备</h1><p>Python环境安装<br>windows/macOS<br>1.访问<a href="https://www.python.org/downloads/" target="_blank" rel="noopener">https://www.python.org/downloads/</a><br><img src="https://img-blog.csdnimg.cn/20200229202642279.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>2.下载合适的版本。<br>3.点击安装<br>Linux<br>1.多数系统自带<br>2.找对应系统手册</p>
<h2 id="Python运行交互界面"><a href="#Python运行交互界面" class="headerlink" title="Python运行交互界面"></a>Python运行交互界面</h2><p><img src="https://img-blog.csdnimg.cn/20200229203116113.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="Jupyter-演示界面"><a href="#Jupyter-演示界面" class="headerlink" title="Jupyter 演示界面"></a>Jupyter 演示界面</h2><p>Ubuntu安装一个Jupyter包，也可以直接装anaconda<br>windows安装anaconda<br><img src="https://img-blog.csdnimg.cn/20200229203356397.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200229203957622.png#pic_center" alt="在这里插入图片描述"><br>出现以上结果表示Jupyter安装正确。</p>
<h2 id="文件编辑和执行"><a href="#文件编辑和执行" class="headerlink" title="文件编辑和执行"></a>文件编辑和执行</h2><p><img src="https://img-blog.csdnimg.cn/202002292044197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200229204516101.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="3-基本语法"><a href="#3-基本语法" class="headerlink" title="3.基本语法"></a>3.基本语法</h1><p><img src="https://img-blog.csdnimg.cn/2020022920485673.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020022920534126.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200229205403862.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200229205446559.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200229211441195.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200229211737886.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200229212456619.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="4-变量"><a href="#4-变量" class="headerlink" title="4.变量"></a>4.变量</h1><p><img src="https://img-blog.csdnimg.cn/20200303092421876.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303092713279.png" alt="在这里插入图片描述"></p>
<h1 id="5-算符和表达式"><a href="#5-算符和表达式" class="headerlink" title="5.算符和表达式"></a>5.算符和表达式</h1><p><img src="https://img-blog.csdnimg.cn/20200303095319626.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303100031262.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303100247207.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303100610446.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020030310062682.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303100803414.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303100850807.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303100945136.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303101139854.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303101216539.png" alt="在这里插入图片描述"></p>
<h1 id="6-执行流控制"><a href="#6-执行流控制" class="headerlink" title="6.执行流控制"></a>6.执行流控制</h1><p>执行流控制代码控制流的先后执行顺序。<br><img src="https://img-blog.csdnimg.cn/20200303151432425.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303151722678.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303151817935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303151946775.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303152139318.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303152738204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303153232281.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/202003031534031.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303153837665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303154246410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020030315430226.png" alt="在这里插入图片描述"></p>
<h1 id="7-函数"><a href="#7-函数" class="headerlink" title="7.函数"></a>7.函数</h1><p><img src="https://img-blog.csdnimg.cn/20200303154633153.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020030315473271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="8-局部变量"><a href="#8-局部变量" class="headerlink" title="8.局部变量"></a>8.局部变量</h1><p><img src="https://img-blog.csdnimg.cn/2020030315534579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303155431641.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/202003031600052.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303160047161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="9-作用域"><a href="#9-作用域" class="headerlink" title="9.作用域"></a>9.作用域</h1><p><img src="https://img-blog.csdnimg.cn/20200303160252129.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303160401877.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="10-参数默认值"><a href="#10-参数默认值" class="headerlink" title="10.参数默认值"></a>10.参数默认值</h1><p><img src="https://img-blog.csdnimg.cn/20200303160540754.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020030316101898.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200303160952445.png" alt="在这里插入图片描述"></p>
<h1 id="11-返回值"><a href="#11-返回值" class="headerlink" title="11.返回值"></a>11.返回值</h1><p><img src="https://img-blog.csdnimg.cn/2020030409391573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304095134437.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304095247136.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304095739936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="12-lambda"><a href="#12-lambda" class="headerlink" title="12.lambda"></a>12.lambda</h1><p><img src="https://img-blog.csdnimg.cn/20200304100034899.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304100616195.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304100638820.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="13-模块"><a href="#13-模块" class="headerlink" title="13.模块"></a>13.模块</h1><p><img src="https://img-blog.csdnimg.cn/20200304100856774.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020030410151827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304101714709.png" alt="在这里插入图片描述"></p>
<h1 id="14-from-import"><a href="#14-from-import" class="headerlink" title="14.from import"></a>14.from import</h1><p><img src="https://img-blog.csdnimg.cn/20200304102217804.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304102614102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="15-模块预编译"><a href="#15-模块预编译" class="headerlink" title="15.模块预编译"></a>15.模块预编译</h1><p><img src="https://img-blog.csdnimg.cn/20200304103511258.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304104835741.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="16-main文件模式化写法"><a href="#16-main文件模式化写法" class="headerlink" title="16.main文件模式化写法"></a>16.main文件模式化写法</h1><p><img src="https://img-blog.csdnimg.cn/20200304105422145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304105839284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304110512926.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> dir(sys):</span><br><span class="line">    <span class="keyword">if</span> name.startswith(<span class="string">'s'</span>) <span class="keyword">and</span> name.endswith(<span class="string">'e'</span>):</span><br><span class="line">        print(name)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200304110638682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="17-内置数据结构"><a href="#17-内置数据结构" class="headerlink" title="17.内置数据结构"></a>17.内置数据结构</h1><p><img src="https://img-blog.csdnimg.cn/20200304112932468.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304113025241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304113240915.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304113415742.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304113925589.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304114651931.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304114829315.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304114849998.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304115002530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200304115036900.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/%E6%94%B9%E5%8F%98%E7%94%9F%E6%B4%BB%E6%96%B9%E5%BC%8F%E4%BB%8E%E6%94%B9%E5%96%84%E9%A5%AE%E9%A3%9F%E4%B9%A0%E6%83%AF%E5%BC%80%E5%A7%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/%E6%94%B9%E5%8F%98%E7%94%9F%E6%B4%BB%E6%96%B9%E5%BC%8F%E4%BB%8E%E6%94%B9%E5%96%84%E9%A5%AE%E9%A3%9F%E4%B9%A0%E6%83%AF%E5%BC%80%E5%A7%8B/" class="post-title-link" itemprop="url">改变生活方式从改善饮食习惯开始</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:51:33 / Modified: 12:51:40" itemprop="dateCreated datePublished" datetime="2020-03-05T12:51:33+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="减肥不是目的，改变饮食习惯才是"><a href="#减肥不是目的，改变饮食习惯才是" class="headerlink" title="减肥不是目的，改变饮食习惯才是"></a>减肥不是目的，改变饮食习惯才是</h1><h2 id="1-戒掉所有含糖饮料和高热量垃圾食品（炸鸡、方便面等）"><a href="#1-戒掉所有含糖饮料和高热量垃圾食品（炸鸡、方便面等）" class="headerlink" title="1.戒掉所有含糖饮料和高热量垃圾食品（炸鸡、方便面等）"></a>1.戒掉所有含糖饮料和高热量垃圾食品（炸鸡、方便面等）</h2><h2 id="2-去超市买任何食物前对比热量和营养价值表选购，了解自己都在吃些什么"><a href="#2-去超市买任何食物前对比热量和营养价值表选购，了解自己都在吃些什么" class="headerlink" title="2.去超市买任何食物前对比热量和营养价值表选购，了解自己都在吃些什么"></a>2.去超市买任何食物前对比热量和营养价值表选购，了解自己都在吃些什么</h2><h2 id="3-记录下每天吃的食物的总热量"><a href="#3-记录下每天吃的食物的总热量" class="headerlink" title="3.记录下每天吃的食物的总热量"></a>3.记录下每天吃的食物的总热量</h2><h2 id="4-吃六七分饱就停筷，哪怕留着过一会儿饿了再吃也不能吃撑（坚决不吃自助）"><a href="#4-吃六七分饱就停筷，哪怕留着过一会儿饿了再吃也不能吃撑（坚决不吃自助）" class="headerlink" title="4.吃六七分饱就停筷，哪怕留着过一会儿饿了再吃也不能吃撑（坚决不吃自助）"></a>4.吃六七分饱就停筷，哪怕留着过一会儿饿了再吃也不能吃撑（坚决不吃自助）</h2><h2 id="5-饿了就吃些黄瓜西红柿胡萝卜或者煮鸡蛋充饥"><a href="#5-饿了就吃些黄瓜西红柿胡萝卜或者煮鸡蛋充饥" class="headerlink" title="5.饿了就吃些黄瓜西红柿胡萝卜或者煮鸡蛋充饥"></a>5.饿了就吃些黄瓜西红柿胡萝卜或者煮鸡蛋充饥</h2><h2 id="6-尽量避免使用交通工具"><a href="#6-尽量避免使用交通工具" class="headerlink" title="6.尽量避免使用交通工具"></a>6.尽量避免使用交通工具</h2><h2 id="7-能爬楼梯就不要坐电梯"><a href="#7-能爬楼梯就不要坐电梯" class="headerlink" title="7.能爬楼梯就不要坐电梯"></a>7.能爬楼梯就不要坐电梯</h2><h2 id="8-能站着绝对不坐"><a href="#8-能站着绝对不坐" class="headerlink" title="8.能站着绝对不坐"></a>8.能站着绝对不坐</h2><h2 id="9-粗粮细粮搭配吃"><a href="#9-粗粮细粮搭配吃" class="headerlink" title="9.粗粮细粮搭配吃"></a>9.粗粮细粮搭配吃</h2><h2 id="10-吃东西一口尽量咀嚼20次再咽下"><a href="#10-吃东西一口尽量咀嚼20次再咽下" class="headerlink" title="10.吃东西一口尽量咀嚼20次再咽下"></a>10.吃东西一口尽量咀嚼20次再咽下</h2><h2 id="11-饭后半小时内站立或走动"><a href="#11-饭后半小时内站立或走动" class="headerlink" title="11.饭后半小时内站立或走动"></a>11.饭后半小时内站立或走动</h2><h2 id="12-禁止一切夜宵"><a href="#12-禁止一切夜宵" class="headerlink" title="12.禁止一切夜宵"></a>12.禁止一切夜宵</h2><h2 id="13-用牛肉鱼肉鸡肉代替猪肉，猪肉是肉类中营养价值最低热量最高的"><a href="#13-用牛肉鱼肉鸡肉代替猪肉，猪肉是肉类中营养价值最低热量最高的" class="headerlink" title="13.用牛肉鱼肉鸡肉代替猪肉，猪肉是肉类中营养价值最低热量最高的"></a>13.用牛肉鱼肉鸡肉代替猪肉，猪肉是肉类中营养价值最低热量最高的</h2><h2 id="14-社交活动是用茶代替一切饮品，因为是零卡"><a href="#14-社交活动是用茶代替一切饮品，因为是零卡" class="headerlink" title="14.社交活动是用茶代替一切饮品，因为是零卡"></a>14.社交活动是用茶代替一切饮品，因为是零卡</h2><h2 id="15-保持每周三次的有氧运动"><a href="#15-保持每周三次的有氧运动" class="headerlink" title="15.保持每周三次的有氧运动"></a>15.保持每周三次的有氧运动</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jeff Tian</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jeff Tian</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.7.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
