<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"./public/search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="求知若饥,虚心若愚。">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="求知若饥,虚心若愚。">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jeff Tian">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>求知若饥,虚心若愚。</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">求知若饥,虚心若愚。</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="search-pop-overlay">
  <div class="popup search-popup">
      <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

  </div>
</div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/hexo%20+%20github%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/20/hexo%20+%20github%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" class="post-title-link" itemprop="url">hexo + github 搭建个人博客</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-20 14:45:51 / Modified: 14:45:58" itemprop="dateCreated datePublished" datetime="2020-03-20T14:45:51+08:00">2020-03-20</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a href="https://www.bilibili.com/video/av44544186?from=search&seid=9932896529971234779" target="_blank" rel="noopener">参考1</a><br><a href="https://mp.weixin.qq.com/s?__biz=MzI0NTM1MzA2Mw==&mid=2247484843&idx=1&sn=288496d86fa5113204c0c72b15b8b082&chksm=e94e9aa3de3913b562153b73d6214eb4a09e4ba0177ae7f0476437494c5f45408af4cf894e66&scene=0&xtrack=1&key=e51915e870aec09f6e41a79da145b0ab448d82be2a2a12b8eece7d810e9a2bdeeeea0f223abfc78511bc429c17ee90f780b725a8efb81694e170b383de892af09a010b870dd4e3be25bdc8039b968991&ascene=14&uin=MjIwMTIwMTc2NA%3D%3D&devicetype=Windows+7&version=62080079&lang=zh_CN&exportkey=AT75AFeUXpzDy9xUDMFcmsQ%3D&pass_ticket=hK5jlu7Cz0eTpjwBdf3EVwNcsI%2BPsCIMzJS0%2FajmyFRrdZQSdTVhWTkXGOo3kFqi" target="_blank" rel="noopener">参考2</a></p>
<h1 id="1-hexo安装"><a href="#1-hexo安装" class="headerlink" title="1.hexo安装"></a>1.hexo安装</h1><p>由于hexo是基于node.js制作的一款博客管理工具，所以要按照hexo就需要事先安装node，<a href="http://nodejs.cn/download/" target="_blank" rel="noopener">nodejs下载</a>安装，安装完成后打开==git bash==，执行下面命令:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ node -v</span><br><span class="line">$ npm -v</span><br></pre></td></tr></table></figure>

<p>查看node和npm是否安装成功。</p>
<p>然后国内建议执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br><span class="line">$ cnpm -v</span><br></pre></td></tr></table></figure>
<p>安装cnpm，并查看版本。</p>
<p>然后全局安装hexo-cli，并查看版本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cnpm install -g hexo-cli</span><br><span class="line">$ hexo -v</span><br></pre></td></tr></table></figure>
<h1 id="2-hexo初始化"><a href="#2-hexo初始化" class="headerlink" title="2.hexo初始化"></a>2.hexo初始化</h1><p>安装hexo之后需要对hexo进行初始化，首先需要新建文件夹，进入到新建文件夹之后再进行初始化，</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir blog</span><br><span class="line">$ <span class="built_in">cd</span> blog</span><br><span class="line">$ hexo init</span><br></pre></td></tr></table></figure>
<p>然后安装一些依赖包，</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install</span><br></pre></td></tr></table></figure>
<h1 id="3-部署到GitHub"><a href="#3-部署到GitHub" class="headerlink" title="3.部署到GitHub"></a>3.部署到GitHub</h1><p>在GitHub上创建一个名为<code>yourname.github.io</code>的库。这里的==yourname==一定是你的GitHub用户名。</p>
<p>然后再blog目录下安装一个插件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cnpm install  --save  hexo-deployer-git</span><br></pre></td></tr></table></figure>
<p>配置<code>_config.yml</code>文件，找到文件最下边的Deployment，添加三行：<br><img src="https://img-blog.csdnimg.cn/20200320111044836.png#pic_center" alt="在这里插入图片描述"><br>部署文件到GitHub</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo d</span><br></pre></td></tr></table></figure>

<h1 id="4-hexo使用"><a href="#4-hexo使用" class="headerlink" title="4.hexo使用"></a>4.hexo使用</h1><p>前面提到过，hexo其实类似于git，通过一些命令来实现静态网页生成、部署等工作，我们在维护博客过程中主要使用的有如下几个命令，</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ hexo n blogname      <span class="comment"># 新建文章，例如，hexo n ComputerScience</span></span><br><span class="line">$ hexo clean          <span class="comment"># 清除缓存文件</span></span><br><span class="line">$ hexo g              <span class="comment"># 生成静态文件</span></span><br><span class="line">$ hexo s              <span class="comment"># 启动本地服务器，预览网页</span></span><br><span class="line">$ hexo d              <span class="comment"># 部署文件到指定的仓库</span></span><br></pre></td></tr></table></figure>
<p>记住上述命令就可以进行日常的个人博客维护工作。</p>
<h1 id="5-更换主题"><a href="#5-更换主题" class="headerlink" title="5.更换主题"></a>5.更换主题</h1><p>克隆主题yilia或next到themes中：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> git@github.com:litten/hexo-theme-yilia.git themes/yilia</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure>
<p>修改blog根目录下的配置文件<code>_config.yml</code>：将theme: landscape改为==theme: next==。</p>
<p>然后执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean  </span><br><span class="line">$ hexo g  </span><br><span class="line">$ hexo s</span><br><span class="line">$ hexo d  <span class="comment">#推到GitHub上</span></span><br></pre></td></tr></table></figure>
<p>完成撒花。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/08/git%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/08/git%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/" class="post-title-link" itemprop="url">git命令集合</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-08 16:56:36 / Modified: 17:10:21" itemprop="dateCreated datePublished" datetime="2020-03-08T16:56:36+08:00">2020-03-08</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="创建版本库"><a href="#创建版本库" class="headerlink" title="创建版本库"></a>创建版本库</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global user.name <span class="string">"jefftian"</span></span><br><span class="line">$ git config --global user.email <span class="string">"email@163.com"</span></span><br><span class="line">$ git init</span><br></pre></td></tr></table></figure>
<h1 id="版本控制"><a href="#版本控制" class="headerlink" title="版本控制"></a>版本控制</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ git add test.cpp</span><br><span class="line">$ git commit -m <span class="string">"描述信息"</span></span><br><span class="line"></span><br><span class="line">$ git status</span><br><span class="line">$ git diff test.cpp <span class="comment">#查看上次对 file 进行的修改</span></span><br><span class="line">$ git <span class="built_in">log</span> --pretty=oneline</span><br><span class="line"></span><br><span class="line">$ git reset --hard head^ <span class="comment">#把当前版本回退到上一个版本</span></span><br><span class="line">$ git reset --hard 版本号  <span class="comment">#回到版本回退之前的版本</span></span><br><span class="line">$ git reflog  <span class="comment">#记录你的每一次命令</span></span><br><span class="line"></span><br><span class="line">$ git reset head test.cpp  <span class="comment">#把暂存区的修改撤销掉（unstage）</span></span><br><span class="line">$ git checkout -- test.cpp  <span class="comment">#把文件在工作区的修改全部撤销，让这个文件回到最近一次git commit或git add时的状态</span></span><br><span class="line"></span><br><span class="line">$ git rm test.txt  <span class="comment">#相当于把“rm file”这个操作 add 到暂存区，这个file必须是tracking file</span></span><br><span class="line">$ git commit -m <span class="string">"remove test.txt"</span></span><br></pre></td></tr></table></figure>

<h1 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C <span class="string">"youremail@163.com"</span>    <span class="comment">#产生SSH密钥</span></span><br><span class="line">$ git remote add origin git@github.com:jefftian666/learngit.git  <span class="comment">#关联本地库和远程库，并将远程库命名为origin</span></span><br><span class="line">$ git push -u origin master <span class="comment"># 加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来</span></span><br><span class="line">$ git push origin master  <span class="comment">#把本地的master分支内容推送的远程master分支</span></span><br><span class="line"></span><br><span class="line">$ git <span class="built_in">clone</span> git@github.com:jefftian666/gitskills.git</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/jefftian666/gitskills.git</span><br><span class="line"></span><br><span class="line">$ git remote rm origin  <span class="comment">#将远程库在本地的名字删除 </span></span><br><span class="line">$ git remote add github_origin git@github.com:jefftian666/learngit.git</span><br><span class="line">$ git remote add gitee_origin git@gitee.com:jefftian666/learngit.git</span><br><span class="line">$ git remote -v  <span class="comment">#显示和本地关联的远程库的详细信息</span></span><br><span class="line"></span><br><span class="line">$ git push github_origin master</span><br><span class="line">$ git push gitee_origin master</span><br></pre></td></tr></table></figure>
<h1 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ git branch  <span class="comment">#查看分支</span></span><br><span class="line">$ git branch &lt;name&gt; <span class="comment">#新建分支</span></span><br><span class="line">$ git checkout &lt;name&gt;或者git switch &lt;name&gt;  <span class="comment">#切换分支</span></span><br><span class="line">$ git checkout -b &lt;name&gt;或者git switch -c &lt;name&gt;  <span class="comment">#创建+切换分支</span></span><br><span class="line"></span><br><span class="line">$ git merge &lt;name&gt;  <span class="comment">#合并某分支到当前分支</span></span><br><span class="line">$ git merge --no-ff -m <span class="string">"merge with no-ff"</span> &lt;name&gt;  <span class="comment">#合并时禁用Fast forward模式，Git就会在merge时生成一个新的commit</span></span><br><span class="line">$ git branch -d &lt;name&gt;  <span class="comment">#删除分支</span></span><br><span class="line"></span><br><span class="line">$ git stash save <span class="string">"储存信息"</span>  <span class="comment">#将暂存区文件和工作区的tracking文件存储到stash区。针对当前分支对文件有修改，且不想commit时，切换分支前使用</span></span><br><span class="line">$ git stash list  <span class="comment">#查看stash区</span></span><br><span class="line">$ git stash pop  <span class="comment">#恢复的同时把stash内容也删了</span></span><br><span class="line">$ git stash apply stash@&#123;0&#125;  <span class="comment">#恢复指定的stash</span></span><br><span class="line"></span><br><span class="line">$ git cherry-pick &lt;commit&gt;   <span class="comment">#在master分支上修复的bug，想要合并到当前dev分支</span></span><br><span class="line">$ git branch -D feature-vulcan  <span class="comment">#强行删除分支。分支还没有被合并，如果删除，将丢失掉修改.</span></span><br><span class="line"></span><br><span class="line">$ git checkout -b dev origin/dev  <span class="comment">#创建远程origin的dev分支到本地</span></span><br><span class="line">$ git branch --<span class="built_in">set</span>-upstream-to=origin/dev dev  <span class="comment">#指定本地dev分支与远程origin/dev分支的链接</span></span><br><span class="line">$ git pull  <span class="comment">#把最新的提交从origin/dev抓下来</span></span><br><span class="line"></span><br><span class="line">$ git rebase操作可以把本地未push的分叉提交历史整理成直线；目的是使得我们在查看历史提交的变化时更容易，因为分叉的提交需要三方对比。</span><br></pre></td></tr></table></figure>
<h1 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ git tag v1.0  <span class="comment">#给commit打标签</span></span><br><span class="line">$ git tag <span class="comment">#查看标签</span></span><br><span class="line">$ git tag v0.9 f52c633  <span class="comment">#给特定commit打标签</span></span><br><span class="line">$ git show v0.9  <span class="comment">#查看标签信息</span></span><br><span class="line">$ git tag -a v0.1 -m <span class="string">"version 0.1 released"</span> 1094adb  <span class="comment">#创建带有说明的标签，用-a指定标签名，-m指定说明文字</span></span><br><span class="line"></span><br><span class="line">$ git tag -d v0.1  <span class="comment">#删除标签</span></span><br><span class="line">$ git push origin v1.0  <span class="comment">#推送某个标签到远程</span></span><br><span class="line">$ git push origin --tags  <span class="comment">#一次性推送全部尚未推送到远程的本地标签</span></span><br><span class="line"><span class="comment">#删除远程标签</span></span><br><span class="line">$ git tag -d v0.9  <span class="comment">#先从本地删除</span></span><br><span class="line">$ git push origin :refs/tags/v0.9  <span class="comment">#然后，从远程删除</span></span><br></pre></td></tr></table></figure>













<table>
<thead>
<tr>
<th>alias</th>
<th>realname</th>
</tr>
</thead>
<tbody><tr>
<td>lg</td>
<td>log –color –graph –pretty=format:’%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset’ –abbrev-commit</td>
</tr>
<tr>
<td>st</td>
<td>status</td>
</tr>
<tr>
<td>co</td>
<td>checkout</td>
</tr>
<tr>
<td>ci</td>
<td>commit</td>
</tr>
<tr>
<td>br</td>
<td>branch</td>
</tr>
<tr>
<td>unstage</td>
<td>reset HEAD</td>
</tr>
<tr>
<td>last</td>
<td>log -1</td>
</tr>
</tbody></table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%80%EF%BC%88%E6%AD%A6%E6%B1%89%E5%8A%A0%E6%B2%B9%E3%80%81%E4%B8%AD%E5%9B%BD%E5%8A%A0%E6%B2%B9%E3%80%81%E4%B8%8D%E5%A5%BD%E7%9A%84%E4%BA%8B%E5%BF%85%E5%B0%86%E8%BF%87%E5%8E%BB%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%80%EF%BC%88%E6%AD%A6%E6%B1%89%E5%8A%A0%E6%B2%B9%E3%80%81%E4%B8%AD%E5%9B%BD%E5%8A%A0%E6%B2%B9%E3%80%81%E4%B8%8D%E5%A5%BD%E7%9A%84%E4%BA%8B%E5%BF%85%E5%B0%86%E8%BF%87%E5%8E%BB%EF%BC%89/" class="post-title-link" itemprop="url">论文阅读一（武汉加油、中国加油、不好的事必将过去）</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:56:24 / Modified: 12:56:31" itemprop="dateCreated datePublished" datetime="2020-03-05T12:56:24+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="一、MID-Fusion-Octree-based-Object-Level-Multi-Instance-Dynamic-SLAM"><a href="#一、MID-Fusion-Octree-based-Object-Level-Multi-Instance-Dynamic-SLAM" class="headerlink" title="一、MID-Fusion: Octree-based Object-Level Multi-Instance Dynamic SLAM"></a>一、MID-Fusion: Octree-based Object-Level Multi-Instance Dynamic SLAM</h1><h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><p>1）第一个使用体积表示法的RGB-D多实例动态SLAM系统；<br>2）一种更鲁棒的跟踪方法，利用测量不确定性加权并重新设置参数以用于对象跟踪；<br>3）一个集成了几何信息、光度信息和语义信息的分割方法；<br>4）将语义分布和前景对象概率融合到基于八叉树的物体模型中。</p>
<h2 id="A-系统概述"><a href="#A-系统概述" class="headerlink" title="A.系统概述"></a>A.系统概述</h2><p><img src="https://img-blog.csdnimg.cn/20200206202339689.png#pic_center" alt="在这里插入图片描述"><br>图2显示了我们提出的系统的流程。它由四个部分组成： <strong>segmentation, tracking, fusion and raycasting</strong>。每个输入的RGB-D图像都由Mask R-CNN处理以执行实例分割，然后进行几何边缘分割和计算运动残差以优化蒙版边界（第IV-D节）。对于tracking，我们首先根据不包括人类蒙版区域的所有顶点计算相机位姿（第IV-B节），然后从该位姿进行光线投射，以找出哪些物体在当前帧中是可见的。这也可以帮助将局部对象蒙版与现有对象模型相关联。我们评估每个对象的运动残差以确定其是否处于运动状态，然后追踪运动物体（第IV-C节）并根据静态世界（包括当前的静态对象）改进相机的位姿（第IV-B节）。使用相机和物体的估计位姿，将深度和颜色信息以及预测的语义和前景概率融合到物体模型中（第IV-E节）。 IV-F节介绍了可见物体的检测以及射线投射。</p>
<h2 id="B-RGB-D-Camera-tracking"><a href="#B-RGB-D-Camera-tracking" class="headerlink" title="B.RGB-D Camera tracking"></a>B.RGB-D Camera tracking</h2><p>计算相机位姿分为两步<br>    1.根据除人类外的所有模型的顶点计算相机位姿。<br>    2.根据静态场景计算相机位姿。</p>
<p>通过最小化密集的点到面的ICP残差eg和光度（RGB）残差ep来进行这两个步骤，这些残差由单独的测量不确定度wg和wp加权。<br><img src="https://img-blog.csdnimg.cn/20200206211018509.png#pic_center" alt="在这里插入图片描述"><br>在初始化相机位姿之后，我们进行光线投射以在视图中找到可见的物体。 为了找到运动的物体，我们需要按公式7重新计算RGB残差，然后在当前帧上对Etrack（TWCL）进行最优的评估，并设定一个阈值（个人理解：小于阈值的点为inlier）。 如果物体的蒙版中的inlier比率低于0.9，则我们认为该物体正在移动并按照IV-C节中描述优化其位姿。</p>
<p>然后，通过使用相同的目标函数和优化策略仅根据静态物体来优化相机的位姿。</p>
<h2 id="C-Object-pose-estimation"><a href="#C-Object-pose-estimation" class="headerlink" title="C.Object pose estimation"></a>C.Object pose estimation</h2><p>根据公式8和公式9重新最小化公式1，即可求得运动物体的位姿。</p>
<h2 id="D-Combined-semantic-geometric-motion-segmentation"><a href="#D-Combined-semantic-geometric-motion-segmentation" class="headerlink" title="D.Combined semantic-geometric-motion segmentation"></a>D.Combined semantic-geometric-motion segmentation</h2><p>对于每个RGB-D帧，我们使用Mask R-CNN [16]进行实例分割，然后通过几何边缘优化来解决泄漏的Mask边界[14]。</p>
<p>然后，我们通过光线投射将每个地图物体的实例蒙版渲染到当前帧。 </p>
<p>通过计算 IoU 与渲染蒙版的交集，我们将从Mask R-CNN和几何优化生成的局部分割蒙版与现有物体模型相关联。</p>
<p>在将分割蒙版与物体模型关联之后，我们将基于物体的运动残差进一步优化分割蒙版。根据公式10重新计算公式1，对于ICP和RGB残差过高的像素将被视为outlier，并在分割蒙版中滤除。</p>
<h2 id="E-Object-level-fusion"><a href="#E-Object-level-fusion" class="headerlink" title="E.Object-level fusion"></a>E.Object-level fusion</h2><p>将深度、颜色、语义、前景概率信息集成到物体模型</p>
<h2 id="F-Raycasting"><a href="#F-Raycasting" class="headerlink" title="F.Raycasting"></a>F.Raycasting</h2><p>光线投射方法是基于图像序列的直接体绘制算法。从图像的每一个像素，沿固定方向（通常是视线方向）发射一条光线，光线穿越整个图像序列，并在这个过程中，对图像序列进行采样获取颜色信息，同时依据光线吸收模型将颜色值进行累加，直至光线穿越整个图像序列，最后得到的颜色值就是渲染图像的颜色。</p>
<h1 id="二、Improving-Visual-Localization-Accuracy-in-Dynamic-Environments-Based-on-Dynamic-Region-Removal"><a href="#二、Improving-Visual-Localization-Accuracy-in-Dynamic-Environments-Based-on-Dynamic-Region-Removal" class="headerlink" title="二、Improving Visual Localization Accuracy in Dynamic Environments Based on Dynamic Region Removal"></a>二、Improving Visual Localization Accuracy in Dynamic Environments Based on Dynamic Region Removal</h1><p>使用神经网络获得先验边界框，人为确定动态权重，权重低于0.5为静态区域。在静态区域中选择特征点，初步估算两帧之间的相对运动。根据得到的变换矩阵，将参考帧映射到当前帧。将当前帧划分网格，计算特征点对的距离，据此给各个网格分配动态权重，进而计算各个网格的动态概率。然后根据贝叶斯定理，更新动态概率。最后去掉动态区域，计算相机位姿。</p>
<h1 id="三、DSOD-DSO-in-Dynamic-Environments"><a href="#三、DSOD-DSO-in-Dynamic-Environments" class="headerlink" title="三、DSOD: DSO in Dynamic Environments"></a>三、DSOD: DSO in Dynamic Environments</h1><p><a href="https://zhuanlan.zhihu.com/p/29177540" target="_blank" rel="noopener">DSO详解</a></p>
<p>本文将语义分割网络与深度预测网络相结合，以提供先验的深度和语义信息。</p>
<p>我们提出的方法基于DSO。 因此，我们首先在III-A部分中简要描述DSO。 然后，我们提出的算法的整体方法在第III-B节中显示。<br>深度预测和语义分割分别在第III-C节和第III-D节中介绍。 最后，我们在III-E节中介绍了检查运动一致性和滤除动态点的方法</p>
<h2 id="A、BASELINE-DSO-METHOD"><a href="#A、BASELINE-DSO-METHOD" class="headerlink" title="A、BASELINE DSO METHOD"></a>A、BASELINE DSO METHOD</h2><p>最小化光度误差：<br><img src="https://img-blog.csdnimg.cn/20200207193054661.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="B、DSOD-FRAMEWORK"><a href="#B、DSOD-FRAMEWORK" class="headerlink" title="B、DSOD FRAMEWORK"></a>B、DSOD FRAMEWORK</h2><p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200207193525902.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200207193525902.png#pic_center</a> =3000x400)<br>我们提出的方法的总体框架如图1所示。我们的方法的输入由RGB图像序列组成，这些RGB图像序列被分解为红色，绿色和蓝色通道。 将光度校准应用于三个通道，然后将它们合并。 校正后的RGB图像是深度预测和分割网络的输入。原始的RGB图像也将转换为灰度图像，进行光度校准，然后用作动态点选择的输入。</p>
<p>与DSO相反，在DSOD中引入了深度预测网络以提供初始深度并对点模式进行编码，以匹配投影的初始位置。 目的是加速深度的融合。网络在单目SLAM系统中在一定程度上补偿了尺度漂移。 此外，语义分割网络用于检查移动一致性，以减少动态环境中的错误。 最后，DSOD的输出是估计的位姿。</p>
<h2 id="C、DEPTH-PREDICTION-NETWORK"><a href="#C、DEPTH-PREDICTION-NETWORK" class="headerlink" title="C、DEPTH PREDICTION NETWORK"></a>C、DEPTH PREDICTION NETWORK</h2><p>DSO仅从当前关键帧中选择像素。 因为它使用用不确定深度信息初始化的像素作为候选点，沿大范围沿对极线进行搜索时，可能会生成错误的投影对。 因此，我们将深度预测网络引入到我们的方法中。 我们使用无监督的单目深度估计来预测候选点的初始深度。 该估计通过单图像深度预测网络为候选点初始化过程提供了先验的深度信息。</p>
<p>在沿对极线对候选点模式的代码和搜索点模式的代码进行XOR操作之后，最相似的投影就是我们的目标模式。 第二步是执行高斯–牛顿迭代，以优化投影对并更新候选点的深度。 最后，我们确定经过1次迭代后候选点的深度（相对于参考系）是否收敛。 如果没有，则不应将该候选点用于姿势估计</p>
<h2 id="D、SEMANTIC-SEGMENTATION-NETWORK"><a href="#D、SEMANTIC-SEGMENTATION-NETWORK" class="headerlink" title="D、SEMANTIC SEGMENTATION NETWORK"></a>D、SEMANTIC SEGMENTATION NETWORK</h2><p>在本文中，语义信息被用来标记动态环境中的潜在动态点。</p>
<p>为了提高分割的准确性，我们采用了可以提供像素级分割的分割网络。分割网络在COCO数据集上进行了训练[33]，它可以检测30类物体。其中，人，汽车，自行车，公共汽车和摩托车被定义为潜在的移动物体。</p>
<h2 id="E、MOVEMENT-CONSISTENCY-CHECK"><a href="#E、MOVEMENT-CONSISTENCY-CHECK" class="headerlink" title="E、MOVEMENT CONSISTENCY CHECK"></a>E、MOVEMENT CONSISTENCY CHECK</h2><p>用RANSAC方法和合适的特征点，确定基础矩阵。<br>利用基础矩阵和参考帧的像素坐标计算极线。<br>计算当前帧的像素点到极线的距离，若大于阈值则为动态点。</p>
<h1 id="四、DS-SLAM-A-Semantic-Visual-SLAM-towards-Dynamic-Environments"><a href="#四、DS-SLAM-A-Semantic-Visual-SLAM-towards-Dynamic-Environments" class="headerlink" title="四、DS-SLAM: A Semantic Visual SLAM towards Dynamic Environments"></a>四、DS-SLAM: A Semantic Visual SLAM towards Dynamic Environments</h1><p><a href="https://www.sohu.com/a/282110338_715754" target="_blank" rel="noopener">泡泡图灵智库解读</a><br><a href="https://blog.csdn.net/pikachu_777/article/details/86479564" target="_blank" rel="noopener">一个兄弟的解读</a></p>
<h2 id="A、主要贡献"><a href="#A、主要贡献" class="headerlink" title="A、主要贡献"></a>A、主要贡献</h2><ul>
<li>基于ORB-SLAM2 提出了动态环境中的完整语义SLAM系统（DS-SLAM），可以减少动态对象对位姿估计的影响。</li>
<li>本文将一个实时语义分割网络放在一个独立的线程中，它将语义分割与移动一致性检查方法结合起来，过滤掉场景的动态部分，如走路的人。因此，在动态场景中，提升了定位模块和建图模块的稳定性和鲁棒性。</li>
<li>DS-SLAM创建了一个单独的线程来构建稠密的语义3D八叉树地图。稠密的语义三维八叉树地图采用优势对数计分法滤除不稳定体素并更新这些体素的语义。<h2 id="B、概述"><a href="#B、概述" class="headerlink" title="B、概述"></a>B、概述</h2>图1 DS-SLAM概述图。 原始RGB图像用于同时进行语义分割和移动一致性检查。 然后删除异常值并估计位姿。 基于位姿，深度图像和语义分割结果，在独立线程中构建语义八叉树地图。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200207195846535.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200207195846535.png#pic_center</a> =3000x400)<h2 id="C、框架"><a href="#C、框架" class="headerlink" title="C、框架"></a>C、框架</h2>图2 DS-SLAM的框架图。 局部地图线程和回环检测线程与ORB-SLAM2相同。 前者处理新的关键帧并执行局部BA以在相机姿势的周围实现最佳重建，而后者搜索回环并在检测到回环时执行图优化。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200207200004739.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200207200004739.png#pic_center</a> =3000x400)<h2 id="D、semantic-segmentation"><a href="#D、semantic-segmentation" class="headerlink" title="D、semantic segmentation"></a>D、semantic segmentation</h2></li>
<li>使用的网络结构是SegNet，在caffe上使用VOC训练，共20个类别。</li>
<li>认为标签为行人的特征点最有可能是外点。<h2 id="E、Moving-Consistency-Check"><a href="#E、Moving-Consistency-Check" class="headerlink" title="E、Moving Consistency Check"></a>E、Moving Consistency Check</h2>通过光流法匹配特征点<br>若某对匹配的特征点接近边界，或其像素值与以它为中心的3x3区域内的像素值差别太大，就丢弃这对匹配。<br>用RANSAC方法和合适的特征点，确定基础矩阵。<br>利用基础矩阵和参考帧的像素坐标计算极线。<br>计算当前帧的像素点到极线的距离，若大于阈值则为动态点。<br><img src="https://img-blog.csdnimg.cn/20200207200305427.png" alt="在这里插入图片描述"><h2 id="F、动态点剔除"><a href="#F、动态点剔除" class="headerlink" title="F、动态点剔除"></a>F、动态点剔除</h2>语义分割结果无法判断物体是否是动态的。<br>运动一致性检查得到的点，不是这个物体包含的所有点，缺少精确轮廓<br>因此，将这两个步骤的结果结合起来，如果在一个物体的语义分割边界内，有足够数量的由移动一致性检测得到的移动点，那么这个物体的所有点都被视为动态的。然后剔除动态点进行位姿估计。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">《机器学习》读书笔记</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:56:14 / Modified: 12:56:21" itemprop="dateCreated datePublished" datetime="2020-03-05T12:56:14+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="一、绪论"><a href="#一、绪论" class="headerlink" title="一、绪论"></a>一、绪论</h1><h2 id="基本术语"><a href="#基本术语" class="headerlink" title="基本术语"></a>基本术语</h2><p>数据集（data set）、示例instance、样本sample、属性attribute、特征feature、属性值attribute value、属性空间attribute space、样本空间sample space、特征向量feature vector。</p>
<p>从数据中学得模型的过程称为“学习”（learning）或“训练”（training），这个过程通过执行某个学习算法来完成.训练过程中使用的数据称为“训练数据”（training data），其中每个样本称为一个“训练样本”（training sample），训练样本组成的集合称为“训练集”（training set）.学得模型对应了关于数据的某种潜在的规律，因此亦称“假设”（hypothesis）；这种潜在规律自身，则称为“真相”或“真实”（ground-truth），学习过程就是为了找出或逼近真相.有时将模型称为“学习器”（learner），可看作学习算法在给定数据和参数空间上的实例化。拥有了标记信息的示例，则称为“样例”（example）。</p>
<p>若我们欲预测的是离散值，此类学习任务称为“分类”（classification）；若欲预测的是连续值，此类学习任务称为“回归”（regression）.对只涉及两个类别的“二分类”（binary classification）任务，通常称其中一个类为“正类”（positive class），另一个类为“反类”（negative class）；涉及多个类别时，则称为“多分类”（multi-class classification）任务。</p>
<p>学得模型后，使用其进行预测的过程称为“测试”（testing），被预测的样本称为“测试样本”（testing sample）。我们还可以对西瓜做“聚类”（clustering），即将训练集中的西瓜分成若干组，每组称为一个“簇”（cluster）；这些自动形成的簇可能对应一些潜在的概念划分，例如“浅色瓜”“深色瓜”，甚至“本地瓜”“外地瓜”.这样的学习过程有助于我们了解数据内在的规律，能为更深入地分析数据建立基础.需说明的是，在聚类学习中，“浅色瓜”“本地瓜”这样的概念我们事先是不知道的，而且学习过程中使用的训练样本通常不拥有标记信息.</p>
<p>根据训练数据是否拥有标记信息，学习任务可大致划分为两大类：“监督学习”（supervised learning）和“无监督学习”（unsupervised learning），分类和回归是前者的代表，而聚类则是后者的代表.</p>
<p>学得模型适用于新样本的能力，称为“泛化”（generalization）能力</p>
<h1 id="二、模型评估与选择"><a href="#二、模型评估与选择" class="headerlink" title="二、模型评估与选择"></a>二、模型评估与选择</h1><h1 id="三、线性模型"><a href="#三、线性模型" class="headerlink" title="三、线性模型"></a>三、线性模型</h1><p>在属性空间中拟合出一条满足所有样本的曲线。</p>
<h1 id="四、决策树"><a href="#四、决策树" class="headerlink" title="四、决策树"></a>四、决策树</h1><p>决策树（decision tree）是一类常见的机器学习方法.以二分类任务为例，我们希望从给定训练数据集学得一个模型用以对新示例进行分类，这个把样本分类的任务，可看作对“当前样本属于正类吗？”这个问题的“决策”或“判定”过程.顾名思义，决策树是基于树结构来进行决策的，这恰是人类在面临决策问题时一种很自然的处理机制，剪枝（pruning）是决策树学习算法对付“过拟合”的主要手段.</p>
<h1 id="五、神经网络"><a href="#五、神经网络" class="headerlink" title="五、神经网络"></a>五、神经网络</h1><h2 id="1、神经元模型"><a href="#1、神经元模型" class="headerlink" title="1、神经元模型"></a>1、神经元模型</h2><p>1943年提出的M-P神经元模型：（此结构只能解决线性可分的问题，无法解决异或等非线性可分问题）<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200211200929560.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200211200929560.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =500x300)<br>理想的激活函数是阶跃函数，但其数学性质不好，实际上多用sigmoid函数。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200211201059281.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200211201059281.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =500x300)<br>把许多个这样的神经元按一定的层次结构连接起来，就得到了神经网络.</p>
<h2 id="2、感知机与多层网络"><a href="#2、感知机与多层网络" class="headerlink" title="2、感知机与多层网络"></a>2、感知机与多层网络</h2><p>要解决非线性可分问题，需考虑使用多层功能神经元。例如图5.5中这个简单的两层感知机就能解决异或问题.在图5.5（a）中，输出层与输入层之间的一层神经元，被称为隐层或隐含层（hidden layer），隐含层和输出层神经元都是拥有激活函数的功能神经元。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200211202700359.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200211202700359.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =500x300)<br>更一般的，常见的神经网络是形如图5.6所示的层级结构，每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接.这样的神经网络结构通常称为“多层前馈神经网络”（multi-layer feedforward neural networks），其中输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出；换言之，输入层神经元仅是接受输入，不进行函数处理，隐层与输出层包含功能神经元。因此，图5.6（a）通常被称为“两层网络”.为避免歧义，本书称其为“单隐层网络”.只需包含隐层，即可称为多层网络.神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”（connection weight）以及每个功能神经元的阀值；换言之，神经网络“学”到的东西，蕴涵在<strong>连接权</strong>与<strong>阀值</strong>中.<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200211202720537.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200211202720537.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =500x300)</p>
<h2 id="3、误差逆传播算法"><a href="#3、误差逆传播算法" class="headerlink" title="3、误差逆传播算法"></a>3、误差逆传播算法</h2><p>多层网络的学习能力比单层感知机强得多.欲训练多层网络，式（5.1）的简单感知机学习规则显然不够了，需要更强大的学习算法.误差逆传播（error BackPropagation，简称BP）算法就是其中最杰出的代表，它是迄今最成功的神经网络学习算法.现实任务中使用神经网络时，大多是在使用BP算法进行训练.值得指出的是，BP算法不仅可用于多层前馈神经网络，还可用于其他类型的神经网络，例如训练递归神经网络[Pineda，1987].但通常说“BP网络”时，一般是指用BP算法训练的多层前馈神经网络.<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200211203707554.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200211203707554.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =500x300)<br>图5.8给出了BP算法的工作流程.对每个训练样例，BP算法执行以下操作：先将输入示例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果；然后计算输出层的误差（第4-5行），再将误差逆向传播至隐层神经元（第6行），最后根据隐层神经元的误差来对连接权和阈值进行调整（第7行）.该迭代过程循环进行，直到达到某些停止条件为止，例如训练误差已达到一个很小的值.<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200211203819522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200211203819522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =450x300)</p>
<h2 id="4、深度学习"><a href="#4、深度学习" class="headerlink" title="4、深度学习"></a>4、深度学习</h2><p>典型的深度学习模型就是很深层的神经网络（深度学习模型通常有八九层甚至更多隐层）.显然，对神经网络模型，提高容量的办法是增加隐层的数目和通过单纯增加隐层神经元的数目，增加隐层的数目显然比增加隐层神经元的数目更有效，然而，多隐层神经网络难以直接用经典算法（例如标准BP算法）进行训练，因为误差在多隐层内逆传播时，往往会“发散”（diverge）而不能收敛到稳定状态.</p>
<p>无监督逐层训练（unsupervised layer-wise training）是多隐层网络训练的有效手段，其基本思想是每次训练一层隐结点，训练时将上一层隐结点的输出作为输入，而本层隐结点的输出作为下一层隐结点的输入，这称为“预训练”（pre-training）；在预训练全部完成后，再对整个网络进行“微调”（fine-tuning）训练.</p>
<p>事实上，“预训练+微调”的做法可视为将大量参数分组，对每组先找到局部看来比较好的设置，然后再基于这些局部较优的结果联合起来进行全局寻优.这样就在利用了模型大量参数所提供的自由度的同时，有效地节省了训练开销.</p>
<p>另一种节省训练开销的策略是“权共享”（weight sharing），即让一组神经元使用相同的连接权.这个策略在卷积神经网络（Convolutional Neural Network，简称 CNN）[LeCun and Bengio，1995；LeCun et al.，1998]中发挥了重要作用.</p>
<h1 id="六、支持向量机"><a href="#六、支持向量机" class="headerlink" title="六、支持向量机"></a>六、支持向量机</h1><p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200211220723688.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200211220723688.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =400x300)<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200211220855668.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200211220855668.png#pic_center</a> =400x70)<br>如图6.2所示，距离超平面最近的这几个训练样本点使式（6.3）的等号成立，它们被称为“支持向量”（support vector），两个异类支持向量到超平面的距离之和为<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200211220937953.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200211220937953.png#pic_center</a> =400x70)<br>它被称为“间隔”（margin）.</p>
<h1 id="七、贝叶斯分类"><a href="#七、贝叶斯分类" class="headerlink" title="七、贝叶斯分类"></a>七、贝叶斯分类</h1><h1 id="八、集成学习"><a href="#八、集成学习" class="headerlink" title="八、集成学习"></a>八、集成学习</h1><p>集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统（multi-classifier system）、基于委员会的学习（committee-based learning）等.</p>
<p>图8.1显示出集成学习的一般结构：先产生一组“个体学习器”（individual learner），再用某种策略将它们结合起来.个体学习器通常由一个现有的学习算法从训练数据产生，例如C4.5决策树算法、BP神经网络算法等，此时集成中只包含同种类型的个体学习器，例如“决策树集成”中全是决策树，“神经网络集成”中全是神经网络，这样的集成是“同质”的（homogeneous）.同质集成中的个体学习器亦称“基学习器”（base learner），相应的学习算法称为“基学习算法”（base learning algorithm）.集成也可包含不同类型的个体学习器，例如同时包含决策树和神经网络，这样的集成是“异质”的（heterogenous）.异质集成中的个体学习器由不同的学习算法生成，这时就不再有基学习算法；相应的，个体学习器一般不称为基学习器，常称为“组件学习器”（component learner）或直接称为个体学习器.<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200211221538253.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200211221538253.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =400x200)<br>根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting，后者的代表是Bagging和“随机森林”（Random Forest）.</p>
<p>Boosting是一族可将弱学习器提升为强学习器的算法.这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这个基学习器进行加权结合.</p>
<p>Bagging[Breiman，1996a]是并行式集成学习方法最著名的代表.从名字即可看出，它直接基于我们在2.2.3节介绍过的自助采样法（bootstrap sampling）.给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m次随机采样操作，我们得到含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现.</p>
<p>随机森林（Random Forest，简称RF）[Breiman，2001a]是Bagging的一个扩展变体.RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择.具体来说，传统决策树在选择划分属性时是在当前结点的属性集合（假定有d个属性）中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分.这里的参数k控制了随机性的引入程度：若令k=d，则基决策树的构建与传统决策树相同；若令k=1，则是随机选择一个属性用于划分；一般情况下，推荐值k=log<del>2</del>d.</p>
<h1 id="九、聚类"><a href="#九、聚类" class="headerlink" title="九、聚类"></a>九、聚类</h1><p>在“无监督学习”（unsupervised learning）中，训练样本的标记信息是未知的，目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础.此类学习任务中研究最多、应用最广的是“聚类”（clustering）.</p>
<p>聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”（cluster）.通过这样的划分，每个簇可能对应于一些潜在的概念（类别）；需说明的是，这些概念对聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者来把握和命名.</p>
<h1 id="十、降维与度量学习"><a href="#十、降维与度量学习" class="headerlink" title="十、降维与度量学习"></a>十、降维与度量学习</h1><h1 id="十一、特征选择与稀疏学习"><a href="#十一、特征选择与稀疏学习" class="headerlink" title="十一、特征选择与稀疏学习"></a>十一、特征选择与稀疏学习</h1><p>我们将属性称为“特征”（feature），对当前学习任务有用的属性称为“相关特征”（relevant feature）、没什么用的属性称为“无关特征”（irrelevant feature）.从给定的特征集合中选择出相关特征子集的过程，称为“特征选择”（feature selection）.</p>
<p>特征选择是一个重要的“数据预处理”（data preprocessing）过程，在现实机器学习任务中，获得数据之后通常先进行特征选择，此后再训练学习器.那么，为什么要进行特征选择呢？</p>
<p>有两个很重要的原因：首先，我们在现实任务中经常会遇到维数灾难问题，这是由于属性过多而造成的，若能从中选择出重要的特征，使得后续学习过程仅需在一部分特征上构建模型，则维数灾难问题会大为减轻.从这个意义上说，特征选择与第10章介绍的降维有相似的动机；事实上，它们是处理高维数据的两大主流技术.第二个原因是，去除不相关特征往往会降低学习任务的难度，这就像侦探破案一样，若将纷繁复杂的因素抽丝剥茧，只留下关键因素，则真相往往更易看清</p>
<h1 id="十二、计算学习理论"><a href="#十二、计算学习理论" class="headerlink" title="十二、计算学习理论"></a>十二、计算学习理论</h1><p>顾名思义，计算学习理论（computational learning theory）研究的是关于通过“计算”来进行“学习”的理论，即关于机器学习的理论基础，其目的是分析学习任务的困难本质，为学习算法提供理论保证，并根据分析结果指导算法设计.</p>
<h1 id="十三、半监督学习"><a href="#十三、半监督学习" class="headerlink" title="十三、半监督学习"></a>十三、半监督学习</h1><p>事实上，未标记样本虽未直接包含标记信息，但若它们与有标记样本是从同样的数据源独立同分布采样而来，则它们所包含的关于数据分布的信息对建立模型将大有裨益.图13.1给出了一个直观的例示.若仅基于图中的一个正例和一个反例，则由于待判别样本恰位于两者正中间，大体上只能随机猜测；若能观察到图中的未标记样本，则将很有把握地判别为正例.<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/2020021122380129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/2020021122380129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =400x200)<br>让学习器不依赖外界交互、自动地利用未标记样本来提升学习性能，就是半监督学习（semi-supervised learning）.半监督学习的现实需求非常强烈，因为在现实应用中往往能容易地收集到大量未标记样本，而获取“标记”却需耗费人力、物力.例如，在进行计算机辅助医学影像分析时，可以从医院获得大量医学影像，但若希望医学专家把影像中的病灶全都标识出来则是不现实的.“有标记数据少，未标记数据多”半监督学习恰是提供了一条利用“廉价”的未标记样本的途径.</p>
<p>要利用未标记样本，必然要做一些将未标记样本所揭示的数据分布信息与类别标记相联系的假设.最常见的是“聚类假设”（cluster assumption），即假设数据存在簇结构，同一个簇的样本属于同一个类别.图13.1就是基于聚类假设来利用未标记样本，由于待预测样本与正例样本通过未标记样本的“撮合”聚在一起，与相对分离的反例样本相比，待判别样本更可能属于正类.半监督学习中另一种常见的假设是“流形假设”（manifold assumption），即假设数据分布在一个流形结构上，邻近的样本拥有相似的输出值.“邻近”程度常用“相似”程度来刻画，因此，流形假设可看作聚类假设的推广，但流形假设对输出值没有限制，因此比聚类假设的适用范围更广，可用于更多类型的学习任务.事实上，无论聚类假设还是流形假设，其本质都是“相似的样本拥有相似的输出”这个基本假设.</p>
<h1 id="十四、概率图模型"><a href="#十四、概率图模型" class="headerlink" title="十四、概率图模型"></a>十四、概率图模型</h1><p>机器学习最重要的任务，是根据一些已观察到的证据（例如训练样本）来对感兴趣的未知变量（例如类别标记）进行估计和推测.概率模型（probabilistic model）提供了一种描述框架，将学习任务归结于计算变量的概率分布.在概率模型中，利用已知变量推测未知变量的分布称为“推断”（inference），其核心是如何基于可观测变量推测出未知变量的条件分布.</p>
<p>概率图模型（probabilistic graphical model）是一类用图来表达变量相关关系的概率模型.它以图为表示工具，最常见的是用一个结点表示一个或一组随机变量，结点之间的边表示变量间的概率相关关系，即“变量关系图”.根据边的性质不同，概率图模型可大致分为两类：第一类是使用有向无环图表示变量间的依赖关系，称为有向图模型或贝叶斯网（Bayesian network）；第二类是使用无向图表示变量间的相关关系，称为无向图模型或马尔可夫网（Markov network）.</p>
<p>隐马尔可夫模型（Hidden Markov Model，简称HMM）是结构最简单的动态贝叶斯网（dynamic Bayesian network），这是一种著名的有向图模型，主要用于时序数据建模，在语音识别、自然语言处理等领域有广泛应用.<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200211224412234.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200211224412234.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =400x200)</p>
<h1 id="十五、规则学习"><a href="#十五、规则学习" class="headerlink" title="十五、规则学习"></a>十五、规则学习</h1><p>机器学习中的“规则”（rule）通常是指语义明确、能描述数据分布所隐含的客观规律或领域概念、可写成“若……，则……”形式的逻辑规则<br>[Firnkranz et al.，2012].“规则学习”（rule learning）是从训练数据中学习出一组能用于对未见示例进行判别的规则.</p>
<p>与神经网络、支持向量机这样的“黑箱模型”相比，规则学习具有更好的可解释性，能使用户更直观地对判别过程有所了解.另一方面，数理逻辑具有极强的表达能力，绝大多数人类知识都能通过数理逻辑进行简洁的刻画和表达.规则学习能更自然地在学习过程中引入领域知识.此外，逻辑规则的抽象描述能力在处理一些高度复杂的AI任务时具有显著的优势，例如在问答系统中有时可能遇到非常多、甚至无穷种可能的答案，此时若能基于逻辑规则进行抽象表述或者推理，则将带来极大的便利.</p>
<h1 id="十六、强化学习"><a href="#十六、强化学习" class="headerlink" title="十六、强化学习"></a>十六、强化学习</h1><p>我们考虑一下如何种西瓜.种瓜有许多步骤，从一开始的选种，到定期浇水、施肥、除草、杀虫，经过一段时间才能收获西瓜.通常要等到收获后，我们才知道种出的瓜好不好.若将得到好瓜作为辛勤种瓜劳动的奖赏，则在种瓜过程中当我们执行某个操作（例如，施肥）时，并不能立即获得这个最终奖赏，甚至难以判断当前操作对最终奖赏的影响，仅能得到一个当前反馈（例如，瓜苗看起来更健壮了）.我们需多次种瓜，在种瓜过程中不断摸索，然后才能总结出较好的种瓜策略.这个过程抽象出来，就是“强化学习”（reinforcement learning）.</p>
<p>机器要做的是通过在环境中不断地尝试而学得一个“策略”（policy）π，根据这个策略，在状态x下就能得知要执行的动作a=π（x），例如看到瓜苗状态是缺水时，能返回动作“浇水”.</p>
<p>策略的优劣取决于长期执行这一策略后得到的累积奖赏，例如某个策略使得瓜苗枯死，它的累积奖赏会很小，另一个策略种出了好瓜，它的累积奖赏会很大.在强化学习任务中，学习的目的就是要找到能使长期累积奖赏最大化的策略长期累积奖赏有多种计算方式</p>
<p>读者也许已经感觉到强化学习与监督学习的差别.若将这里的“状态”对应为监督学习中的“示例”、“动作”对应为“标记”，则可看出，强化学习中的“策略”实际上就相当于监督学习中的“分类器”（当动作是离散的）或“回归器”（当动作是连续的），模型的形式并无差别.但不同的是，在强化学习中并没有监督学习中的有标记样本（即“示例-标记”对），换言之，没有人直接告诉机器在什么状态下应该做什么动作，只有等到最终结果揭晓，才能通过“反思”之前的动作是否正确来进行学习.因此，强化学习在某种意义上可看作具有“延迟标记信息”的监督学习问题</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%90%86%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%90%86%E8%A7%A3/" class="post-title-link" itemprop="url">卷积的理解</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:56:00 / Modified: 12:56:01" itemprop="dateCreated datePublished" datetime="2020-03-05T12:56:00+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="卷积的定义"><a href="#卷积的定义" class="headerlink" title="卷积的定义"></a>卷积的定义</h1><p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216134122403.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216134122403.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =500x300 )</p>
<h1 id="卷积大白话"><a href="#卷积大白话" class="headerlink" title="卷积大白话"></a>卷积大白话</h1><p>卷积是一种运算，比加减乘除稍复杂的运算。<br>1.f(x)和g(y)是两个一元函数   // f(x)和g(y)分别是x和y的一个映射，这个映射是任意的。<br>2.以x为横轴，y为纵轴建立直角坐标系<br>3.定义：U(x,y) = f(x) * g(y)   //函数U不是x、y的函数，而是x、y映射的函数，即f(x)、g(y)的函数<br>4.求直线 y = -x + n 上的所有点，的U(x,y)，的和，这个和就叫f(x)和g(y)的卷积。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216140942733.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216140942733.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =400x500)</p>
<h1 id="矩阵卷积"><a href="#矩阵卷积" class="headerlink" title="矩阵卷积"></a>矩阵卷积</h1><p>向量的內积是<strong>向量变标量</strong>的一种运算方法<br>矩阵的內积是<strong>矩阵变向量</strong>的一种运算方法<br>矩阵的卷积是<strong>矩阵变标量</strong>的一种运算方法</p>
<h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p>图像处理中，图像可以表示为矩阵形式，如我们在原始图像矩阵中，取出像素（u,v）处的3x3矩阵：<br><img src="https://img-blog.csdnimg.cn/20200216152628552.png#pic_center" alt="在这里插入图片描述"><br>对图像的处理函数（如平滑，或者边缘提取），也可以用一个矩阵来表示，如：<br><img src="https://img-blog.csdnimg.cn/20200216152710682.png#pic_center" alt="在这里插入图片描述"><br>那么矩阵 f 和 g 在（u，v）处的卷积 ( f * g )(u,v) 该如何计算呢？</p>
<p><img src="https://img-blog.csdnimg.cn/20200216152928703.png#pic_center" alt="在这里插入图片描述"><br>即：<br>1.把矩阵 g 旋转180^o^<br>2.把矩阵 f 和矩阵 g 对应位置元素相乘，再相加。<br>3.得到一个标量。</p>
<p><img src="https://img-blog.csdnimg.cn/20200216153041546.png#pic_center" alt="在这里插入图片描述"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/%E7%90%86%E8%A7%A3%E4%B8%8A%E9%87%87%E6%A0%B7%E3%80%81%E4%B8%8B%E9%87%87%E6%A0%B7%E3%80%81%E6%B1%A0%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/%E7%90%86%E8%A7%A3%E4%B8%8A%E9%87%87%E6%A0%B7%E3%80%81%E4%B8%8B%E9%87%87%E6%A0%B7%E3%80%81%E6%B1%A0%E5%8C%96/" class="post-title-link" itemprop="url">理解上采样、下采样、池化</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:55:40 / Modified: 12:55:47" itemprop="dateCreated datePublished" datetime="2020-03-05T12:55:40+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="上采样、下采样"><a href="#上采样、下采样" class="headerlink" title="上采样、下采样"></a>上采样、下采样</h1><h2 id="缩小图像（或称为下采样（subsampled）或降采样（downsampled））"><a href="#缩小图像（或称为下采样（subsampled）或降采样（downsampled））" class="headerlink" title="缩小图像（或称为下采样（subsampled）或降采样（downsampled））"></a>缩小图像（或称为下采样（subsampled）或降采样（downsampled））</h2><p>主要目的有两个：<br>1、使得图像符合显示区域的大小；<br>2、生成对应图像的缩略图。</p>
<h2 id="放大图像（或称为上采样（upsampling）或图像插值（interpolating））"><a href="#放大图像（或称为上采样（upsampling）或图像插值（interpolating））" class="headerlink" title="放大图像（或称为上采样（upsampling）或图像插值（interpolating））"></a>放大图像（或称为上采样（upsampling）或图像插值（interpolating））</h2><p>主要目的是：<br>放大原图像,从而可以显示在更高分辨率的显示设备上。对图像的缩放操作并不能带来更多关于该图像的信息, 因此图像的质量将不可避免地受到影响。然而，确实有一些缩放方法能够增加图像的信息，从而使得缩放后的图像质量超过原图质量的。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>下采样原理：对于一幅图像I尺寸为M<em>N，对其进行s倍下采样，即得到(M/s)</em>(N/s)尺寸的得分辨率图像，当然s应该是M和N的公约数才行，如果考虑的是矩阵形式的图像，就是把原始图像s*s窗口内的图像变成一个像素，这个像素点的值就是窗口内所有像素的均值：</p>
<p>上采样原理：图像放大几乎都是采用内插值方法，即在原有图像像素的基础上在像素点之间采用合适的插值算法插入新的元素。如最近邻插值，双线性插值，均值插值，中值插值等方法。各种插值方法都有各自的优缺点。</p>
<p>无论缩小图像（下采样）还是放大图像（上采样），采样方式有很多种。</p>
<h1 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h1><p>池化（Pooling）是卷积神经网络中一个重要的概念，它实际上是一种形式的降采样。它会压缩输入的特征图，一方面减少了特征，导致了参数减少，进而简化了卷积网络计算时的复杂度；另一方面保持了特征的某种不变性（旋转、平移、伸缩等）。</p>
<p>池化操作主要有两种，一种是平均池化(Average Pooling)，即对邻域内的特征点求平均；另一种是最大池化(Max Pooling)，即对邻域内的特征点取最大。最大池化（Max pooling）是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。直觉上，这种机制能够有效地原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了过拟合。通常来说，CNN的卷积层之间都会周期性地插入池化层。</p>
<p>池化层通常会分别作用于每个输入的特征并减小其大小。当前最常用形式的池化层是每隔2个元素从图像划分出2*2的区块，然后对每个区块中的4个数取最大值。这将会减少75%的数据量。<br><img src="https://img-blog.csdnimg.cn/20200216214827576.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>池化方法特征提取误差主要来自两个部分：一是，邻域大小受限造成了估计值方差增大；二是，卷积层参数误差造成了估计均值的偏移。一般来说，在图像研究领域，对图像进行平均池化操作能减少第一种误差，同时更多地保留图像的背景信息；而另一方面，最大池化能减小第二种误差，更多地保留纹理信息。因此在进行卷积神经网络结构设计时，这两种池化方式往往交替使用。</p>
<p>简而言之，池化就是去除杂余信息，保留关键信息</p>
<h2 id="池化的作用"><a href="#池化的作用" class="headerlink" title="池化的作用"></a>池化的作用</h2><p>池化操作后的结果相比其输入缩小了。池化层的引入是仿照人的视觉系统对视觉输入对象进行降维和抽象。在卷积神经网络过去的工作中，研究者普遍认为池化层有如下三个功效：</p>
<p>1.特征不变形：池化操作是模型更加关注是否存在某些特征而不是特征具体的位置。<br>2.特征降维：池化相当于在空间范围内做了维度约减，从而使模型可以抽取更加广范围的特征。同时减小了下一层的输入大小，进而减少计算量和参数个数。<br>3.在一定程度上防止过拟合，更方便优化。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/%E5%88%9D%E8%AF%86%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/%E5%88%9D%E8%AF%86%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/" class="post-title-link" itemprop="url">初识语义分割</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:55:30 / Modified: 12:55:37" itemprop="dateCreated datePublished" datetime="2020-03-05T12:55:30+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="语义分割基础知识"><a href="#语义分割基础知识" class="headerlink" title="语义分割基础知识"></a>语义分割基础知识</h1><p>在计算机视觉的语义感知部分主要分为图像分类、目标检测、语义分割、实例分割等。</p>
<p>语义分割是一种视觉场景理解任务，它从像素水平上理解、识别图片内容，然后根据语义信息进行图像分割；它是一种稠密标签，目的是预测输入图片中每一个像素的类别标签。</p>
<p>现有语义分割技术会出现分割图像边缘粗糙的现象，这在应用领域造成了一定的影响，尤其在对分割精确度要求较高的医学领域；而且在语义分割模型训练的过程中主要依赖人工标注的数据集样本，耗费较多的人力物力。因此，如何对分割图像边缘粗糙问题进行处理，提高分割精确度；如何降低语义分割模型训练对人工标注的数据集样本的依赖程度，提升网络的泛化能力，对语义分割技术的广泛应用具有非常重要的现实意义。</p>
<h1 id="图像语义分割综述"><a href="#图像语义分割综述" class="headerlink" title="图像语义分割综述"></a>图像语义分割综述</h1><p>转载自<a href="https://zhuanlan.zhihu.com/p/37801090" target="_blank" rel="noopener">这儿</a></p>
<h2 id="什么是语义分割"><a href="#什么是语义分割" class="headerlink" title="什么是语义分割"></a>什么是语义分割</h2><p>语义分割是在像素级别上的分类，属于同一类的像素都要被归为一类，因此语义分割是从像素级别来理解图像的。比如说如下的照片，属于人的像素都要分成一类，属于摩托车的像素也要分成一类，除此之外还有背景像素也被分为一类。注意语义分割不同于实例分割，举例来说，如果一张照片中有多个人，对于语义分割来说，只要将所由人的像素都归为一类，但是实例分割还要将不同人的像素归为不同的类。也就是说实例分割比语义分割更进一步。</p>
<h2 id="语义分割的思路"><a href="#语义分割的思路" class="headerlink" title="语义分割的思路"></a>语义分割的思路</h2><h3 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h3><p>在深度学习方法流行之前，TextonForest和基于随机森林分类器等语义分割方法是用得比较多的方法。不过在深度卷积网络流行之后，深度学习方法比传统方法提升了很多，所以这里就不详细讲传统方法了。</p>
<h3 id="深度学习方法"><a href="#深度学习方法" class="headerlink" title="深度学习方法"></a>深度学习方法</h3><p>深度学习方法在语义分割上得到了巨大成功，深度学习方法解决语义分割问题可以概括为几种思路。下面进行详细介绍。</p>
<p>1.Patch classification<br>最初的深度学习方法应用于图像分割就是Patch classification。Patch classification方法，顾名思义，图像是切成块喂给深度模型的，然后对像素进行分类。使用图像块的主要原因是全连接层需要固定大小的图像。</p>
<p>2.全卷积方法<br>2014年，全卷积网络（FCN）横空出世，FCN将网络全连接层用卷积取代，因此使任意图像大小的输入都变成可能，而且速度比Patch classification方法快很多。</p>
<p>尽管移除了全连接层，但是CNN模型用于语义分割还存在一个问题，就是下采样操作（比如，pooling）。pooling操作可以扩大感受野因而能够很好地整合上下文信息（context中文称为语境或者上下文，通俗的理解就是综合了更多的信息来进行决策），对high-level的任务（比如分类），这是很有效的。但同时，由于pooling下采样操作，使得分辨率降低，因此削弱了位置信息，而语义分割中需要score map和原图对齐，因此需要丰富的位置信息。</p>
<p>3.encoder-decoder架构<br>encoder-decoder是基于FCN的架构。encoder由于pooling逐渐减少空间维度，而decoder逐渐恢复空间维度和细节信息。通常从encoder到decoder还有shortcut connetction（捷径连接，也就是跨层连接）。其中U-net就是这种架构很流行的一种，如下图：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216121702929.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216121702929.png#pic_center</a> =500x300)<br>4.空洞卷积<br>dilated/atrous （空洞卷积）架构，这种结构代替了pooling，一方面它可以保持空间分辨率，另外一方面它由于可以扩大感受野因而可以很好地整合上下文信息。如下图：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216121135445.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216121135445.png#pic_center</a> =500x300)<br>5.条件随机场<br>除了以上思路，还有一种对分割结果进行后处理的方法，那就是条件随机场(Conditional Random Fields (CRFs))后处理用来改善分割效果。DeepLab系列文章基本都采用这种后处理方法，可以较好地改善分割结果，如下图：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216121319384.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216121319384.png#pic_center</a> =500x300)</p>
<h2 id="深度学习语义分割方法"><a href="#深度学习语义分割方法" class="headerlink" title="深度学习语义分割方法"></a>深度学习语义分割方法</h2><p>现在的深度学习语义分割模型基本上都是基于FCN发展而来的，它是开山鼻祖，一张图概括FCN的延伸方法：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216121544933.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216121544933.png#pic_center</a> =500x300)<br>各方法的详细信息<br><img src="https://img-blog.csdnimg.cn/20200216121838301.png" alt="在这里插入图片描述"></p>
<h3 id="各方法的简要介绍"><a href="#各方法的简要介绍" class="headerlink" title="各方法的简要介绍"></a>各方法的简要介绍</h3><p>下面简单总结一些从FCN进行改进的几种架构，关于每种架构的详细解读请看专栏中其他文章。</p>
<h4 id="1-FCN"><a href="#1-FCN" class="headerlink" title="1.FCN"></a>1.FCN</h4><blockquote>
<p>Fully Convolutional Networks for Semantic Segmentation<br>Submitted on 14 Nov 2014</p>
</blockquote>
<p>主要贡献</p>
<p>1.使端对端的卷积语义分割网络变得流行起来。<br>2.通过deconvolutional layers进行上采样。<br>3.通过skip connection改善了上采样的粗糙度。</p>
<p>概要</p>
<p>1.<strong>全卷积化(Fully Convolutional)</strong>：用于解决逐像素(pixel-wise)的预测问题。通过将基础网络(例如VGG)最后面几个全连接层换成卷积层，可实现任意大小的图像输入，并且输出图像大小与输入相对应；<br>2.<strong>反卷积(deconvolution)</strong> ：上采样操作，用于恢复图片尺寸，方便后续进行逐像素预测;<br>3.<strong>跳跃结构(skip architecture)</strong>：用于融合高低层特征信息。通过跨层连接的结构，结合了网络浅层的细(fine-grain)粒度信息信息以及深层的粗糙(coarse)信息，以实现精准的分割任务。</p>
<p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216123101449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216123101449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =500x300)<br>FCN是基于深度学习的语义分割的开山之作，尽管现在很多方法都超越了FCN，但它的思想仍然有很重要的意义。</p>
<h4 id="2-Segnet"><a href="#2-Segnet" class="headerlink" title="2. Segnet"></a>2. Segnet</h4><blockquote>
<p>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation<br>Submitted on 2 Nov 2015</p>
</blockquote>
<p>主要贡献</p>
<p>使用Maxpooling indices来增强位置信息。</p>
<p>简要概述</p>
<p>FCN的upconvolution层+shortcut connections产生的分割图比较粗糙，因此SegNet增加了更多的shortcut connections。不过，SegNet并不是直接将encoder的特征进行直接复制，而是对maxpooling中的indices进行复制，这使得SegNet的效率更高。</p>
<p>maxpooling 的indices复制原理如下：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216220438471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216220438471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =300x)<br>FCN和SegNet都是encoder-decoder架构。<br>SegNet的benchmark表现太差了，不建议用这个网络。</p>
<h4 id="3-Dilated-convolution"><a href="#3-Dilated-convolution" class="headerlink" title="3. Dilated convolution"></a>3. Dilated convolution</h4><p>论文信息</p>
<blockquote>
<p>Multi-Scale Context Aggregation by Dilated Convolutions<br>Submitted on 23 Nov 2015</p>
</blockquote>
<p>创新点</p>
<p>1.使用空洞卷积用来进行稠密预测（dense prediction）。<br>2.提出上下文模块（context module），使用空洞卷积（Dilated Convolutions）来进行多尺度信息的的整合。</p>
<p>简要解释</p>
<p>pooling操作可以增大感受野，对于图像分类任务来说这有很大好处，但由于pooling操作降低了分辨率，这对语义分割来说很不利。因此作者提出一种叫做dilated convolution的操作来解决这个问题。dilated卷积(在deeplab中称为atrous卷积)。可以很好地提升感受野的同时可以保持空间分辨率。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216220838429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216220838429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =500x)<br>网络架构有两种，一种是前端网络，另外一种是前端网络+上下文模块，分别介绍如下：</p>
<p>将VGG网络的最后两个pooling层给拿掉了，之后的卷积层被dilated 卷积取代。并且在pool3和pool4之间空洞卷积的空洞率=2，pool4之后的空洞卷积的空洞率=4。作者将这种架构称为前端（front-end）。</p>
<p>除了前端网络之外，作者还设计了一种叫做上下文模块（context module）的架构，加在前端网络之后。上下文模块中级联了多种不同空洞率的空洞卷积，使得多尺度的上下文信息可以得到整合，从而改善前端网络预测的效果。需要注意的是前端网络和上下文模块是分开训练的，因为作者在实验中发现，如果是联合在一起进行端对端的训练并不能改善性能。</p>
<p>需要特别注意的是，网络输出的分割图并不是和原始图像大小一样的，而是其1/8，需要对输出的分割图进行线性插值才能得到最终的分割结果。这种做法也是很多其他的方法都使用的。</p>
<h4 id="4-DeepLab-v1-v2"><a href="#4-DeepLab-v1-v2" class="headerlink" title="4. DeepLab(v1,v2)"></a>4. DeepLab(v1,v2)</h4><p><strong>论文信息</strong></p>
<blockquote>
<p>v1: Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs<br>Submitted on 22 Dec 2014<br>v2 : DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs<br>Submitted on 2 Jun 2016</p>
</blockquote>
<p><strong>主要贡献</strong></p>
<p>1.使用atrous卷积，也就是后来的空洞卷积，扩大感受野，保持分辨率。<br>2.提出了atrous spatial pyramid pooling (ASPP)，整合多尺度信息。<br>3.使用全连接条件随机场（fully connected CRF)进行后处理，改善分割结果。</p>
<p><strong>简要概述</strong></p>
<p>1.空洞卷积可以在不增加参数的情况下增加感受野。<br>2.通过两种方式来进行多尺度的处理：A.将原始图像的多种尺度喂给网络进行训练。B.通过平行的不同空洞率的空洞卷积层来获得。<br>3.通过全连接条件随机场来进行后处理，以改善分割结果。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216221235418.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216221235418.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =700x)</p>
<h4 id="5-RefineNet"><a href="#5-RefineNet" class="headerlink" title="5.RefineNet"></a>5.RefineNet</h4><p><strong>论文信息</strong></p>
<blockquote>
<p>RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation<br>Submitted on 20 Nov 2016</p>
</blockquote>
<p><strong>主要贡献</strong></p>
<p>精心设计了encoder-decoder架构中的decoder部分，使得性能提升。<br>整个网络的设计都遵循residual connections，网络表达能力更强，梯度更容易反向传播。</p>
<p><strong>简要概述</strong></p>
<p>作者提出空洞卷积方法应用于语义分割也是有缺点的，包括：</p>
<p>因为使用了大分辨率的feature map，因此计算代价大，并且需要大量的内存。对于这个问题，DeepLab的做法是只预测原始输入的1／8。<br>本文提出使用encoder-decoder架构。encoder部分是RESNET-101。decoder具有RefineNet blocks，它将此前的RefineNet blocks的低分辨率特征和encoder部分高分辨率特征进行concatenate/fuse。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216221557410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216221557410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =700x)<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216221623130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216221623130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =700x)</p>
<h4 id="6-PSPNet"><a href="#6-PSPNet" class="headerlink" title="6. PSPNet"></a>6. PSPNet</h4><p>论文信息</p>
<blockquote>
<p>Pyramid Scene Parsing Network<br>Submitted on 4 Dec 2016</p>
</blockquote>
<p><strong>主要贡献</strong></p>
<p>使用pyramid pooling整合context。<br>使用auxiliary loss。</p>
<p><strong>概要</strong></p>
<p>骨架网络使用Resnet，并在此基础上加上pyramid pooling module。该模块用到了很多kernel大小不一的pooling 。将pooling的结果再上采样，经过concatenate进行融合。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216221725398.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216221725398.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =700x)<br>在RESNET的第四阶段（即输入到金字塔池模块）之后，应用auxiliary loss。这种方法在别的地方也被称为intermediate supervision。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/%E3%80%8A%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%E3%80%8B%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/%E3%80%8A%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%E3%80%8B%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">《视觉SLAM十四讲》笔记</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:55:16 / Modified: 12:55:22" itemprop="dateCreated datePublished" datetime="2020-03-05T12:55:16+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-视觉SLAM-系统概述"><a href="#1-视觉SLAM-系统概述" class="headerlink" title="1. 视觉SLAM 系统概述"></a>1. 视觉SLAM 系统概述</h1><p>SLAM 是Simultaneous Localization and Mapping 的缩写，中文译作“同时定位与地图构建” 。它是指搭载特定传感器的主体，在没有环境先验信息的情况下，于运动过程中建立环境的模型，同时估计自己的运动。如果这里的传感器主要为相机，那就称为“视觉SLAM”。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216222952173.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216222952173.png#pic_center</a> =300x)<br>视觉SLAM流程分为以下几步</p>
<ul>
<li>传感器信息读取。在视觉SLAM 中主要为相机图像信息的读取和预处理。<ul>
<li>视觉里程计。视觉里程计任务是估算相邻图像间相机的运动，以及局部地图的样子。</li>
<li>后端优化。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行优化，得到全局一致的轨迹和地图。</li>
<li>回环检测。回环检测判断机器人是否曾经到达过先前的位置。如果检测到回环，它会把信息提供给后端进行处理。</li>
<li>建图。它根据估计的轨迹，建立与任务要求对应的地图。<h1 id="2-前端视觉里程计"><a href="#2-前端视觉里程计" class="headerlink" title="2. 前端视觉里程计"></a>2. 前端视觉里程计</h1>视觉里程计根据相邻图像的信息，估计出粗略的相机运动，给后端提供较好的初始值。视觉里程计的算法主要分为两个大类：特征点法和直接法。基于特征点法的前端，长久以来（直到现在）被认为是视觉里程计的主流方法。它运行稳定，对光照、动态物体不敏感，是目前比较成熟的解决方案。<br>核心问题：如何根据图像估计相机运动。</li>
</ul>
</li>
</ul>
<h2 id="2-1-特征点法"><a href="#2-1-特征点法" class="headerlink" title="2.1. 特征点法"></a>2.1. 特征点法</h2><p>特征点：由关键点和描述子两部分组成。关键点是指该特征点在图像里的位置，有些特征点还具有朝向、大小等信息。描述子通常是一个向量，按照某种人为设计的方式，描述了该关键点周围像素的信息。描述子是按照“外观相似的特征应该有相似的描述子”的原则设计的。</p>
<p>特征匹配：视觉SLAM 中极为关键的一步，特征匹配解决了SLAM 中的数据关联问题，即确定当前看到的路标与之前看到的路标之间的对应关系。通过对图像与图像，或者图像与地图之间的描述子进行准确的匹配，我们可以为后续的姿态估计，优化等操作减轻大量负担。匹配方法：暴力匹配等。</p>
<p>当相机为单目时，我们只知道2D 的像素坐标，因而问题是根据两组2D 点估计运动。该问题用对极几何来解决。</p>
<p>当相机为双目、RGB-D 时，或者我们通过某种方法得到了距离信息，那问题就是根据两组3D 点估计运动。该问题通常用ICP 来解决。</p>
<p>如果我们有3D 点和它们在相机的投影位置，也能估计相机的运动。该问题通过PnP求解。</p>
<h3 id="2-1-1-2D-2D-对极几何"><a href="#2-1-1-2D-2D-对极几何" class="headerlink" title="2.1.1. 2D-2D: 对极几何"></a>2.1.1. 2D-2D: 对极几何</h3><p>假设我们从两张图像中，得到了一对配对好的特征点，如果我们有若干对这样的匹配点，就可以通过这些二维图像点的对应关系，恢复出在两帧之间摄像机的运动。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216223504198.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216223504198.png#pic_center</a> =300x)<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216223545251.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216223545251.png#pic_center</a> =300x)<br>八点法+奇异值分解即可求得旋转矩阵R和位移向量t。</p>
<h3 id="2-1-2-三角测量"><a href="#2-1-2-三角测量" class="headerlink" title="2.1.2. 三角测量"></a>2.1.2. 三角测量</h3><p>在得到运动之后，下一步我们需要用相机的运动估计特征点的空间位置。在单目SLAM 中，仅通过单张图像无法获得像素的深度信息，我们需要通过三角测量（Triangulation）（或三角化）的方法来估计地图点的深度。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216223738272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216223738272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =300x)![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216223807598.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216223807598.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =300x)</p>
<h3 id="2-1-3-3D-2D-PnP"><a href="#2-1-3-3D-2D-PnP" class="headerlink" title="2.1.3.  3D-2D: PnP"></a>2.1.3.  3D-2D: PnP</h3><p>PnP（Perspective-n-Point）是求解3D 到2D 点对运动的方法。它描述了当我们知道n 个3D 空间点以及它们的投影位置时，如何估计相机所在的位姿</p>
<p>如果两张图像中，其中一张特征点的3D 位置已知，那么最少只需三个点对（需要至少一个额外点验证结果）就可以估计相机运动</p>
<p>在双目或RGB-D 的视觉里程计中，我们可以直接使用PnP 估计相机运动。而在单目视觉里程计中，必须先进行初始化，然后才能使用PnP</p>
<p>PnP 问题有很多种求解方法，例如用三对点估计位姿的P3P，直接线性变换（DLT），非线性优化构建最小二乘问题并迭代求解</p>
<h3 id="2-1-4-3D-3D-ICP"><a href="#2-1-4-3D-3D-ICP" class="headerlink" title="2.1.4. 3D-3D: ICP"></a>2.1.4. 3D-3D: ICP</h3><p>假设一组配对好的3D 点（比如对两个RGB-D 图像进行了匹配）：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216224001316.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216224001316.png#pic_center</a> =300x)<br>现在，找一个欧氏变换R; t，使得：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216224025260.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216224025260.png#pic_center</a> =300x)<br>这个问题可以用迭代最近点（Iterative Closest Point, ICP）求解<br>ICP 的求解也分为两种方式：利用线性代数的求解（主要是SVD），以及利用非线性优化方式的求解（类似于Bundle Adjustment）。</p>
<h3 id="2-1-5-特征点法的缺陷"><a href="#2-1-5-特征点法的缺陷" class="headerlink" title="2.1.5. 特征点法的缺陷"></a>2.1.5. 特征点法的缺陷</h3><p><strong>特征点法存在的问题：</strong></p>
<ul>
<li>关键点的提取与描述子的计算非常耗时</li>
<li>使用特征点时，忽略了除特征点以外的所有信息。一张图像有几十万个像素，而特征点只有几百个。只使用特征点丢弃了大部分可能有用的图像信息。</li>
<li>相机有时会运动到特征缺失的地方，往往这些地方没有明显的纹理信息。</li>
</ul>
<p><strong>克服对策：</strong></p>
<ul>
<li>只计算关键点，不计算描述子。同时，使用光流法（Optical Flow）来跟踪特征点的运动。这样可以回避计算和匹配描述子带来的时间，而光流本身的计算时间要小于特征点的计算与匹配。</li>
<li>只计算关键点，不计算描述子。同时，使用直接法（Direct Method）来计算特征点在下一时刻图像的位置。这同样可以跳过描述子的计算过程，而且直接法的计算更加简单。</li>
<li><h2 id="2-2-光流法"><a href="#2-2-光流法" class="headerlink" title="2.2. 光流法"></a>2.2. 光流法</h2>光流法仍然使用特征点，只是把匹配描述子替换成了光流跟踪，估计相机运动时仍使用对极几何、PnP 或ICP 算法。</li>
</ul>
<p>光流是一种描述像素随着时间，在图像之间运动的方法，计算部分像素运动的称为稀疏光流，计算所有像素的称为稠密光流。</p>
<p>LK光流是光流法的一种，它对观测量做了“灰度不变”假设和“某个窗口内的像素具有相同的运动”假设。因而能够从前后两幅图片中追踪到同一个点的位置移动。</p>
<p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216224237540.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216224237540.png#pic_center</a> =300x)<br>在实际应用中，LK光流的作用就是跟踪特征点。与对每一帧提取特征点相比，使用LK光流只需要提取一次特征点，后续视频帧只需要跟踪就可以了，节约了许多特征提取时间。</p>
<h2 id="2-3-直接法"><a href="#2-3-直接法" class="headerlink" title="2.3. 直接法"></a>2.3. 直接法</h2><p>在直接法中，根据图像的像素灰度信息同时估计相机的运动和点的投影，不要求提取到的点必须为角点。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216224336943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216224336943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =400x)<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216224410575.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216224410575.png#pic_center</a> =400x)<br>直接法的思路是根据当前相机的位姿估计值，来寻找p2 的位置。但若相机位姿不够好，p2 的外观和p1 会有明显差别。于是，为了减小这个差别，我们优化相机的位姿，来寻找与p1 更相似的p2。光度误差（Photometric Error），也就是P 的两个像的亮度误差：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/2020021622444270.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/2020021622444270.png#pic_center</a> =300x)<br>优化目标为该误差的二范数</p>
<p>能够做这种优化的理由，仍是基于灰度不变假设。在直接法中，假设一个空间点在各个视角下，成像的灰度是不变的。有许多个（比如N 个）空间点Pi，那么，整个相机位姿估计问题变为：<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200216224522325.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200216224522325.png#pic_center</a> =400x)<br>然后使用G-N 或L-M 计算增量，迭代求解。</p>
<h1 id="3-后端优化"><a href="#3-后端优化" class="headerlink" title="3. 后端优化"></a>3. 后端优化</h1><p>前端视觉里程计能给出一个短时间内的轨迹和地图，但由于不可避免的误差累积，这个地图在长时间内是不准确的。所以，在视觉里程计的基础上，我们还希望构建一个尺度、规模更大的优化问题，以考虑长时间内的最优轨迹和地图。</p>
<h2 id="3-1-线性系统和卡尔曼滤波KF"><a href="#3-1-线性系统和卡尔曼滤波KF" class="headerlink" title="3.1. 线性系统和卡尔曼滤波KF"></a>3.1. 线性系统和卡尔曼滤波KF</h2><p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/2020021718160174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/2020021718160174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =400x)</p>
<h2 id="3-2-非线性系统和扩展的卡尔曼滤波EKF"><a href="#3-2-非线性系统和扩展的卡尔曼滤波EKF" class="headerlink" title="3.2. 非线性系统和扩展的卡尔曼滤波EKF"></a>3.2. 非线性系统和扩展的卡尔曼滤波EKF</h2><p>把卡尔曼滤波器的结果拓展到非线性系统中来，称为扩展卡尔曼滤波器（ExtendedKalman Filter，EKF）。通常的做法是，在某个点附近考虑运动方程以及观测方程的一阶泰勒展开，只保留一阶项，即线性的部分，然后按照线性系统进行推导。</p>
<p>先定义一个卡尔曼增益Kk：</p>
<p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200217181656173.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200217181656173.png#pic_center</a> =300x)<br>在卡尔曼增益的基础上，后验概率的形式为：<br><img src="https://img-blog.csdnimg.cn/20200217181722547.png#pic_center" alt="在这里插入图片描述"></p>
<h2 id="3-3-光束法平差BA"><a href="#3-3-光束法平差BA" class="headerlink" title="3.3. 光束法平差BA"></a>3.3. 光束法平差BA</h2><p>所谓的Bundle Adjustment，是指从视觉重建中提炼出最优的3D 模型和相机参数（内参数和外参数）。从每一个特征点反射出来的几束光线（bundles of light rays），在我们把相机姿态和特征点空间位置做出最优的调整(adjustment) 之后，最后收束到相机光心的这个过程，简称为BA。<br><img src="https://img-blog.csdnimg.cn/20200217181837888.png#pic_center" alt="在这里插入图片描述"><br>左侧的p 是全局坐标系下的三维坐标点，右侧的us， vs 是该点在图像平面上的最终像素坐标。<br>系统的观测方程为：</p>
<p>z=h(T,p)</p>
<p>其中，T为相机的位姿变换矩阵,其对应的李代数为ξ。<br>则以最小二乘的角度考虑，可得此次观测的误差：</p>
<p>e=z-h(T,p)</p>
<p>然后，把其他时刻的观测量也考虑进来，我们可以给误差添加一个下标。设zij 为在位姿ξi 处观察路标pj 产生的数据，那么整体的代价函数（Cost Function）为：<br><img src="https://img-blog.csdnimg.cn/20200217181939854.png#pic_center" alt="在这里插入图片描述"><br>对这个最小二乘进行求解，相当于对位姿和路标同时作了调整，也就是所谓的BA。</p>
<h2 id="3-4-位姿图优化"><a href="#3-4-位姿图优化" class="headerlink" title="3.4. 位姿图优化"></a>3.4. 位姿图优化</h2><p>构建一个只有轨迹的图优化，而位姿节点之间的边，可以由两个关键帧之间通过特征匹配之后得到的运动估计来给定初始值。一旦初始估计完成，我们就不再优化那些路标点的位置，而只关心所有的相机位姿之间的联系了。通过这种方式，我们省去了大量的特征点优化的计算，只保留了关键帧的轨迹，从而构建了所谓的位姿图<br><img src="https://img-blog.csdnimg.cn/20200217182110268.png#pic_center" alt="在这里插入图片描述"><br>位姿图优化中的节点表示相机位姿，边表示两个节点之间相对运动的估计。<br>边可表示为<br><img src="https://img-blog.csdnimg.cn/20200217182156248.png#pic_center" alt="在这里插入图片描述"><br>或按李群的写法：<br><img src="https://img-blog.csdnimg.cn/20200217182226320.png#pic_center" alt="在这里插入图片描述"><br>然后构建误差eij：<br><img src="https://img-blog.csdnimg.cn/20200217182249842.png#pic_center" alt="在这里插入图片描述"><br>所有的位姿顶点和位姿——位姿边构成了一个图优化，本质上是一个最小二乘问题，优化变量为各个顶点的位姿，边来自于位姿观测约束。记ε 为所有边的集合，那么总体目标函数为：<br><img src="https://img-blog.csdnimg.cn/20200217182312196.png#pic_center" alt="在这里插入图片描述"><br>我们依然可以用Gauss-Newton、Levenberg-Marquardt 等方法求解此问题，除了用李代数表示优化位姿以外，别的都是相似的。</p>
<h1 id="4-回环检测"><a href="#4-回环检测" class="headerlink" title="4. 回环检测"></a>4. 回环检测</h1><h2 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1. 概述"></a>4.1. 概述</h2><h3 id="4-1-1-回环检测的意义"><a href="#4-1-1-回环检测的意义" class="headerlink" title="4.1.1. 回环检测的意义"></a>4.1.1. 回环检测的意义</h3><p>前端提供特征点的提取和轨迹、地图的初值，而后端负责对这所有的数据进行优化。然而，如果像VO 那样仅考虑相邻时间上的关联，那么，之前产生的误差将不可避免地累计到下一个时刻，使得整个SLAM 会出现累积误差。长期估计的结果将不可靠，或者说，我们无法构建全局一致的轨迹和地图。</p>
<p>回环检测模块，能够给出除了相邻帧之外的，一些时隔更加久远的约束。回环检测的关键，就是如何有效地检测出相机经过同一个地方这件事。如果能够成功地检测这件事，就可以为后端的Pose Graph 提供更多的有效数据，使之得到更好的估计，特别是得到一个全局一致（Global Consistent）的估计。</p>
<h3 id="4-1-2-核心问题："><a href="#4-1-2-核心问题：" class="headerlink" title="4.1.2. 核心问题："></a>4.1.2. 核心问题：</h3><p>如何计算图像间的相似性</p>
<h3 id="4-1-3-准确率和召回率"><a href="#4-1-3-准确率和召回率" class="headerlink" title="4.1.3. 准确率和召回率"></a>4.1.3. 准确率和召回率</h3><p><img src="https://img-blog.csdnimg.cn/20200217182505929.png#pic_center" alt="在这里插入图片描述"><br>准确率：算法提取的所有回环中，确实是真实回环的概率。<br>召回率：在所有真实回环中，被正确检测出来的概率。</p>
<p>为了评价算法的好坏，我们会测试它在各种配置下的P 和R 值，然后做出一条Precision-Recall 曲线。当用召回率为横轴，用准确率为纵轴时，我们会关心整条曲线偏向右上方的程度、100% 准确率下的召回率，或者50% 召回率时候的准确率，作为评价算法的指标。</p>
<p>值得一提的是，在SLAM 中，我们对准确率要求更高，而对召回率则相对宽容一些。由于假阳性的（检测结果是而实际不是的）回环将在后端的Pose Graph 中添加根本错误的边，有些时候会导致优化算法给出完全错误的结果。而相比之下，召回率低一些，则顶多有部分的回环没有被检测到，地图可能受一些累积误差的影响——然而仅需一两次回环就可以完全消除它们了。所以说在选择回环检测算法时，我们更倾向于把参数设置地更严格一些，或者在检测之后再加上回环验证的步骤。</p>
<h2 id="4-2-词袋模型"><a href="#4-2-词袋模型" class="headerlink" title="4.2. 词袋模型"></a>4.2. 词袋模型</h2><p>词袋，也就是Bag-of-Words（BoW），目的是用“图像上有哪几种特征”来描述一个图像。<br>字典中的单词，假设为w1、w2、w3。然后，对于任意图像A，根据它们含有的单词，可记为：<br><img src="https://img-blog.csdnimg.cn/20200217182622842.png#pic_center" alt="在这里插入图片描述"><br>字典是固定的，所以只要用[1  1  0]T 这个向量就可以表达A 的意义。通过字典和单词，只需一个向量就可以描述整张图像了。</p>
<p>同理，用[2  0  1]T 可以描述图像B。如果只考虑“是否出现”而不考虑数量的话，也可以是[1  0  1]T ，这时候这个向量就是二值的。于是，根据这两个向量，设计一定的计算方式，就能确定图像间的相似性了。当然如果对两个向量求差仍然有一些不同的做法，比如说对于a，b∈ RW，可以计算：<br><img src="https://img-blog.csdnimg.cn/20200217182649201.png#pic_center" alt="在这里插入图片描述"><br>其中范数取L1 范数，即各元素绝对值之和。请注意在两个向量完全一样时，我们将得到1；完全相反时（a 为0 的地方b 为1）得到0。这样就定义了两个描述向量的相似性，也就定义了图像之间的相似程度。</p>
<h2 id="4-3-字典创建"><a href="#4-3-字典创建" class="headerlink" title="4.3. 字典创建"></a>4.3. 字典创建</h2><h3 id="4-3-1-K均值算法"><a href="#4-3-1-K均值算法" class="headerlink" title="4.3.1. K均值算法"></a>4.3.1. K均值算法</h3><p>字典由很多单词组成，而每一个单词代表了一个概念。一个单词与一个单独的特征点不同，它不是从单个图像上提取出来的，而是某一类特征的组合。所以，字典生成问题类似于一个聚类问题。当我们有N 个特征点，想要归成k 个类，那么用K-means 来做，主要有以下几个步骤：</p>
<p><img src="https://img-blog.csdnimg.cn/20200217182738524.png" alt="在这里插入图片描述"></p>
<h3 id="4-3-2-k叉树字典"><a href="#4-3-2-k叉树字典" class="headerlink" title="4.3.2. k叉树字典"></a>4.3.2. k叉树字典</h3><p>使用一种k叉树来表达字典。它的思路很简单，类似于层次聚类，是K-means的直接扩展。假定我们有N 个特征点，希望构建一个深度为d，每次分叉为k 的树，那么做法如下：<br><img src="https://img-blog.csdnimg.cn/2020021718280963.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200217182842827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>实际上，最终我们仍在叶子层构建了单词，而树结构中的中间节点仅供快速查找时使用。这样一个k 分支，深度为d 的树，可以容纳kd个单词。另一方面，在查找某个给定特征对应的单词时，只需将它与每个中间结点的聚类中心比较（一共d 次），即可找到最后的单词，保证了对数级别的查找效率。</p>
<h2 id="4-4-相似度计算"><a href="#4-4-相似度计算" class="headerlink" title="4.4. 相似度计算"></a>4.4. 相似度计算</h2><p>考虑权重以后，对于某个图像A，它的特征点可对应到许多个单词，组成它的Bag-of-Words：</p>
<p><img src="https://img-blog.csdnimg.cn/20200217182921656.png#pic_center" alt="在这里插入图片描述"><br>对于给定的VA和VB，通过某些方式即可比较其相似度。如L1范数：<br><img src="https://img-blog.csdnimg.cn/20200217182945286.png#pic_center" alt="在这里插入图片描述"></p>
<h1 id="5-建图"><a href="#5-建图" class="headerlink" title="5. 建图"></a>5. 建图</h1><p>所谓地图，即所有路标点的集合。一旦我们确定了路标点的位置，那就可以说我们完成了建图。SLAM 作为一种底层技术，往往是用来为上层应用提供信息的。应用层面对于“定位”的需求是相似的，他们希望SLAM 提供相机或搭载相机的主体的空间位姿信息。而对于地图，则存在着许多不同的需求。<br><img src="https://img-blog.csdnimg.cn/20200217183024765.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>稀疏地图只建模感兴趣的部分，也就是前面说了很久的特征点（路标点）。<br>稠密地图是指，建模所有看到过的部分。<br>在稠密重建，我们需要知道每一个像素点（或大部分像素点）的距离，那么大致上有以下几种解决方案：</p>
<ol>
<li>使用单目相机，利用移动相机之后进行三角化，测量像素的距离。</li>
<li>使用双目相机，利用左右目的视差计算像素的距离（多目原理相同）。</li>
<li>使用RGB-D 相机直接获得像素距离。<br>前两种方式称为立体视觉（Stereo Vision），其中移动单目的又称为移动视角的立体视觉（Moving View Stereo）。相比于RGB-D 直接测量的深度，单目和双目对深度的获取往往是“费力不讨好”的——我们需要花费大量的计算，最后得到一些不怎么可靠的¬深度估计。当然，RGB-D 也有一些量程、应用范围和光照的限制，不过相比于单目和双目的结果，使用RGB-D 进行稠密重建往往是更常见的选择。而单目双目的好处，是在目前RGB-D还无法很好应用的室外、大场景场合中，仍能通过立体视觉估计深度信息。<h2 id="5-1-单目稠密建图"><a href="#5-1-单目稠密建图" class="headerlink" title="5.1. 单目稠密建图"></a>5.1. 单目稠密建图</h2>在稠密深度图估计中，我们无法把每个像素都当作特征点，计算描述子。因此，稠密深度估计问题中，匹配就成为很重要的一环：如何确定第一张图的某像素，出现在其他图里的位置呢？这需要用到极线搜索和块匹配技术。然后，当我们知道了某个像素在各个图中的位置，就能像特征点那样，利用三角测量确定它的深度。<br><img src="https://img-blog.csdnimg.cn/20200217183125466.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>左边的相机观测到了某个像素p1。由于这是一个单目相机，我们无从知道它的深度，所以假设这个深度可能在某个区域之内，不妨说是某最小值到无穷远之间：(dmin，+∞)。因此，该像素对应的空间点就分布在某条线段（本例中是射线）上。在另一个视角（右侧相机）看来，这条线段的投影也形成图像平面上的一条线，我们知道这称为极线。<br>在p1 周围取一个大小为w * w 的小块，然后在极线上也取很多同样大小的小块进行比较，就可以一定程度上提高区分性。这就是所谓的块匹配。<br>然后计算小块与小块间的差异，存在很多计算方法，如<br><img src="https://img-blog.csdnimg.cn/20200217183155258.png#pic_center" alt="在这里插入图片描述"><br>它计算的是两个小块的相关性，接近0表示两个图像不相似，而接近1表示相似。<br><img src="https://img-blog.csdnimg.cn/2020021718322212.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><h2 id="5-2-RGB-D稠密建图"><a href="#5-2-RGB-D稠密建图" class="headerlink" title="5.2. RGB-D稠密建图"></a>5.2. RGB-D稠密建图</h2>除了使用单目和双目进行稠密重建之外，在适用范围内，RGB-D 相机是一种更好的选择。在RGB-D 相机中可以完全通过传感器中硬件测量得到深度，无需消耗大量的计算资源来估计它们。并且，RGB-D 的结构光或飞时原理，保证了深度数据对纹理的无关性。即使面对纯色的物体，只要它能够反射光，我们就能测量到它的深度。这亦是RGB-D 传感器的一大优势。</li>
</ol>
<p>利用RGB-D 进行稠密建图是相对容易的。不过，根据地图形式不同，也存在着若干种不同的主流建图方式。最直观最简单的方法，就是根据估算的相机位姿，将RGB-D 数据转化为点云（Point Cloud），然后进行拼接，最后得到一个由离散的点组成的点云地图（Point Cloud Map）。在此基础上，如果我们对外观有进一步的要求，希望估计物体的表面，可以使用三角网格（Mesh），面片（Surfel）进行建图。另一方面，如果希望知道地图的障碍物信息并在地图上导航，亦可通过体素（Voxel）建立占据网格地图（Occupancy Map）。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/%E3%80%8ADeepLearning%20with%20Python%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/%E3%80%8ADeepLearning%20with%20Python%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/" class="post-title-link" itemprop="url">《DeepLearning with Python》读书笔记（一）</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:55:01 / Modified: 12:55:08" itemprop="dateCreated datePublished" datetime="2020-03-05T12:55:01+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="1-什么是深度学习"><a href="#1-什么是深度学习" class="headerlink" title="1.什么是深度学习"></a>1.什么是深度学习</h1><p>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200217185528695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200217185528695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =800x)</p>
<h2 id="三张图理解深度学习工作原理"><a href="#三张图理解深度学习工作原理" class="headerlink" title="三张图理解深度学习工作原理"></a>三张图理解深度学习工作原理</h2><p>神经网络中每层对输入数据所做的具体操作保存在该层的权重（weight）中，其本质是一串数字。用术语来说，每层实现的变换由其权重来参数化（parameterize，见图 1-7）。权重有时也被称为该层的参数（parameter）。在这种语境下，学习的意思是为神经网络的所有层找到一组权重值，使得该网络能够将每个示例输入与其目标正确地一一对应。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200217190131517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200217190131517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =500x)<br>想要控制一件事物，首先需要能够观察它。想要控制神经网络的输出，就需要能够衡量该输出与预期值之间的距离。这是神经网络损失函数（loss function）的任务，该函数也叫目标函数（objective function）。损失函数的输入是网络预测值与真实目标值（即你希望网络输出的<br>结果），然后计算一个距离值，衡量该网络在这个示例上的效果好坏<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200217190500415.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200217190500415.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =400x)<br>深度学习的基本技巧是利用这个距离值作为反馈信号来对权重值进行微调，以降低当前示 例对应的损失值（见图 1-9）。这种调节由优化器（optimizer）来完成，它实现了所谓的反向 传播（backpropagation）算法，这是深度学习的核心算法。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200217194424145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200217194424145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center</a> =400x)<br>一开始对神经网络的权重随机赋值，因此网络只是实现了一系列随机变换。其输出结果自然也和理想值相去甚远，相应地，损失值也很高。但随着网络处理的示例越来越多，权重值也 在向正确的方向逐步微调，损失值也逐渐降低。这就是训练循环（training loop），将这种循环重复足够多的次数（通常对数千个示例进行数十次迭代），得到的权重值可以使损失函数最小。具有最小损失的网络，其输出值与目标值尽可能地接近，这就是训练好的网络。再次强调，这是一个简单的机制，一旦具有足够大的规模，将会产生魔法般的效果。</p>
<h1 id="2-神经网络的数学基础"><a href="#2-神经网络的数学基础" class="headerlink" title="2.神经网络的数学基础"></a>2.神经网络的数学基础</h1><h2 id="2-2-神经网络的数据表示"><a href="#2-2-神经网络的数据表示" class="headerlink" title="2.2 神经网络的数据表示"></a>2.2 神经网络的数据表示</h2><p>数据存储在多维 Numpy 数组中，也叫张量（tensor）。一般来说，当前所 有机器学习系统都使用张量作为基本数据结构。张量对这个领域非常重要，重要到 Google 的 TensorFlow 都以它来命名。那么什么是张量？ 张量这一概念的核心在于，它是一个数据容器。它包含的数据几乎总是数值数据，因此它 是数字的容器。你可能对矩阵很熟悉，它是二维张量。张量是矩阵向任意维度的推广［注意， 张量的维度（dimension）通常叫作轴（axis）］。</p>
<h3 id="2-2-1标量（0D张量）"><a href="#2-2-1标量（0D张量）" class="headerlink" title="2.2.1标量（0D张量）"></a>2.2.1标量（0D张量）</h3><p>仅包含一个数字的张量叫作标量（scalar，也叫标量张量、零维张量、0D 张量）。在 Numpy 中，一个 float32 或 float64 的数字就是一个标量张量（或标量数组）。标量张量有 0 个轴。张量轴的个数也叫作 阶（rank）。</p>
<h3 id="2-2-2向量（1D张量）"><a href="#2-2-2向量（1D张量）" class="headerlink" title="2.2.2向量（1D张量）"></a>2.2.2向量（1D张量）</h3><p>数字组成的数组叫作向量（vector）或一维张量（1D 张量）。一维张量只有一个轴。下面是一个 Numpy 向量。<br>![在这里插入图片描述](<a href="https://img-blog.csdnimg.cn/20200217202516264.png#pic_center" target="_blank" rel="noopener">https://img-blog.csdnimg.cn/20200217202516264.png#pic_center</a> =300x)<br>这个向量有 5 个元素，所以被称为 5D 向量。不要把 5D 向量和 5D 张量弄混！ 5D 向量只 有一个轴，沿着轴有 5 个维度，而 5D 张量有 5 个轴（沿着每个轴可能有任意个维度）。维度 （dimensionality）可以表示沿着某个轴上的元素个数（比如 5D 向量），也可以表示张量中轴的个 数（比如 5D 张量），这有时会令人感到混乱。对于后一种情况，技术上更准确的说法是 5 阶张量 （张量的阶数即轴的个数），但 5D 张量这种模糊的写法更常见。</p>
<h3 id="2-2-3-矩阵（2D张量）"><a href="#2-2-3-矩阵（2D张量）" class="headerlink" title="2.2.3 矩阵（2D张量）"></a>2.2.3 矩阵（2D张量）</h3><p>向量组成的数组叫作矩阵（matrix）或二维张量（2D 张量）。矩阵有 2 个轴（通常叫作行和列）。</p>
<h3 id="2-2-4-3D-张量与更高维张量"><a href="#2-2-4-3D-张量与更高维张量" class="headerlink" title="2.2.4     3D 张量与更高维张量"></a>2.2.4     3D 张量与更高维张量</h3><p>将多个矩阵组合成一个新的数组，可以得到一个 3D 张量，你可以将其直观地理解为数字组成的立方体。将多个 3D 张量组合成一个数组，可以创建一个 4D 张量，以此类推。深度学习处理的一般是 0D 到 4D 的张量，但处理视频数据时可能会遇到 5D 张量。</p>
<h3 id="2-2-5-关键属性"><a href="#2-2-5-关键属性" class="headerlink" title="2.2.5 关键属性"></a>2.2.5 关键属性</h3><p>张量是由以下三个关键属性来定义的。</p>
<ul>
<li><strong>轴的个数（阶）</strong>。例如，3D 张量有 3 个轴，矩阵有 2 个轴。这在 Numpy 等 Python 库中也叫张量的 ndim。</li>
<li><strong>形状</strong>。这是一个整数元组，表示张量沿每个轴的维度大小（元素个数）。例如，前面矩阵示例的形状为 (3, 5)，3D 张量示例的形状为 (3, 3, 5)。向量的形状只包含一个元素，比如 (5,)，而标量的形状为空，即 ()。</li>
<li><strong>数据类型（在 Python 库中通常叫作 dtype）</strong>。这是张量中所包含数据的类型，例如，张量的类型可以是 float32、uint8、float64 等。在极少数情况下，你可能会遇到字符（char）张量。注意，Numpy（以及大多数其他库）中不存在字符串张量，因为张量存储在预先分配的连续内存段中，而字符串的长度是可变的，无法用这种方式存储。<h3 id="2-2-6-在-Numpy-中操作张量"><a href="#2-2-6-在-Numpy-中操作张量" class="headerlink" title="2.2.6　在 Numpy 中操作张量"></a>2.2.6　在 Numpy 中操作张量</h3>我们使用语法 train_images[i] 来选择沿着第一个轴的特定数字。选择张量的特定元素叫作张量切片（tensor slicing）<h3 id="2-2-7-数据批量的概念"><a href="#2-2-7-数据批量的概念" class="headerlink" title="2.2.7　数据批量的概念"></a>2.2.7　数据批量的概念</h3>通常来说，深度学习中所有数据张量的第一个轴（0 轴，因为索引从 0 开始）都是样本轴（samples axis，有时也叫样本维度）。在 MNIST 的例子中，样本就是数字图像。</li>
</ul>
<p>此外，深度学习模型不会同时处理整个数据集，而是将数据拆分成小批量。具体来看，下面是 MNIST 数据集的一个批量，批量大小为 128。<br><img src="https://img-blog.csdnimg.cn/20200217203734895.png" alt="在这里插入图片描述"><br>对于这种批量张量，第一个轴（0 轴）叫作批量轴（batch axis）或批量维度（batch dimension）。在使用 Keras 和其他深度学习库时，你会经常遇到这个术语。</p>
<h3 id="2-2-8-现实世界中的数据张量"><a href="#2-2-8-现实世界中的数据张量" class="headerlink" title="2.2.8　现实世界中的数据张量"></a>2.2.8　现实世界中的数据张量</h3><p>我们用几个你未来会遇到的示例来具体介绍数据张量。你需要处理的数据几乎总是以下类别之一。<br><img src="https://img-blog.csdnimg.cn/20200217203908145.png" alt="在这里插入图片描述"></p>
<h3 id="2-2-9-向量数据"><a href="#2-2-9-向量数据" class="headerlink" title="2.2.9　向量数据"></a>2.2.9　向量数据</h3><p>这是最常见的数据。对于这种数据集，每个数据点都被编码为一个向量，因此一个数据批量就被编码为 2D 张量（即向量组成的数组），其中第一个轴是样本轴，第二个轴是特征轴。</p>
<p>例如，人口统计数据集，其中包括每个人的年龄、邮编和收入。每个人可以表示为包含 3 个值的向量，而整个数据集包含 100 000 个人，因此可以存储在形状为 (100000, 3) 的 2D张量中。</p>
<h3 id="2-2-10-时间序列数据或序列数据"><a href="#2-2-10-时间序列数据或序列数据" class="headerlink" title="2.2.10　时间序列数据或序列数据"></a>2.2.10　时间序列数据或序列数据</h3><p>当时间（或序列顺序）对于数据很重要时，应该将数据存储在带有时间轴的 3D 张量中。每个样本可以被编码为一个向量序列（即 2D 张量），因此一个数据批量就被编码为一个 3D 张量（见图 2-3）。<br><img src="https://img-blog.csdnimg.cn/20200217204620754.png#pic_center" alt="在这里插入图片描述"><br>根据惯例，时间轴始终是第 2 个轴（索引为 1 的轴）。例如，股票价格数据集。每一分钟，我们将股票的当前价格、前一分钟的最高价格和前一分钟的最低价格保存下来。因此每分钟被编码为一个 3D 向量，整个交易日被编码为一个形状为 (390, 3) 的 2D 张量（一个交易日有 390 分钟），而 250 天的数据则可以保存在一个形状为 (250, 390, 3) 的 3D 张量中。这里每个样本是一天的股票数据。</p>
<h3 id="2-2-11-图像数据"><a href="#2-2-11-图像数据" class="headerlink" title="2.2.11　图像数据"></a>2.2.11　图像数据</h3><p>图像通常具有三个维度：高度、宽度和颜色深度。虽然灰度图像（比如 MNIST 数字图像） 只有一个颜色通道，因此可以保存在 2D 张量中，但按照惯例，图像张量始终都是 3D 张量，灰 度图像的彩色通道只有一维。因此，如果图像大小为 256×256，那么 128 张灰度图像组成的批 量可以保存在一个形状为 (128, 256, 256, 1) 的张量中，而 128 张彩色图像组成的批量则 可以保存在一个形状为 (128, 256, 256, 3) 的张量中（见图 2-4）。</p>
<p><img src="https://img-blog.csdnimg.cn/20200217205041235.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>图像张量的形状有两种约定：通道在后（channels-last）的约定（在 TensorFlow 中使用）和 通道在前（channels-first）的约定（在 Theano 中使用）。Google 的 TensorFlow 机器学习框架将 颜色深度轴放在最后：(samples, height, width, color_depth)。与此相反，Theano 将图像深度轴放在批量轴之后：(samples, color_depth, height, width)。如果采 用 Theano 约定，前面的两个例子将变成 (128, 1, 256, 256) 和 (128, 3, 256, 256)。 Keras 框架同时支持这两种格式。</p>
<h3 id="2-2-12-视频数据"><a href="#2-2-12-视频数据" class="headerlink" title="2.2.12　视频数据"></a>2.2.12　视频数据</h3><p>视频数据是现实生活中需要用到 5D 张量的少数数据类型之一。视频可以看作一系列帧， 每一帧都是一张彩色图像。由于每一帧都可以保存在一个形状为 (height, width, color_ depth) 的 3D 张量中，因此一系列帧可以保存在一个形状为 (frames, height, width,  color_depth) 的 4D 张量中，而不同视频组成的批量则可以保存在一个 5D 张量中，其形状为 (samples, frames, height, width, color_depth)。 举个例子，一个以每秒 4 帧采样的 60 秒 YouTube 视频片段，视频尺寸为 144×256，这个 视频共有 240 帧。4 个这样的视频片段组成的批量将保存在形状为 (4, 240, 144, 256, 3) 的张量中。总共有 106 168 320 个值！如果张量的数据类型（dtype）是 float32，每个值都是 32 位，那么这个张量共有 405MB。好大！你在现实生活中遇到的视频要小得多，因为它们不以 float32 格式存储，而且通常被大大压缩，比如 MPEG 格式。</p>
<h2 id="2-3-神经网络的“齿轮”：张量运算"><a href="#2-3-神经网络的“齿轮”：张量运算" class="headerlink" title="2.3 神经网络的“齿轮”：张量运算"></a>2.3 神经网络的“齿轮”：张量运算</h2><h3 id="2-3-6-深度学习的几何解释"><a href="#2-3-6-深度学习的几何解释" class="headerlink" title="2.3.6　深度学习的几何解释"></a>2.3.6　深度学习的几何解释</h3><p>前面讲过，神经网络完全由一系列张量运算组成，而这些张量运算都只是输入数据的几何变换。因此，你可以将神经网络解释为高维空间中非常复杂的几何变换，这种变换可以通过许多简单的步骤来实现。</p>
<p>对于三维的情况，下面这个思维图像是很有用的。想象有两张彩纸：一张红色，一张蓝色。将其中一张纸放在另一张上。现在将两张纸一起揉成小球。这个皱巴巴的纸球就是你的输入数据，每张纸对应于分类问题中的一个类别。神经网络（或者任何机器学习模型）要做的就是找<br>到可以让纸球恢复平整的变换，从而能够再次让两个类别明确可分。通过深度学习，这一过程可以用三维空间中一系列简单的变换来实现，比如你用手指对纸球做的变换，每次做一个动作，如图 2-9 所示。<br><img src="https://img-blog.csdnimg.cn/20200217211411141.png#pic_center" alt="在这里插入图片描述"><br>让纸球恢复平整就是机器学习的内容：为复杂的、高度折叠的数据流形找到简洁的表示。现在你应该能够很好地理解，为什么深度学习特别擅长这一点：它将复杂的几何变换逐步分解为一长串基本的几何变换，这与人类展开纸球所采取的策略大致相同。深度网络的每一层都通<br>过变换使数据解开一点点——许多层堆叠在一起，可以实现非常复杂的解开过程。</p>
<h2 id="2-4-神经网络的“引擎”：基于梯度的优化"><a href="#2-4-神经网络的“引擎”：基于梯度的优化" class="headerlink" title="2.4　神经网络的“引擎”：基于梯度的优化"></a>2.4　神经网络的“引擎”：基于梯度的优化</h2><h1 id="4-机器学习基础"><a href="#4-机器学习基础" class="headerlink" title="4.机器学习基础"></a>4.机器学习基础</h1><h2 id="4-1-机器学习的四个分支"><a href="#4-1-机器学习的四个分支" class="headerlink" title="4.1 机器学习的四个分支"></a>4.1 机器学习的四个分支</h2><p><img src="https://img-blog.csdnimg.cn/20200220170900394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>监督学习</strong></p>
<p>监督学习是目前最常见的机器学习类型。给定一组样本（通常由人工标注），它可以学会将输入数据映射到已知目标［也叫标注（annotation）］。一般来说，近年来广受关注的深度学习应用几乎都属于监督学习，比如光学字符识别、语音识别、图像分类和语言翻译。虽然监督学习主要包括分类和回归，但还有更多的奇特变体，主要包括如下几种。</p>
<ul>
<li>序列生成（sequence generation）。给定一张图像，预测描述图像的文字。序列生成有时可以被重新表示为一系列分类问题，比如反复预测序列中的单词或标记。</li>
<li>语法树预测（syntax tree prediction）。给定一个句子，预测其分解生成的语法树。</li>
<li>目标检测（object detection）。给定一张图像，在图中特定目标的周围画一个边界框。这个问题也可以表示为分类问题（给定多个候选边界框，对每个框内的目标进行分类）或分类与回归联合问题（用向量回归来预测边界框的坐标）。</li>
<li>图像分割（image segmentation）。给定一张图像，在特定物体上画一个像素级的掩模（mask）。</li>
</ul>
<p><strong>无监督学习</strong></p>
<p>无监督学习是指在没有目标的情况下寻找输入数据的有趣变换，其目的在于数据可视化、数据压缩、数据去噪或更好地理解数据中的相关性。无监督学习是数据分析的必备技能，在解决监督学习问题之前，为了更好地了解数据集，它通常是一个必要步骤。降维（dimensionality<br>reduction）和聚类（clustering）都是众所周知的无监督学习方法。</p>
<p>　<strong>自监督学习</strong><br>自监督学习是监督学习的一个特例，它与众不同，值得单独归为一类。自监督学习是没有 人工标注的标签的监督学习，你可以将它看作没有人类参与的监督学习。标签仍然存在（因为 总要有什么东西来监督学习过程），但它们是从输入数据中生成的，通常是使用启发式算法生 成的。 </p>
<p>举个例子，自编码器（autoencoder）是有名的自监督学习的例子，其生成的目标就是未经 修改的输入。同样，给定视频中过去的帧来预测下一帧，或者给定文本中前面的词来预测下一个词， 都是自监督学习的例子［这两个例子也属于时序监督学习（temporally supervised learning），即用 未来的输入数据作为监督］。注意，监督学习、自监督学习和无监督学习之间的区别有时很模糊， 这三个类别更像是没有明确界限的连续体。自监督学习可以被重新解释为监督学习或无监督学 习，这取决于你关注的是学习机制还是应用场景。</p>
<p>　<strong>强化学习</strong><br>　 强化学习一直以来被人们所忽视，但最近随着 Google 的 DeepMind 公司将其成功应用于学 习玩 Atari 游戏（以及后来学习下围棋并达到最高水平），机器学习的这一分支开始受到大量关注。 在强化学习中，智能体（agent）接收有关其环境的信息，并学会选择使某种奖励最大化的行动。 例如，神经网络会“观察”视频游戏的屏幕并输出游戏操作，目的是尽可能得高分，这种神经 网络可以通过强化学习来训练。</p>
<p><strong>分类和回归术语表</strong><br>分类和回归都包含很多专业术语。<br>这些术语在机器学习领域都有确切的定义，你应该了解这些定义。</p>
<ul>
<li>样本（sample）或输入（input）：进入模型的数据点。</li>
<li>预测（prediction）或输出（output）：从模型出来的结果。</li>
<li>目标（target）：真实值。对于外部数据源，理想情况下，模型应该能够预测出目标。</li>
<li>预测误差（prediction error）或损失值（loss value）：模型预测与目标之间的距离。</li>
<li>类别（class）：分类问题中供选择的一组标签。例如，对猫狗图像进行分类时，“狗”和“猫”就是两个类别。</li>
<li>标签（label）：分类问题中类别标注的具体例子。比如，如果 1234 号图像被标注为包含类别“狗”，那么“狗”就是 1234 号图像的标签。</li>
<li>真值（ground-truth）或标注（annotation）：数据集的所有目标，通常由人工收集。</li>
<li>二分类（binary classification）：一种分类任务，每个输入样本都应被划分到两个互斥的类别中。</li>
<li>多分类（multiclass classification）：一种分类任务，每个输入样本都应被划分到两个以上的类别中，比如手写数字分类。</li>
<li>多标签分类（multilabel classification）：一种分类任务，每个输入样本都可以分配多个标签。举个例子，如果一幅图像里可能既有猫又有狗，那么应该同时标注“猫”标签和“狗”标签。每幅图像的标签个数通常是可变的。</li>
<li>标量回归（scalar regression）：目标是连续标量值的任务。预测房价就是一个很好的例子，不同的目标价格形成一个连续的空间。</li>
<li>向量回归（vector regression）：目标是一组连续值（比如一个连续向量）的任务。如果对多个值（比如图像边界框的坐标）进行回归，那就是向量回归。</li>
<li>小批量（mini-batch）或批量（batch）：模型同时处理的一小部分样本（样本数通常为 8~128）。样本数通常取 2 的幂，这样便于 GPU 上的内存分配。训练时，小批量用来为模型权重计算一次梯度下降更新。</li>
</ul>
<h1 id="5-深度学习用于计算机视觉"><a href="#5-深度学习用于计算机视觉" class="headerlink" title="5.深度学习用于计算机视觉"></a>5.深度学习用于计算机视觉</h1><h2 id="5-1-卷积神经网络简介"><a href="#5-1-卷积神经网络简介" class="headerlink" title="5.1　卷积神经网络简介"></a>5.1　卷积神经网络简介</h2><h3 id="5-1-1-卷积运算"><a href="#5-1-1-卷积运算" class="headerlink" title="5.1.1　卷积运算"></a>5.1.1　卷积运算</h3><p>密集连接层和卷积层的根本区别在于，Dense 层从输入特征空间中学到的是全局模式,比如对于 MNIST 数字，全局模式就是涉及所有像素的模式），而卷积层学到的是局部模式（见 图 5-1），对于图像来说，学到的就是在输入图像的二维小窗口中发现的模式。在上面的例子中， 这些窗口的大小都是 3×3。<br><img src="https://img-blog.csdnimg.cn/20200220171521683.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这个重要特性使卷积神经网络具有以下两个有趣的性质。</p>
<ul>
<li>卷积神经网络学到的模式具有<strong>平移不变性（translation invariant）</strong>。卷积神经网络在图像 右下角学到某个模式之后，它可以在任何地方识别这个模式，比如左上角。对于密集连 接网络来说，如果模式出现在新的位置，它只能重新学习这个模式。这使得卷积神经网 络在处理图像时可以高效利用数据（因为视觉世界从根本上具有平移不变性），它只需 要更少的训练样本就可以学到具有泛化能力的数据表示。 <ul>
<li>卷积神经网络可以学到模式的<strong>空间层次结构（spatial hierarchies of patterns）</strong>，见图 5-2。 第一个卷积层将学习较小的局部模式（比如边缘），第二个卷积层将学习由第一层特征 组成的更大的模式，以此类推。这使得卷积神经网络可以有效地学习越来越复杂、越来 越抽象的视觉概念（因为视觉世界从根本上具有空间层次结构）。<br><img src="https://img-blog.csdnimg.cn/20200220171822621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>在 MNIST 示例中，第一个卷积层接收一个大小为 (28, 28, 1) 的特征图，并输出一个大 小为 (26, 26, 32) 的特征图，即它在输入上计算 32 个过滤器。对于这 32 个输出通道，每个 通道都包含一个 26×26 的数值网格，它是过滤器对输入的响应图（response map），表示这个过 滤器模式在输入中不同位置的响应（见图 5-3）。这也是特征图这一术语的含义：深度轴的每个 维度都是一个特征（或过滤器），而 2D 张量 output[:, :, n] 是这个过滤器在输入上的响应 的二维空间图（map）。<br><img src="https://img-blog.csdnimg.cn/20200220172040749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>卷积由以下两个关键参数所定义。</li>
</ul>
</li>
<li>从输入中提取的图块尺寸：这些图块的大小通常是 3×3 或 5×5。本例中为 3×3，这是很常见的选择。</li>
<li>输出特征图的深度：卷积所计算的过滤器的数量。本例第一层的深度为 32，最后一层的深度是 64。</li>
</ul>
<p>卷积的工作原理：在 3D 输入特征图上滑动（slide）这些 3×3 或 5×5 的窗口，在每个可能 的位置停止并提取周围特征的 3D 图块［形状为 (window_height, window_width, input_ depth)］。然后每个 3D 图块与学到的同一个权重矩阵［叫作卷积核（convolution kernel）］做 张量积，转换成形状为 (output_depth,) 的 1D 向量。然后对所有这些向量进行空间重组， 使其转换为形状为 (height, width, output_depth) 的 3D 输出特征图。输出特征图中的 每个空间位置都对应于输入特征图中的相同位置（比如输出的右下角包含了输入右下角的信 息）。举个例子，利用 3×3 的窗口，向量 output[i, j, :] 来自 3D 图块 input[i-1:i+1,  j-1:j+1, :]。整个过程详见下图 。<br><img src="https://img-blog.csdnimg.cn/20200220173236295.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>注意，输出的宽度和高度可能与输入的宽度和高度不同。不同的原因可能有两点。</p>
<ul>
<li>边界效应，可以通过对输入特征图进行填充来抵消。</li>
<li>使用了步幅（stride），稍后会给出其定义。</li>
</ul>
<p><strong>填充</strong><br>如果你希望输出特征图的空间维度与输入相同，那么可以使用填充（padding）。填充是在输入特征图的每一边添加适当数目的行和列，使得每个输入方块都能作为卷积窗口的中心。对于 3×3 的窗口，在左右各添加一列，在上下各添加一行。对于 5×5 的窗口，各添加两行和两<br>列（见图 5-6）。<br><img src="https://img-blog.csdnimg.cn/20200220173424317.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>对于 Conv2D 层，可以通过 padding 参数来设置填充，这个参数有两个取值：”valid” 表示不使用填充（只使用有效的窗口位置）；”same” 表示“填充后输出的宽度和高度与输入相同”。padding 参数的默认值为 “valid”。</p>
<p><strong>卷积步幅</strong><br>两个连续窗口的距离是卷积的一个参数，叫作步幅，默认值为 1。也可以使用步进卷积（strided convolution），即步幅大于 1 的卷积。在图 5-7 中，你可以看到用步幅为 2 的 3×3 卷积从 5×5 输入中提取的图块（无填充）。<br><img src="https://img-blog.csdnimg.cn/20200220173618418.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h3 id="5-1-2-最大池化运算"><a href="#5-1-2-最大池化运算" class="headerlink" title="5.1.2　最大池化运算"></a>5.1.2　最大池化运算</h3><p>最大池化的作用：对特征图进行下采样，与步进卷积类似。<br>最大池化是从输入特征图中提取窗口，并输出每个通道的最大值。它的概念与卷积类似，但是最大池化使用硬编码的 max 张量运算对局部图块进行变换，而不是使用学到的线性变换（卷积核）。最大池化与卷积的最大不同之处在于，最大池化通常使用 2×2 的窗口和步幅 2，其目的是将特征图下采样 2 倍。与此相对的是，卷积通常使用 3×3 窗口和步幅 1。</p>
<p>简而言之，使用下采样的原因，一是减少需要处理的特征图的元素个数，二是通过让连续 卷积层的观察窗口越来越大（即窗口覆盖原始输入的比例越来越大），从而引入空间过滤器的层 级结构。 </p>
<p>注意，最大池化不是实现这种下采样的唯一方法。你已经知道，还可以在前一个卷积层中 使用步幅来实现。此外，你还可以使用平均池化来代替最大池化，其方法是将每个局部输入图 块变换为取该图块各通道的平均值，而不是最大值。但最大池化的效果往往比这些替代方法更好。 简而言之，原因在于特征中往往编码了某种模式或概念在特征图的不同位置是否存在（因此得 名特征图），而观察不同特征的最大值而不是平均值能够给出更多的信息。因此，最合理的子采 样策略是首先生成密集的特征图（通过无步进的卷积），然后观察特征每个小图块上的最大激活， 而不是查看输入的稀疏窗口（通过步进卷积）或对输入图块取平均，因为后两种方法可能导致 错过或淡化特征是否存在的信息。</p>
<h3 id="5-3-3-小结"><a href="#5-3-3-小结" class="headerlink" title="5.3.3　小结"></a>5.3.3　小结</h3><ul>
<li>卷积神经网络是用于计算机视觉任务的最佳机器学习模型。即使在非常小的数据集上也可以从头开始训练一个卷积神经网络，而且得到的结果还不错。</li>
<li>在小型数据集上的主要问题是过拟合。在处理图像数据时，数据增强是一种降低过拟合的强大方法。</li>
<li>利用特征提取，可以很容易将现有的卷积神经网络复用于新的数据集。对于小型图像数据集，这是一种很有价值的方法。</li>
<li>作为特征提取的补充，你还可以使用微调，将现有模型之前学到的一些数据表示应用于新问题。这种方法可以进一步提高模型性能。</li>
<li>现在你已经拥有一套可靠的工具来处理图像分类问题，特别是对于小型数据集。</li>
</ul>
<h2 id="9-2-深度学习的局限性"><a href="#9-2-深度学习的局限性" class="headerlink" title="9.2　深度学习的局限性"></a>9.2　深度学习的局限性</h2><p>深度学习模型只是将一个向量空间映射到另一个向量空间的简单而又连续的几何 变换链。它能做的只是将一个数据流形 X 映射到另一个流形 Y，前提是从 X 到 Y 存在可学习的 连续变换。深度学习模型可以被看作一种程序，但反过来说，大多数程序都不能被表示为深度 学习模型。对于大多数任务而言，要么不存在相应的深度神经网络能够解决任务，要么即使存 在这样的网络，它也可能是不可学习的（learnable）。后一种情况的原因可能是相应的几何变换 过于复杂，也可能是没有合适的数据用于学习。 </p>
<p>通过堆叠更多的层并使用更多训练数据来扩展当前的深度学习技术，只能在表面上缓解一 些问题，无法解决更根本的问题，比如深度学习模型可以表示的内容非常有限，比如大多数你 想要学习的程序都不能被表示为数据流形的连续几何变形。</p>
<p>简而言之，深度学习模型并不理解它们的输入，至少不是人类所说的理解。我们自己对图 像、声音和语言的理解是基于我们作为人类的感觉运动体验。机器学习模型无法获得这些体验， 因此也就无法用与人类相似的方式来理解它们的输入。通过对输入模型的大量训练样本进行标 记，我们可以让模型学会一个简单几何变换，这个变换在一组特定样本上将数据映射到人类概念， 但这种映射只是我们头脑中原始模型的简化。我们头脑中的原始模型是从我们作为具身主体的 体验发展而来的。机器学习模型就像是镜子中的模糊图像（见图 9-3）。<br><img src="https://img-blog.csdnimg.cn/20200220175249156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RpYW53ZW5ibzY2NjY=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/05/Linux%E5%A4%87%E5%BF%98%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jeff Tian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求知若饥,虚心若愚。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/05/Linux%E5%A4%87%E5%BF%98%E5%BD%95/" class="post-title-link" itemprop="url">Linux备忘录</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-05 12:54:34 / Modified: 13:03:17" itemprop="dateCreated datePublished" datetime="2020-03-05T12:54:34+08:00">2020-03-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Ubuntu备忘"><a href="#Ubuntu备忘" class="headerlink" title="Ubuntu备忘"></a>Ubuntu备忘</h1><table>
<thead>
<tr>
<th align="center">指令</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Ctrl+h     或者 ls   -a</td>
<td>显示隐藏文件</td>
</tr>
<tr>
<td align="center">ls -l name.file</td>
<td>查看文件详细信息</td>
</tr>
<tr>
<td align="center">cp source.file target.file -r</td>
<td>复制文件和目录</td>
</tr>
<tr>
<td align="center">mv file1 file2</td>
<td>改名字</td>
</tr>
<tr>
<td align="center">mv flie1 ~/a/b/file2</td>
<td>移动位置</td>
</tr>
<tr>
<td align="center">file  target.file</td>
<td>查看文件类型</td>
</tr>
<tr>
<td align="center">less name.file</td>
<td>以bash手册的形式查看name.file的内容，支持空格、PageDown翻下页，PageUp翻上页，Enter翻下行，q退出。</td>
</tr>
<tr>
<td align="center">touch name.file</td>
<td>若name.file不存在，则创建新文件；若name.file存在，则更新修改时间。</td>
</tr>
<tr>
<td align="center">ps -ef</td>
<td>显示所有进程的扩展信息。</td>
</tr>
<tr>
<td align="center">top</td>
<td>实时监控进程，按q退出,按d设置刷新间隔。</td>
</tr>
<tr>
<td align="center">kill PID</td>
<td>关闭第PID个进程，程度弱，可能被忽略。</td>
</tr>
<tr>
<td align="center">killall name*</td>
<td>强制关闭name开头的所有进程，谨慎使用，使用通配符*时可能关闭系统进程，损坏系统。</td>
</tr>
<tr>
<td align="center">××××××××××</td>
<td>××××××××××××</td>
</tr>
<tr>
<td align="center">sudo dpkg -i package.deb</td>
<td>安装软件包</td>
</tr>
<tr>
<td align="center">sudo apt-get install -f</td>
<td>修复有问题的软件包的依赖项</td>
</tr>
<tr>
<td align="center">grep netease-cloud-music</td>
<td>查看网易云音乐软件包是否已经安装</td>
</tr>
<tr>
<td align="center">sudo apt-get remove netease-cloud-music</td>
<td>卸载网易云音乐</td>
</tr>
<tr>
<td align="center">dpkg -l</td>
<td>显示所有安装了的软件包</td>
</tr>
<tr>
<td align="center">×××××××××××××××</td>
<td>×××××××××××××××××××</td>
</tr>
<tr>
<td align="center"></td>
<td></td>
</tr>
</tbody></table>
<ul>
<li>点击图标实现最小化应用</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gsettings <span class="built_in">set</span> org.compiz.unityshell:/org/compiz/profiles/unity/plugins/unityshell/ launcher-minimize-window <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h1 id="vim备忘"><a href="#vim备忘" class="headerlink" title="vim备忘"></a>vim备忘</h1><table>
<thead>
<tr>
<th>指令</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>Ctrl + f</td>
<td>向下翻整页</td>
</tr>
<tr>
<td>Ctrl + b</td>
<td>向上翻整页</td>
</tr>
<tr>
<td>-  <code>/etc/vim/vimrc</code>      vimrc是vim的配置文件</td>
<td></td>
</tr>
</tbody></table>
<h1 id="shell备忘"><a href="#shell备忘" class="headerlink" title="shell备忘"></a>shell备忘</h1><ul>
<li>查看电脑有哪些shell</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/shells</span><br></pre></td></tr></table></figure>

<h1 id="ros备忘"><a href="#ros备忘" class="headerlink" title="ros备忘"></a>ros备忘</h1><h2 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"source /opt/ros/kinetic/setup.bash"</span> &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>source这个单词，代表驱寻找的意思，后面一长串/opt/ros/kinetic/setup.bash就是ROS本身工作空间环境变量配置脚本文件的路径，&gt;&gt; ~/.bashrc表示将这个环境变量配置脚本写到终端配置文件.bashrc中。</p>
<p>这里出现了一个新的文件，也就是终端配置文件.bashrc，这是我们打开的终端的一个配置文件，配置环境变量就相当于将工作空间的环境变量脚本，包括其路径，记录到这个终端配置文件.bashrc上。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"source /home/twb/catkin_ws/devel/setup.bash"</span> &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>这个代码就是配置我们所创建的工作空间的环境变量，/home/twb/catkin_ws/devel/setup.bash这个是路径。<br>.bashrc中增加内容如下<br><img src="https://img-blog.csdnimg.cn/20200303191501134.png#pic_center" alt="在这里插入图片描述"><br>查看一下现在ROS内部的环境变量有哪些</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$ROS_PACKAGE_PATH</span></span><br></pre></td></tr></table></figure>
<p>结果如下：<br><img src="https://img-blog.csdnimg.cn/20200303191825194.png#pic_center" alt="在这里插入图片描述"><br>冒号前面是我自己创建的工作空间的路径，那个/src文件夹是专门存放源代码和功能包的。冒号后面是ROS本身工作空间下源代码和软件包所存放的文件夹的路径。</p>
<h1 id="Python备忘"><a href="#Python备忘" class="headerlink" title="Python备忘"></a>Python备忘</h1><ul>
<li>安装 anaconda 后Linux的终端界面前部出现（base）字样</li>
</ul>
<p>1.打开一个终端 ，输入命令：vim ～/.bashrc<br>2.在 .bashrc文件最后面添加命令：conda deactivate</p>
<p>再重新打开终端即可消除base字样</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jeff Tian</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jeff Tian</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.7.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
